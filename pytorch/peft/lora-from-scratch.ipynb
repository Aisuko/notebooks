{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd65eb9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007561,
     "end_time": "2024-02-21T12:34:34.205916",
     "exception": false,
     "start_time": "2024-02-21T12:34:34.198355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "**Note: All the images are come from the credit setion at the bottom.**\n",
    "\n",
    "Low-rank adaptation(LoRA) is a machine learning technique that modifies a pretrained model(for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters.\n",
    "\n",
    "This approach is important because it allows for efficient finetuning of large models on task-specific data significantly reducing the computational cost and time required for finetuning.\n",
    "\n",
    "Since LLMs are large, updating all model weights during training can be expensive due to GPU memory limitations. Suppoese we have a large weight matrix $W$ for a given layer. During backpropagation, we learn a $\\Delta W$ matrix, which contains information on how much we want to update the original weights to minimize the loss function during training.\n",
    "\n",
    "In regular training and finetuning, the weight update is defined as follows:\n",
    "\n",
    "$$W_{updated}=W+\\Delta W$$\n",
    "\n",
    "The LoRA method proposed by [Hu et al](https://arxiv.org/abs/2106.09685), offers a more efficient alternative to computing the weight updates $\\Delta W$ **by learning an approximation of it**, $\\Delta W \\approx AB$. In other words, in LoRA, we have the following, where $A$ and $B$ are two small weight matrices:\n",
    "\n",
    "$$W_{updated}=W+A.B$$\n",
    "\n",
    "(The \".\" in \"A.B\" stands for matrix multiplication)\n",
    "\n",
    "The figure below illustrates these formulas for full finetuning and LoRA side by side.\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/966/350/443/956/461/small/e5254a6add0141f4.webp)\n",
    "\n",
    "\n",
    "## How does LoRA save GPU memory\n",
    "\n",
    "For example, if a pretrained weight matrix $W$ is a $1000*1000$ matrix, then the weight update matrix $\\Delta W$ in regular finetuning is a $1000*1000$ matrix as well. In this case, $\\Delta W$ has $1,000,000$ parameters. If we consider a LoRA rank of $2$, them $A$ is a $1000*2$ matrix, and $B$ is a $2*1000$ matrix, and we only have $2*2*1000=4000$ parameters that we ned to update when using LoRA. In the previous example, with a rank of $2$, that's $250$ times fewer parameters.\n",
    "\n",
    "Of course, $A$ and $B$ can't capture all the information that $\\Delta W$ could capture, **but this is by design**. When using LoRA, we hypothesize that the model requires $W$ to be a large matrix with full rank to capture all the knowledge in the pretraining dataset. However, when we finetune an LLM, we don't need to update all the weights and capture the core information for the adaptation in a smaller number of weights than $\\Delta W$ would; hence, we have the low-rank updates via $AB$.\n",
    "\n",
    "If we paid close attention, the full finetuning and LoRA depictions in the figure above look slightly different from the formulas I have shown earlier. That's due to the distributive law of matrix multiplication: we don't have to add the weights with the updated weights but can keep them separate. For instance, if $x$ is the input data, then we can write the following for regular finetuning:\n",
    "\n",
    "$$x.(W+\\Delta W)=x.W+x.\\Delta W$$\n",
    "\n",
    "Similarly, we can write the following for LoRA:\n",
    "\n",
    "$$x.(W+A.B)=x.W+x.A.B$$\n",
    "\n",
    "The fact that we can keep the LoRA weight matrices separate makes LoRA especially attractive. In practice, this means that we don't have to modify the weights of the pretrained model at all, as we can apply the LoRA matrices on the fly. This is especially useful if you are considering hosting a model for multiple customers. Instead of having to save the large updated models for each customer, you only have to save the small set of LoRA weights alongside the original pretrained model.\n",
    "\n",
    "\n",
    "# A LoRA Layer Code Implementation\n",
    "\n",
    "We begin by initializing the **LoRALayer** that creates the matrices A and B, along with the **alpha** scaling hyperparameter and the rank hyperparameters. This layer can accept an input and compute the corresponding output, as illustrated in the figure below(The LoRA matrices A and B with rank r).\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/966/454/563/523/540/original/fcc1997b5e645a15.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9bc739c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:34.221599Z",
     "iopub.status.busy": "2024-02-21T12:34:34.221334Z",
     "iopub.status.idle": "2024-02-21T12:34:37.387309Z",
     "shell.execute_reply": "2024-02-21T12:34:37.386502Z"
    },
    "papermill": {
     "duration": 3.176692,
     "end_time": "2024-02-21T12:34:37.389720",
     "exception": false,
     "start_time": "2024-02-21T12:34:34.213028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # NVIDIA CUDA Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev=1/torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A=nn.Parameter(torch.randn(in_dim, rank)*std_dev)\n",
    "        self.B=nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha=alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.alpha*(x@self.A@self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed5713",
   "metadata": {
    "papermill": {
     "duration": 0.006883,
     "end_time": "2024-02-21T12:34:37.403980",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.397097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this code above, $rank$ is the hyperparameter that controls the inner dimension of the matrices $A$ and $B$. In other words, this parameter controls the number of additional parameters introduced by LoRA and is a key factor in determining the balance between model adaptability and parameter efficiency.\n",
    "\n",
    "The second hyperparameter, $alpha$ is a scaling hyperparameter applied to the output of the low-rank adaptation. It essentially controls the extent to which the adapted layer's output is allowed to influence the original output of the layer being adapted. This can be seen as a way to regulate the impact of the low-rank adaptation on the layer's output.\n",
    "\n",
    "So far, the $LoRALayer$ class we implemented above allows us to transform the layer inputs x. However, in LoRA, we are usually interested in replacing existing $Linear$ layers so that the weight update is applied to the existing pretrained weights, as shown in the figure below(LoRA applied to an existing linear layer):\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/966/573/735/919/892/original/d5bcf6590c3a346b.png)\n",
    "\n",
    "To incorporate the original Linear layer weights as shown in the figure above, we will implement a $LinearWithLoRA$ layer that uses the previously implemented $LoRALayer$ and can be used to replace existing $Linear$ layers in a neural network, for example; self-attention module or feed forward modules in an LLM: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa83208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:37.419134Z",
     "iopub.status.busy": "2024-02-21T12:34:37.418761Z",
     "iopub.status.idle": "2024-02-21T12:34:37.424157Z",
     "shell.execute_reply": "2024-02-21T12:34:37.423340Z"
    },
    "papermill": {
     "duration": 0.015178,
     "end_time": "2024-02-21T12:34:37.426055",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.410877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)+self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ac52f",
   "metadata": {
    "papermill": {
     "duration": 0.006822,
     "end_time": "2024-02-21T12:34:37.439843",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.433021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that since we initialize the weight matrix B(self.b in LoRALayer) with zero values in the LoRA layer, the matrix multiplication between A and B results in a matrix consisting of 0's and doesn't affect the original weights (since adding 0 to the original weights does not modify them).\n",
    "\n",
    "\n",
    "# Define a small Single Layer Neural Network\n",
    "\n",
    "Let's try out LoRA on a small neural network layer represented by a single $Linear$ layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb24c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:37.455155Z",
     "iopub.status.busy": "2024-02-21T12:34:37.454530Z",
     "iopub.status.idle": "2024-02-21T12:34:37.556138Z",
     "shell.execute_reply": "2024-02-21T12:34:37.555060Z"
    },
    "papermill": {
     "duration": 0.111577,
     "end_time": "2024-02-21T12:34:37.558298",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.446721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5490,  0.3671,  0.1219,  0.6466, -1.4168,  0.8429, -0.6307,  1.2340,\n",
      "          0.3127,  0.6972]])\n",
      "Linear(in_features=10, out_features=2, bias=True)\n",
      "Original output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "random_seed=123\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "layer=nn.Linear(10,2)\n",
    "x=torch.randn((1, 10))\n",
    "\n",
    "print(x)\n",
    "print(layer)\n",
    "print('Original output:', layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c3cd8",
   "metadata": {
    "papermill": {
     "duration": 0.006857,
     "end_time": "2024-02-21T12:34:37.572697",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.565840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Applying LoRA to Linear Layer\n",
    "\n",
    "Let's apply LoRA to the Linear layer, we see that the results are the same since we haven't trained the LoRA weights yet. In other words, everything works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab38f47c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:37.587598Z",
     "iopub.status.busy": "2024-02-21T12:34:37.587320Z",
     "iopub.status.idle": "2024-02-21T12:34:37.599258Z",
     "shell.execute_reply": "2024-02-21T12:34:37.598363Z"
    },
    "papermill": {
     "duration": 0.021517,
     "end_time": "2024-02-21T12:34:37.601127",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.579610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6639, 0.4487]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Applying LoRA to Linear Layer\n",
    "layer_lora_1=LinearWithLoRA(layer, rank=2, alpha=4)\n",
    "print(layer_lora_1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17251d",
   "metadata": {
    "papermill": {
     "duration": 0.006849,
     "end_time": "2024-02-21T12:34:37.615962",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.609113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merging LoRA matrices and Original Weights\n",
    "\n",
    "As we mentioned above, the distributive law of matrix multiplication $x.(W+A.B)=x.W+x.A.B$.\n",
    "\n",
    "This means that we can also combine or merge the LoRA matrices and original weights, which should result in an equivalent implementation. In code, this alternative implementation to the LinearWithLoRA layer looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e431083e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:37.631118Z",
     "iopub.status.busy": "2024-02-21T12:34:37.630840Z",
     "iopub.status.idle": "2024-02-21T12:34:37.636914Z",
     "shell.execute_reply": "2024-02-21T12:34:37.636154Z"
    },
    "papermill": {
     "duration": 0.015845,
     "end_time": "2024-02-21T12:34:37.638806",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.622961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# This LoRA code is equivalent to LinearWithLoRA\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lora=self.lora.A @ self.lora.B # combine LoRA metrices\n",
    "        # then combine LoRA original weights\n",
    "        combined_weight=self.linear.weight+self.lora.alpha*lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da78da5",
   "metadata": {
    "papermill": {
     "duration": 0.006959,
     "end_time": "2024-02-21T12:34:37.652863",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.645904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In short, $LinearWithLoRAMerged$ computes the left side of equation $x.(W+A.B)=x.W+x.A.B$ whereas $LinearWithLoRA$ computes the right side -- both are euqivalent. We can verify that this results in the same outputs as before via the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a388dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:37.668507Z",
     "iopub.status.busy": "2024-02-21T12:34:37.668240Z",
     "iopub.status.idle": "2024-02-21T12:34:37.679081Z",
     "shell.execute_reply": "2024-02-21T12:34:37.677901Z"
    },
    "papermill": {
     "duration": 0.020796,
     "end_time": "2024-02-21T12:34:37.680961",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.660165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_lora_2=LinearWithLoRAMerged(layer, rank=2, alpha=4)\n",
    "print(layer_lora_2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd265d",
   "metadata": {
    "papermill": {
     "duration": 0.006995,
     "end_time": "2024-02-21T12:34:37.695198",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.688203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Applying LoRA Layers to LLM\n",
    "\n",
    "**Why did we implement LoRA in the manner described above using PyTorch modules?**\n",
    "\n",
    "THis approach enables us to easily replace a Linear layer in an existing neural network(for example, the feed forward or attention modules of a LLM) with our new $LienarWithLoRA$ or $LinearWithLoRAMerged$ layers.\n",
    "\n",
    "\n",
    "## Multilayer Perceptron Model(without LoRA)\n",
    "\n",
    "For simplicity, let's focus on a small 3-layer multilayer perception instead of an LLM for now, which is illustrated in the figure below:\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/966/738/332/188/839/original/c29fe212ed35f033.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780a75cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:37.710507Z",
     "iopub.status.busy": "2024-02-21T12:34:37.710270Z",
     "iopub.status.idle": "2024-02-21T12:34:40.169996Z",
     "shell.execute_reply": "2024-02-21T12:34:40.169032Z"
    },
    "papermill": {
     "duration": 2.469995,
     "end_time": "2024-02-21T12:34:40.172256",
     "exception": false,
     "start_time": "2024-02-21T12:34:37.702261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.005\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.layers(x)\n",
    "        return x\n",
    "\n",
    "# Architecture\n",
    "num_features=784\n",
    "num_hidden_1=128\n",
    "num_hidden_2=256\n",
    "num_classes=10\n",
    "\n",
    "# Settings\n",
    "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate=0.005\n",
    "num_epochs=10\n",
    "    \n",
    "model=MultilayerPerceptron(\n",
    "    num_features=num_features,\n",
    "    num_hidden_1=num_hidden_1,\n",
    "    num_hidden_2=num_hidden_2,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "optimizer_pretrained=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(DEVICE)\n",
    "print(model)\n",
    "print(optimizer_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ae389",
   "metadata": {
    "papermill": {
     "duration": 0.007571,
     "end_time": "2024-02-21T12:34:40.187501",
     "exception": false,
     "start_time": "2024-02-21T12:34:40.179930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a2f251c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:40.203661Z",
     "iopub.status.busy": "2024-02-21T12:34:40.203260Z",
     "iopub.status.idle": "2024-02-21T12:34:41.635170Z",
     "shell.execute_reply": "2024-02-21T12:34:41.634065Z"
    },
    "papermill": {
     "duration": 1.442264,
     "end_time": "2024-02-21T12:34:41.637157",
     "exception": false,
     "start_time": "2024-02-21T12:34:40.194893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 457494319.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 54614830.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 164269052.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 8527541.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# Note: transforms.ToTensor() scales input images to 0-1 range\n",
    "train_dataset=datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset=datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader=DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader=DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d1425",
   "metadata": {
    "papermill": {
     "duration": 0.008857,
     "end_time": "2024-02-21T12:34:41.655290",
     "exception": false,
     "start_time": "2024-02-21T12:34:41.646433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e007a07c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:41.674393Z",
     "iopub.status.busy": "2024-02-21T12:34:41.674098Z",
     "iopub.status.idle": "2024-02-21T12:34:41.680300Z",
     "shell.execute_reply": "2024-02-21T12:34:41.679498Z"
    },
    "papermill": {
     "duration": 0.017834,
     "end_time": "2024-02-21T12:34:41.682151",
     "exception": false,
     "start_time": "2024-02-21T12:34:41.664317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples=0,0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features=features.view(-1, 28*28).to(device)\n",
    "            targets=targets.to(device)\n",
    "            logits=model(features)\n",
    "            _, predicted_labels=torch.max(logits,1)\n",
    "            num_examples+=targets.size(0)\n",
    "            correct_pred+=(predicted_labels==targets).sum()\n",
    "        return correct_pred.float()/num_examples*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a0854",
   "metadata": {
    "papermill": {
     "duration": 0.008641,
     "end_time": "2024-02-21T12:34:41.699498",
     "exception": false,
     "start_time": "2024-02-21T12:34:41.690857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8550f3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:34:41.718256Z",
     "iopub.status.busy": "2024-02-21T12:34:41.717994Z",
     "iopub.status.idle": "2024-02-21T12:37:02.922424Z",
     "shell.execute_reply": "2024-02-21T12:37:02.921476Z"
    },
    "papermill": {
     "duration": 141.216195,
     "end_time": "2024-02-21T12:37:02.924522",
     "exception": false,
     "start_time": "2024-02-21T12:34:41.708327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010|Batch 000/938| Loss: 2.3172\n",
      "Epoch: 001/010|Batch 400/938| Loss: 0.0508\n",
      "Epoch: 001/010|Batch 800/938| Loss: 0.0969\n",
      "Epoch: 001/010 training accuracy: 96.87%\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 002/010|Batch 000/938| Loss: 0.1350\n",
      "Epoch: 002/010|Batch 400/938| Loss: 0.1447\n",
      "Epoch: 002/010|Batch 800/938| Loss: 0.2466\n",
      "Epoch: 002/010 training accuracy: 97.66%\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 003/010|Batch 000/938| Loss: 0.1082\n",
      "Epoch: 003/010|Batch 400/938| Loss: 0.2204\n",
      "Epoch: 003/010|Batch 800/938| Loss: 0.1170\n",
      "Epoch: 003/010 training accuracy: 97.75%\n",
      "Time elapsed: 0.71 min\n",
      "Epoch: 004/010|Batch 000/938| Loss: 0.0820\n",
      "Epoch: 004/010|Batch 400/938| Loss: 0.0058\n",
      "Epoch: 004/010|Batch 800/938| Loss: 0.0278\n",
      "Epoch: 004/010 training accuracy: 98.04%\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 005/010|Batch 000/938| Loss: 0.0326\n",
      "Epoch: 005/010|Batch 400/938| Loss: 0.0173\n",
      "Epoch: 005/010|Batch 800/938| Loss: 0.0074\n",
      "Epoch: 005/010 training accuracy: 98.43%\n",
      "Time elapsed: 1.17 min\n",
      "Epoch: 006/010|Batch 000/938| Loss: 0.0564\n",
      "Epoch: 006/010|Batch 400/938| Loss: 0.1678\n",
      "Epoch: 006/010|Batch 800/938| Loss: 0.0963\n",
      "Epoch: 006/010 training accuracy: 98.08%\n",
      "Time elapsed: 1.40 min\n",
      "Epoch: 007/010|Batch 000/938| Loss: 0.0789\n",
      "Epoch: 007/010|Batch 400/938| Loss: 0.0220\n",
      "Epoch: 007/010|Batch 800/938| Loss: 0.0154\n",
      "Epoch: 007/010 training accuracy: 98.64%\n",
      "Time elapsed: 1.63 min\n",
      "Epoch: 008/010|Batch 000/938| Loss: 0.1646\n",
      "Epoch: 008/010|Batch 400/938| Loss: 0.1181\n",
      "Epoch: 008/010|Batch 800/938| Loss: 0.0234\n",
      "Epoch: 008/010 training accuracy: 98.77%\n",
      "Time elapsed: 1.86 min\n",
      "Epoch: 009/010|Batch 000/938| Loss: 0.1008\n",
      "Epoch: 009/010|Batch 400/938| Loss: 0.0289\n",
      "Epoch: 009/010|Batch 800/938| Loss: 0.0051\n",
      "Epoch: 009/010 training accuracy: 98.72%\n",
      "Time elapsed: 2.10 min\n",
      "Epoch: 010/010|Batch 000/938| Loss: 0.0043\n",
      "Epoch: 010/010|Batch 400/938| Loss: 0.0063\n",
      "Epoch: 010/010|Batch 800/938| Loss: 0.0802\n",
      "Epoch: 010/010 training accuracy: 98.67%\n",
      "Time elapsed: 2.34 min\n",
      "Total Training Time: 2.34 min\n",
      "Test accuracy: 97.21%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(num_epochs, model, optimizer, train_loader, device):\n",
    "    start_time=time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features=features.view(-1, 28*28).to(device)\n",
    "            targets=targets.to(device)\n",
    "            \n",
    "            # forward and back propagation\n",
    "            logits=model(features)\n",
    "            loss=F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # logging\n",
    "            if not batch_idx %400:\n",
    "                print('Epoch: %03d/%03d|Batch %03d/%03d| Loss: %.4f' % (epoch+1, num_epochs, batch_idx, len(train_loader), loss))\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch+1, num_epochs, compute_accuracy(model, train_loader, device)))\n",
    "        \n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "                  \n",
    "                  \n",
    "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695a7f1",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.011161,
     "end_time": "2024-02-21T12:37:02.947293",
     "exception": false,
     "start_time": "2024-02-21T12:37:02.936132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Replacing Linear with LoRA Layers\n",
    "\n",
    "Using $LinearWithLoRA$, we can then add the LoRA layers by replacing the original $Linear$ layers in the multilayer perception model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa5e0563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:37:02.971369Z",
     "iopub.status.busy": "2024-02-21T12:37:02.971038Z",
     "iopub.status.idle": "2024-02-21T12:37:02.981567Z",
     "shell.execute_reply": "2024-02-21T12:37:02.980435Z"
    },
    "papermill": {
     "duration": 0.024982,
     "end_time": "2024-02-21T12:37:02.983540",
     "exception": false,
     "start_time": "2024-02-21T12:37:02.958558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model_lora=copy.deepcopy(model)\n",
    "\n",
    "model_lora.layers[0]=LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
    "model_lora.layers[2]=LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
    "model_lora.layers[4]=LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
    "model_lora.to(DEVICE)\n",
    "optimizer_lora=torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
    "print(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cb4b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:37:03.007738Z",
     "iopub.status.busy": "2024-02-21T12:37:03.007404Z",
     "iopub.status.idle": "2024-02-21T12:37:05.239726Z",
     "shell.execute_reply": "2024-02-21T12:37:05.238718Z"
    },
    "papermill": {
     "duration": 2.247039,
     "end_time": "2024-02-21T12:37:05.241973",
     "exception": false,
     "start_time": "2024-02-21T12:37:02.994934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy orig model:97.21%\n",
      "Test accuracy LoRA model:97.21%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy orig model:{compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy LoRA model:{compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05ccb8",
   "metadata": {
    "papermill": {
     "duration": 0.011424,
     "end_time": "2024-02-21T12:37:05.265274",
     "exception": false,
     "start_time": "2024-02-21T12:37:05.253850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Freezing the Original Linear Layers\n",
    "\n",
    "Then, we can freeze the original $Lienar$ layers and only make the $LoRALayer$ layers trainable, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268a4dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:37:05.289687Z",
     "iopub.status.busy": "2024-02-21T12:37:05.289344Z",
     "iopub.status.idle": "2024-02-21T12:37:05.296371Z",
     "shell.execute_reply": "2024-02-21T12:37:05.295418Z"
    },
    "papermill": {
     "duration": 0.021731,
     "end_time": "2024-02-21T12:37:05.298550",
     "exception": false,
     "start_time": "2024-02-21T12:37:05.276819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight:False\n",
      "layers.0.linear.bias:False\n",
      "layers.0.lora.A:True\n",
      "layers.0.lora.B:True\n",
      "layers.2.linear.weight:False\n",
      "layers.2.linear.bias:False\n",
      "layers.2.lora.A:True\n",
      "layers.2.lora.B:True\n",
      "layers.4.linear.weight:False\n",
      "layers.4.linear.bias:False\n",
      "layers.4.lora.A:True\n",
      "layers.4.lora.B:True\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "        else:\n",
    "            # recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(model_lora)\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f'{name}:{param.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0fb41",
   "metadata": {
    "papermill": {
     "duration": 0.011625,
     "end_time": "2024-02-21T12:37:05.322034",
     "exception": false,
     "start_time": "2024-02-21T12:37:05.310409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Based on the True and False values above, we can visually confirm that only the LoRA layers are trainble now(**True means trainable, False means frozen**). In practice, we would then train the network with this LoRA configuration on a new dataset or task. Before we do this, let understand DoRA first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99aac994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:37:05.346756Z",
     "iopub.status.busy": "2024-02-21T12:37:05.346433Z",
     "iopub.status.idle": "2024-02-21T12:39:31.969171Z",
     "shell.execute_reply": "2024-02-21T12:39:31.968126Z"
    },
    "papermill": {
     "duration": 146.637395,
     "end_time": "2024-02-21T12:39:31.971240",
     "exception": false,
     "start_time": "2024-02-21T12:37:05.333845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010|Batch 000/938| Loss: 0.0239\n",
      "Epoch: 001/010|Batch 400/938| Loss: 0.0857\n",
      "Epoch: 001/010|Batch 800/938| Loss: 0.0432\n",
      "Epoch: 001/010 training accuracy: 98.47%\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 002/010|Batch 000/938| Loss: 0.0381\n",
      "Epoch: 002/010|Batch 400/938| Loss: 0.0532\n",
      "Epoch: 002/010|Batch 800/938| Loss: 0.0172\n",
      "Epoch: 002/010 training accuracy: 98.68%\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 003/010|Batch 000/938| Loss: 0.0049\n",
      "Epoch: 003/010|Batch 400/938| Loss: 0.0515\n",
      "Epoch: 003/010|Batch 800/938| Loss: 0.0338\n",
      "Epoch: 003/010 training accuracy: 99.07%\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 004/010|Batch 000/938| Loss: 0.0116\n",
      "Epoch: 004/010|Batch 400/938| Loss: 0.0180\n",
      "Epoch: 004/010|Batch 800/938| Loss: 0.3807\n",
      "Epoch: 004/010 training accuracy: 98.66%\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 005/010|Batch 000/938| Loss: 0.0021\n",
      "Epoch: 005/010|Batch 400/938| Loss: 0.0014\n",
      "Epoch: 005/010|Batch 800/938| Loss: 0.0160\n",
      "Epoch: 005/010 training accuracy: 98.88%\n",
      "Time elapsed: 1.21 min\n",
      "Epoch: 006/010|Batch 000/938| Loss: 0.0939\n",
      "Epoch: 006/010|Batch 400/938| Loss: 0.0458\n",
      "Epoch: 006/010|Batch 800/938| Loss: 0.0204\n",
      "Epoch: 006/010 training accuracy: 98.64%\n",
      "Time elapsed: 1.45 min\n",
      "Epoch: 007/010|Batch 000/938| Loss: 0.0024\n",
      "Epoch: 007/010|Batch 400/938| Loss: 0.0251\n",
      "Epoch: 007/010|Batch 800/938| Loss: 0.0282\n",
      "Epoch: 007/010 training accuracy: 99.06%\n",
      "Time elapsed: 1.70 min\n",
      "Epoch: 008/010|Batch 000/938| Loss: 0.0073\n",
      "Epoch: 008/010|Batch 400/938| Loss: 0.0046\n",
      "Epoch: 008/010|Batch 800/938| Loss: 0.0077\n",
      "Epoch: 008/010 training accuracy: 98.97%\n",
      "Time elapsed: 1.94 min\n",
      "Epoch: 009/010|Batch 000/938| Loss: 0.0101\n",
      "Epoch: 009/010|Batch 400/938| Loss: 0.0064\n",
      "Epoch: 009/010|Batch 800/938| Loss: 0.0004\n",
      "Epoch: 009/010 training accuracy: 98.82%\n",
      "Time elapsed: 2.18 min\n",
      "Epoch: 010/010|Batch 000/938| Loss: 0.0039\n",
      "Epoch: 010/010|Batch 400/938| Loss: 0.0006\n",
      "Epoch: 010/010|Batch 800/938| Loss: 0.0127\n",
      "Epoch: 010/010 training accuracy: 99.06%\n",
      "Time elapsed: 2.42 min\n",
      "Total Training Time: 2.42 min\n",
      "Test accuracy LoRA finetune: 97.46%\n"
     ]
    }
   ],
   "source": [
    "optimizer_lora=torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
    "train(num_epochs, model_lora, optimizer_lora, train_loader, DEVICE)\n",
    "print(f'Test accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f12610f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T12:39:32.001982Z",
     "iopub.status.busy": "2024-02-21T12:39:32.001675Z",
     "iopub.status.idle": "2024-02-21T12:39:34.165419Z",
     "shell.execute_reply": "2024-02-21T12:39:34.164486Z"
    },
    "papermill": {
     "duration": 2.180853,
     "end_time": "2024-02-21T12:39:34.167567",
     "exception": false,
     "start_time": "2024-02-21T12:39:31.986714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy orig model:97.21%\n",
      "Test accuracy LoRA model:97.46%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy orig model:{compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy LoRA model:{compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da6756",
   "metadata": {
    "papermill": {
     "duration": 0.013882,
     "end_time": "2024-02-21T12:39:34.195707",
     "exception": false,
     "start_time": "2024-02-21T12:39:34.181825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credit\n",
    "\n",
    "* https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch\n",
    "* https://arxiv.org/abs/2402.09353\n",
    "* https://arxiv.org/abs/2106.09685\n",
    "* https://github.com/rasbt/dora-from-scratch"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 303.841948,
   "end_time": "2024-02-21T12:39:35.430398",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T12:34:31.588450",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
