{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721baea5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003022,
     "end_time": "2024-01-31T01:27:24.760336",
     "exception": false,
     "start_time": "2024-01-31T01:27:24.757314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "Data does not always come in its final processed form that is required for training machine learning algorithms. We use `transformers` to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "For example, the FashionMNIST features are in `PIL Image format`, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use `ToTensor` and `Lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be051c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T01:27:24.767709Z",
     "iopub.status.busy": "2024-01-31T01:27:24.767071Z",
     "iopub.status.idle": "2024-01-31T01:27:35.891087Z",
     "shell.execute_reply": "2024-01-31T01:27:35.890207Z"
    },
    "papermill": {
     "duration": 11.133003,
     "end_time": "2024-01-31T01:27:35.896083",
     "exception": false,
     "start_time": "2024-01-31T01:27:24.763080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 16571768.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 298336.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 5474026.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 11876940.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds=datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(\n",
    "        lambda y: torch.zeros(10, dtype=torch.float).scatter_(0,torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75480d5",
   "metadata": {
    "papermill": {
     "duration": 0.01313,
     "end_time": "2024-01-31T01:27:35.923411",
     "exception": false,
     "start_time": "2024-01-31T01:27:35.910281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ToTensor()\n",
    "\n",
    "ToTensor converts a PIL image or NumPy `ndarray` into a `FloatTensor` and scales the image's pixel intensity values in the range [0,1].\n",
    "\n",
    "\n",
    "## Lambda Transforms\n",
    "\n",
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10(the number of labels in our dataset) and calls `scatter_` which assigns a `value=1` on the index as given by the label `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b3b876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T01:27:35.941983Z",
     "iopub.status.busy": "2024-01-31T01:27:35.941488Z",
     "iopub.status.idle": "2024-01-31T01:27:35.946329Z",
     "shell.execute_reply": "2024-01-31T01:27:35.945434Z"
    },
    "papermill": {
     "duration": 0.013413,
     "end_time": "2024-01-31T01:27:35.948122",
     "exception": false,
     "start_time": "2024-01-31T01:27:35.934709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_transform=Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa1b8e",
   "metadata": {
    "papermill": {
     "duration": 0.005091,
     "end_time": "2024-01-31T01:27:35.958642",
     "exception": false,
     "start_time": "2024-01-31T01:27:35.953551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transforming and Agumenting Images\n",
    "\n",
    "Torchvision supports common computer vision transformations in the `torchvision.transforms` and `torchvision.transforms.v2` modules. Transforms can be used to trainsform or augment data for training or inference of different tasks, like:\n",
    "\n",
    "* image classification\n",
    "* detection\n",
    "* segmentation\n",
    "* video classification\n",
    "\n",
    "Most transformation accept both PIL images and tensot inputs. Both CPU and CUDA tensors are supported. However, we use tensor backend **for performance**.\n",
    "\n",
    "**Tensor image**\n",
    "\n",
    "Tensor image are expected to be of shape (C,H,W), where C is the number of channels, and H and W refer to height and width. Most transforms support batched tensor input. A batch of Tensor images is a tensor of shape(N,C,H,W), where N is a number of images in the batch. The v2 transforms generally accept an arbitrary number of leading dimensions(...,C,H,W) and can handle batched images or batched videos.\n",
    "\n",
    "**Dtype and expected value range**\n",
    "\n",
    "The expected range of the value of a tensor image is impliicity defined by the tensor dtype. Tensir images with a float dtype are expected to have values in [0,1]. Tensor images with an integer dtype expected to have values in [0, MAX_DTYPE] where MAX_DTYPE is the largest value that can be represented in that dtype. Typically, images of dtype `torch.unit8` are expected to have values in [0,255]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74413d50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T01:27:35.971042Z",
     "iopub.status.busy": "2024-01-31T01:27:35.970406Z",
     "iopub.status.idle": "2024-01-31T01:27:36.114934Z",
     "shell.execute_reply": "2024-01-31T01:27:36.113780Z"
    },
    "papermill": {
     "duration": 0.153265,
     "end_time": "2024-01-31T01:27:36.117194",
     "exception": false,
     "start_time": "2024-01-31T01:27:35.963929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7933,  0.7933,  0.7933,  ..., -1.0562, -1.0562, -1.0562],\n",
       "         [ 0.7933,  0.7933,  0.7933,  ..., -1.0562, -1.0562, -1.0562],\n",
       "         [ 0.7933,  0.7933,  0.7933,  ..., -1.0562, -1.0562, -1.0562],\n",
       "         ...,\n",
       "         [-0.1828, -0.1828, -0.1828,  ..., -0.9020, -0.9020, -0.9020],\n",
       "         [-0.1828, -0.1828, -0.1828,  ..., -0.9020, -0.9020, -0.9020],\n",
       "         [-0.1828, -0.1828, -0.1828,  ..., -0.9020, -0.9020, -0.9020]],\n",
       "\n",
       "        [[ 0.1176,  0.1176,  0.1176,  ...,  2.2010,  2.2010,  2.2010],\n",
       "         [ 0.1176,  0.1176,  0.1176,  ...,  2.2010,  2.2010,  2.2010],\n",
       "         [ 0.1176,  0.1176,  0.1176,  ...,  2.2010,  2.2010,  2.2010],\n",
       "         ...,\n",
       "         [-0.1450, -0.1450, -0.1450,  ...,  2.0609,  2.0609,  2.0609],\n",
       "         [-0.1450, -0.1450, -0.1450,  ...,  2.0609,  2.0609,  2.0609],\n",
       "         [-0.1450, -0.1450, -0.1450,  ...,  2.0609,  2.0609,  2.0609]],\n",
       "\n",
       "        [[-1.3861, -1.3861, -1.3861,  ..., -0.7936, -0.7936, -0.7936],\n",
       "         [-1.3861, -1.3861, -1.3861,  ..., -0.7936, -0.7936, -0.7936],\n",
       "         [-1.3861, -1.3861, -1.3861,  ..., -0.7936, -0.7936, -0.7936],\n",
       "         ...,\n",
       "         [ 1.8557,  1.8557,  1.8557,  ...,  1.2108,  1.2108,  1.2108],\n",
       "         [ 1.8557,  1.8557,  1.8557,  ...,  1.2108,  1.2108,  1.2108],\n",
       "         [ 1.8557,  1.8557,  1.8557,  ...,  1.2108,  1.2108,  1.2108]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image Classitication\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "H,W=32,32\n",
    "# here we use tensor image\n",
    "img=torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\n",
    "\n",
    "transforms=v2.Compose(\n",
    "    [\n",
    "        # Resize(antialias=True)\n",
    "        v2.RandomResizedCrop(size=(224,224), antialias=True),\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        # Normalize expects float input\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img=transforms(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "886a7b43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T01:27:36.129925Z",
     "iopub.status.busy": "2024-01-31T01:27:36.129501Z",
     "iopub.status.idle": "2024-01-31T01:27:36.155628Z",
     "shell.execute_reply": "2024-01-31T01:27:36.154549Z"
    },
    "papermill": {
     "duration": 0.035213,
     "end_time": "2024-01-31T01:27:36.158082",
     "exception": false,
     "start_time": "2024-01-31T01:27:36.122869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[-4.6361, -4.7707, -4.9725,  ..., -0.5351,  0.0146, -1.1962],\n",
       "          [-3.3942, -3.5753, -3.8254,  ..., -0.7680, -0.3320, -1.4032],\n",
       "          [-1.9681, -2.2026, -2.5081,  ..., -1.0356, -0.7337, -1.6773],\n",
       "          ...,\n",
       "          [-7.2151, -7.3478, -7.5477,  ..., -0.4729, -1.1234, -1.0871],\n",
       "          [-7.1326, -7.1792, -7.2897,  ..., -0.7800, -1.4644, -1.2884],\n",
       "          [-7.0291, -7.0291, -7.0914,  ..., -1.0473, -1.7644, -1.4954]],\n",
       " \n",
       "         [[-1.9013, -0.7061,  0.5336,  ..., -7.6463, -8.2241, -8.1538],\n",
       "          [-2.7125, -1.6633, -0.5750,  ..., -6.5285, -6.9479, -7.0722],\n",
       "          [-3.6441, -2.7625, -1.8480,  ..., -5.2447, -5.4785, -5.7920],\n",
       "          ...,\n",
       "          [ 0.8555,  0.4355, -0.0317,  ...,  1.4106,  1.3505,  0.8279],\n",
       "          [-0.2004, -0.4869, -0.8242,  ...,  0.8549,  0.7226,  0.4793],\n",
       "          [-1.1197, -1.2603, -1.5363,  ...,  0.4042,  0.2089,  0.2089]],\n",
       " \n",
       "         [[ 2.2600,  1.2148,  0.1307,  ...,  0.4651,  0.0136,  0.0136],\n",
       "          [ 2.4745,  1.4774,  0.3987,  ..., -0.2674, -0.6296, -0.6296],\n",
       "          [ 2.6830,  1.7413,  0.7002,  ..., -1.0835, -1.3984, -1.3306],\n",
       "          ...,\n",
       "          [-3.6814, -3.7154, -3.7174,  ...,  2.9783,  2.6137,  1.6739],\n",
       "          [-4.6048, -4.5833, -4.4966,  ...,  2.7155,  2.2720,  1.3661],\n",
       "          [-5.4088, -5.3391, -5.2023,  ...,  2.4921,  2.0040,  1.0981]]]),\n",
       " 'boxes': BoundingBoxes([[170,  40, 219, 112],\n",
       "                [153,   0, 219,   0],\n",
       "                [211,  30, 211,  75]], format=BoundingBoxFormat.XYXY, canvas_size=(224, 224))}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detection\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "img=torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\n",
    "boxes=torch.randint(0, H//2, size=(3,4))\n",
    "boxes[:, 2:]+=boxes[:,:2]\n",
    "boxes=tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(H,W))\n",
    "\n",
    "# the same transforms can be used\n",
    "img, boxes=transforms(img, boxes)\n",
    "# and you can pass arbitary input structures\n",
    "output_dict=transforms({\"image\": img, \"boxes\": boxes})\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabbe46f",
   "metadata": {
    "papermill": {
     "duration": 0.005465,
     "end_time": "2024-01-31T01:27:36.169206",
     "exception": false,
     "start_time": "2024-01-31T01:27:36.163741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html\n",
    "* https://pytorch.org/vision/stable/transforms.html\n",
    "* https://pytorch.org/vision/stable/auto_examples/transforms/plot_custom_transforms.html#sphx-glr-auto-examples-transforms-plot-custom-transforms-py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.245956,
   "end_time": "2024-01-31T01:27:37.396520",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-31T01:27:22.150564",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
