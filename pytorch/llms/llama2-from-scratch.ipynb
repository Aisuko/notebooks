{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nLlama2, like the original Llama model, is based on the Google transformer architecture, with improvements. Llama's improvements include\n\n* RMSNorm pre-normalization, inspired by GPT-3\n* SwiGLU activation function, inspired by Google's PaLM\n* [Multi-query attention](https://arxiv.org/abs/1911.02150) instead of multi-head attention\n* Rotary positional embeddings(RoPE), inspired by GPT Neo\n\nLlama training used the [AdamW](https://arxiv.org/abs/1711.05101) optimizer. Llama2's primary differences from Llama are increased context length (4096 vs. 2048 tokens) and [grouped-query attention(GQA)](https://arxiv.org/abs/2305.13245) instead of [multi-query attention(MQA)](https://arxiv.org/abs/1911.02150) in the two larger models.\n\n\n# Architecture \n\n> The components that we need to implement\n\nAccording to the paper, LLAMA is based on transformer architecture.\n\n* RMSNorm\n* SwiGLU\n* RoPE\n* Multi-Query-Attention","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.37.2\n!pip install datasets==2.17.0\n!pip install sentencepiece==0.1.99","metadata":{"execution":{"iopub.status.busy":"2024-02-17T04:07:46.477023Z","iopub.execute_input":"2024-02-17T04:07:46.477376Z","iopub.status.idle":"2024-02-17T04:08:36.924309Z","shell.execute_reply.started":"2024-02-17T04:07:46.477348Z","shell.execute_reply":"2024-02-17T04:08:36.922948Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Root Mean Square Layer Normalization(RMSNorm)\n\nLLaMA2 normalizes the input of each transformer sub-layer, instead of normalizing the output. RMSNorm is extension of Layer Normalization(LayerNorm). Reason behind using RMSNorm is the computational overhead in LayerNorm. This makes improvements slow and expensive. RMSNorm achieves comparable performance against LayerNorm but reduces the runing time. For the LayerNorm, it has two properties.\n\n**Re-centring**\n\nIt makes model insensitive to shift noises on both input and weights.\n\n**Re-scaling**\n\nIt keeps the output representations intact when both inputs and weighs are randomly scaled. RMSNorm claims that most of the benefits comes from re-scaling.\n\nRMSNorm does re-scaling invariance and regularizes the summed inputs simply according to the root mean square(RMS) statistic. RMSNorm simplifies LayerNorm by totally removing the mean statistic in Eq. at the cost of sacrificing the invariance that mean normalization affords. When the mean of summed inputs is zero, RMSNorm is exactly equal to LayerNoFm. Although RMSNorm does not re-center.\n\nJust like Layer Normalization, we also have a learnable parameter `gamma` $g$ that is multiplied by the normalized values.\n\n$$\\bar{a_i}=\\frac{a_i}{RMS(a)}*g_{i}, where RMS(a)=\\sqrt{\\frac{1}{n}*\\sum_{i=1}^n*a_i^2}$$\n\nThis custom scripts first standardizes the input `x`, by dividing it by its root mean square, thereby making it invariant to scaling changes. The learned weight parameter `self.weight` is applied to each element in the standarized tensor. This operation ajusts the magnitude of the values based on the learned scaling factor.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass RMSnorm(nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.eps=eps\n        # the gamma parameter\n        self.weight=nn.Parameter(torch.ones(dim))\n    \n    def _norm(self, x:torch.Tensor):\n        # (B, seq_len, dim)*(B, seq_len,1)=(B, seq_len, dim)\n        # rsqrt: 1/sqrt(x)\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True)+self.eps)\n    \n    def forward(self, x:torch.Tensor):\n        # weight is a gain parameter used to re-scale the standardized summed inputs\n        return self.weight*self._norm(x.float()).type_as(x)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T04:09:33.756139Z","iopub.execute_input":"2024-02-17T04:09:33.756747Z","iopub.status.idle":"2024-02-17T04:09:33.763916Z","shell.execute_reply.started":"2024-02-17T04:09:33.756715Z","shell.execute_reply":"2024-02-17T04:09:33.762634Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Rotary Positional Embeddings(RoPE)\n\n**What's the difference between the absolute positional encodings and the relative ones?**\n\n1. **Absolute positional encodings** are fixed vectors that are added to the embedding of a token represent its absolute position in the sentence. So, it deals with one token at a time. You can think of it as the pair(latitude, longitude) on a map: each point on earth will have a unique pair.\n\n$$e_{ij}=\\frac{(x_{i}*W^2)(x_{i}*W^K)^T}{\\sqrt{d_{z}}}$$\n\n2. **Relative positional encodings**, on the other hand, deals with two toknes at a time and it is involved when we calculate the attention; since the attention machnism captures the \"intensity\" of how much two words are related two each other, relative positional encodings tells the attention mechanism the distance between the two words involved in it. So, given two tokens, we creat a vector that represents their distance.\n\n$$e_{ij}=\\frac{x_iW^Q*(x_j*W^K+a_{ij}^K)^T}{\\sqrt{d_z}}$$\n\n\n**What are Rotary Positional Embeddings?**\n\nRoPE is a way to encode positional information in natural langauge processing models. This type of position embedding uses a rotation matrix to include explicit relative position dependency in self-attention formulation.","metadata":{}}]}