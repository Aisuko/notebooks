{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nLlama2, like the original Llama model, is based on the Google transformer architecture, with improvements. Llama's improvements include\n\n* RMSNorm pre-normalization, inspired by GPT-3\n* SwiGLU activation function, inspired by Google's PaLM\n* [Multi-query attention](https://arxiv.org/abs/1911.02150) instead of multi-head attention\n* Rotary positional embeddings(RoPE), inspired by GPT Neo\n\nLlama training used the [AdamW](https://arxiv.org/abs/1711.05101) optimizer. Llama2's primary differences from Llama are increased context length (4096 vs. 2048 tokens) and [grouped-query attention(GQA)](https://arxiv.org/abs/2305.13245) instead of [multi-query attention(MQA)](https://arxiv.org/abs/1911.02150) in the two larger models.\n\n\n# The components are need to implement\n\n* RMSNorm\n* SwiGLU\n* RoPE\n* Transformer architecture with Multi-Query-Attention","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.37.2\n!pip install datasets==2.17.0\n!pip install sentencepiece==0.1.99","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Root Mean Square Layer Normalization(RMSNorm)\n\nLLaMA2 normalizes the input of each transformer sub-layer, instead of normalizing the output. RMSNorm is extension of Layer Normalization(LayerNorm). Reason behind using RMSNorm is the computational overhead in LayerNorm. This makes improvements slow and expensive. RMSNorm achieves comparable performance against LayerNorm but reduces the runing time. For the LayerNorm, it has two properties.\n\n**Re-centring**\n\nIt makes model insensitive to shift noises on both input and weights.\n\n**Re-scaling**\n\nIt keeps the output representations intact when both inputs and weighs are randomly scaled. RMSNorm claims that most of the benefits comes from re-scaling.\n\nRMSNorm does re-scaling invariance and regularizes the summed inputs simply according to the root mean square(RMS) statistic.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.nn import nn\n\nclass RMSnorm(nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.eps=eps\n        self.weight=nn.Parameter(torch.ones(dim))\n    \n    def _norm(self, x:torch.Tensor):\n        # (m, seq_len, dim)*(m.seq_len,1)","metadata":{},"execution_count":null,"outputs":[]}]}