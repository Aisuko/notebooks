{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/represent-words-with-embeddings?scriptVersionId=164507476\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn the [Representing Text as PyTorch Tensors](https://www.kaggle.com/code/aisuko/representing-text-as-pytorch-tensors) notebook, we operated on high-dimensional bag-of-words vectors with the length of `vocab_size`, and we were explicity converting from low-dimentional positional representation vectors into sparse one-hot representations. Here is an example for embedding classifier:\n\nThe goal of using word embeddings and reducing the dimensionality are:\n* Finding the meaning of words based on their word approximation to other words. This is done by taken two word vectors and analyzing how often the words in the vectors are used together. The higher the frequency, the more you can find a correlation and relationship between the words.\n* This process of training the word embeddings to find word approximations between words in a given dimension is how we reduce the word representation of low-dimensions.\n* Embedding vectors serve as numeric representations of words and are used as input to other machine learning network layers.\n* The embedding vector becomes the stored lookup table for words in the vocabulary.\n\nHere we are continue exploring the **News AG** dataset. To begin, let's load the data and get some definitions.","metadata":{}},{"cell_type":"code","source":"!pip install portalocker==2.8.2\n!pip install gensim==4.3.2","metadata":{"execution":{"iopub.status.busy":"2024-02-27T08:07:02.646407Z","iopub.execute_input":"2024-02-27T08:07:02.646805Z","iopub.status.idle":"2024-02-27T08:07:33.649014Z","shell.execute_reply.started":"2024-02-27T08:07:02.646771Z","shell.execute_reply":"2024-02-27T08:07:33.64769Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting portalocker==2.8.2\n  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker\nSuccessfully installed portalocker-2.8.2\nCollecting gensim==4.3.2\n  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim==4.3.2) (1.23.5)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim==4.3.2) (1.11.1)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim==4.3.2) (6.3.0)\nInstalling collected packages: gensim\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.1\n    Uninstalling gensim-4.3.1:\n      Successfully uninstalled gensim-4.3.1\nSuccessfully installed gensim-4.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport warnings\n\nif torch.cuda.is_available():\n    torch_device = 'cuda'\nelse:\n    torch_device = 'cpu'\n\nwarnings.filterwarnings('ignore')\n\nprint(torch_device)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T08:07:37.999821Z","iopub.execute_input":"2024-02-27T08:07:38.000506Z","iopub.status.idle":"2024-02-27T08:07:38.010308Z","shell.execute_reply.started":"2024-02-27T08:07:38.000471Z","shell.execute_reply":"2024-02-27T08:07:38.009079Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dealing with Variable Sequence Size\n\nWhen working with words, you are going to have text sequences or sentences that are of different lengths. This can be problematic in training the word embeddings neural network. For consistency in the word embedding and improve training performance, we would have to apply some padding. This can be done using the `torch.nn.functional.pad` on a tokenized dataset. It adds zero values to the empty indices at the end of the vector.","metadata":{}},{"cell_type":"code","source":"import torchtext\nfrom torchtext.data import get_tokenizer\n\n# Loading dataset\ntrain_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\ntrain_dataset, test_dataset = list(train_dataset), list(test_dataset)\nclasses = ['World','Sports','Business','Sci/Tech']\n\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')","metadata":{"execution":{"iopub.status.busy":"2024-02-27T08:07:38.013628Z","iopub.execute_input":"2024-02-27T08:07:38.014154Z","iopub.status.idle":"2024-02-27T08:07:46.070517Z","shell.execute_reply.started":"2024-02-27T08:07:38.014116Z","shell.execute_reply":"2024-02-27T08:07:46.069196Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Building a Vocabulary of All Tokens\n\nWe first build the dictionary using the `Counter` object, and then create `Vocab` object that would help us deal with vectorization. Let's use the first 2 sentences as example to view the text length difference and effects of padding.","metadata":{}},{"cell_type":"code","source":"import collections\n\n# Building vocab\ndef build_vocab(train_dataset,ngrams=1,min_freq=1):\n    counter = collections.Counter()\n    for (label, line) in train_dataset:\n        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line), ngrams=ngrams))\n    vocab = torchtext.vocab.vocab(counter, min_freq=min_freq)\n    return vocab\n\ndef encode(x, voc=None,tokenizer=tokenizer):\n    v =vocab if not voc else voc\n    return [v.get_stoi()[s] for s in tokenizer(x)]\n\ndef padify(b):\n    # b is the list of tuples of length batch_size\n    # - first element of a tuple = label\n    # - second = feature (text, sequence)\n    # build vectorized sequence\n    v = [encode(x[1]) for x in b]\n    # first, compute max length of a sequnce in this minibatch\n    l = max(map(len,v))\n    return (\n        # tuple of two tensors - labels and features\n        torch.LongTensor([t[0]-1 for t in b]),\n        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)), mode='constant', value=0) for t in v])\n    )\n\nvocab = build_vocab(train_dataset,ngrams=1,min_freq=1)\n\n\nfirst_sentence = train_dataset[0][1]\nsecond_sentence = train_dataset[1][1]\n\nf_tokens = encode(first_sentence)\ns_tokens = encode(second_sentence)\n\nprint(f'First sentence in dataset:\\n{first_sentence}')\nprint('Length:', len(train_dataset[0][1]), '\\n')\nprint(f'\\nSecond sentence in dataset:\\n{second_sentence}')\nprint('Length:', len(train_dataset[1][1]), '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-02-27T08:07:46.07174Z","iopub.execute_input":"2024-02-27T08:07:46.072125Z","iopub.status.idle":"2024-02-27T08:08:00.181572Z","shell.execute_reply.started":"2024-02-27T08:07:46.072092Z","shell.execute_reply":"2024-02-27T08:08:00.180222Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"First sentence in dataset:\nWall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\nLength: 144 \n\n\nSecond sentence in dataset:\nCarlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\nLength: 266 \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's use the text sequence from the news article headlines in our dataset to change into a tokenize vector. As you will see, the text sequence have different lengths. We will apply padding so all the text sequence wil have a fixed length. This approach is used when you have a large set of text sequences in your dataset.\n* The length of the 1st and 2nd sentences displayed have difference lengths.\n* The max length of the dataset tensors is the length of the longest sentence length in the entire dataset.\n* The zeros are added to the empty indexes in the tensor.","metadata":{}},{"cell_type":"code","source":"vocab_size =len(vocab)\nlabels, features = padify(train_dataset)\nprint(f'features:{features}')\n\nprint(f'\\nlength of first sentence: {len(f_tokens)}')\nprint(f'length of second sentence: {len(s_tokens)}')\nprint(f'size of features: {features.size()}')","metadata":{"execution":{"iopub.status.busy":"2024-02-27T08:08:00.183666Z","iopub.execute_input":"2024-02-27T08:08:00.184505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What is embedding?\n\n**Note: Images are from Microsoft Learning**\n\nThe idea of `embedding` is the process of mapping words into vectors, which reflects the `semantic meaning of a word`. The length of its vectors are the embedding dimensions size. We will later discuss how to build meningful word embeddings, but for now let's just think of embeddings as a way to lower dimensionality of a word vector. So, embedding layer would take a word as an input, and produce an output vector of specified `embedding_size`. In a sense, it is very similar to `Linear` layer, but instead of taking one-hot encoded vector, it will be able to take a word number as an input.\n\nBy using embedding layer as a first layer in our network, we can switch from bag-or-words to `embedding bag` model, where we first convert each word in our text into corresponding embedding, and then compute some aggregate function over all those embeddings, such as `sum`, `avergae` or `max`.\n\n![](https://hostux.social/system/media_attachments/files/110/750/619/792/006/183/original/94496909c3a93b6e.png)\n\n\n# Define Embedding Classifier\n\nThis time our classifier neural network will start with an embedding layer, then aggregation layer, and a linear classifier on top of it:\n* `vocab_size` are the size of the total number of words we have in our vocabulary\n* `embed_dim` are the length of the word dimensions that show relationship between words passed as in the network\n* `num_class` are the number of new categories we are trying to classify (e.g. Word, Sports, Business, Sci/Tech)","metadata":{}},{"cell_type":"code","source":"class EmbedClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = torch.nn.Linear(embed_dim, num_class)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.mean(x, dim=1) # torch.mean() computes the mean of all elements in a tensor, it is a reduction operation\n        return self.fc(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Embedding Classifer\n\nNow we'll define our training dataloader and use the `collate_fn` to apply the padify function to the datasets as they loaded in each batch. As a result, the training dataset will be padded. We can train the model using the training function to run the embedding network. The training output serves as a vector lookup store based on the unique index tokens from the vocabulary.\n\nWe are only training for 25k records here (less than one full epoch) for the sake of time, but you can continue training, write a function to train for several epochs, and experiement with learning rate parameter to achieve higher accuracy. You should be able to go to the accuracy of about 90%.","metadata":{}},{"cell_type":"code","source":"def train_eopch(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(), epoch_size=None, report_freq=200):\n    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n    net.train()\n    total_loss, acc, count,i = 0,0,0,0\n    for labels, features in dataloader:\n        labels, features = labels.to(torch_device), features.to(torch_device)\n        optimizer.zero_grad()\n        output = net(features)\n        loss = loss_fn(output, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss\n        _, predicted = torch.max(output, 1)\n        acc += (predicted == labels).sum()\n        count += len(labels)\n        i+=1\n        if i%report_freq == 0:\n            print(f'iteration {count}, loss {total_loss.item()/count}, accuracy {acc.item()/count}') # item() is used to get the value of a tensor\n        if epoch_size  and count >= epoch_size:\n            break\n    \n    return total_loss.item()/count, acc.item()/count\n\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=padify)\n\nnet = EmbedClassifier(vocab_size, embed_dim=32, num_class=len(classes)).to(torch_device)\n\ntrain_eopch(net, train_loader,lr=1, epoch_size=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EmbeddingBag Layer and VariableLength SequnceRepresentation\n\nIn the previous architecture, we need to pad all sequences to the same length in order to fit them into a minibatch. This is not the most efficient way to represent variable length sequynces - another approach would be to use `offset` vector, which would hold offsets to all sequences stored in one large vector.\n\n![](https://hostux.social/system/media_attachments/files/110/751/844/099/559/089/original/c67bdb8c6e8afa7f.png)\n\nOn the picture above, we show a sequence of characters, but in our example we are working with sequences of words. However, the general principle of representing sequences with offset vector remains the same. To work with offset representation, we use PyTorch's `EmbeddingBag` layer. It is similar to `Embedding`, but it takes content vector and offset vector as input, and it also includes averaging layer, which can be `mean`, `sum` or `max`. Here is modified network that uses `EmbeddingBag` layer:\n","metadata":{}},{"cell_type":"code","source":"class EmbedClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) # sparse=True is used to save memory\n        self.fc = torch.nn.Linear(embed_dim, num_class)\n    \n    def forward(self, text, off):\n        x = self.embedding(text, off)\n        return self.fc(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To prepare the dataset for training, we need to provide a conversition funciton that will prepare the offset vector. The offset vector is calculated by first combining the sentences indices into one tensor sequence, then extracting the staring index location of each sentence in the sequence. For example:\n* The length of the first sentence in our training dataset is 29. Meaning the first index of the offset will 0\n* The length of the second sentence in the dataset is 42. Meaning the second index of the offset of will be 29, where the first sentence ended.\n* The third index of the offset will be 29 +42=71, where the 2nd sentence ended.","metadata":{}},{"cell_type":"code","source":"def offsetify(b):\n    # first, compute data tensot from all sequences\n    x = [torch.tensor(encode(t[1])) for t in b]\n    # now, compute the offsets by accumulating the tensor of sequence lengths\n    o = [0] + [len(t) for t in x]\n    o = torch.tensor(o[:-1]).cumsum(dim=0)\n    return (\n        torch.LongTensor([t[0]-1 for t in b]), # labels\n        torch.cat(x), # text\n        o\n    )\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=offsetify)\n\nlabels, features, offset = offsetify(train_dataset)\nprint(f'offset:{offset}')\nprint(f'\\nlength of first sentence: {len(f_tokens)}')\nprint(f'length of second sentence: {len(s_tokens)}')\nprint(f'size of data vector: {features.size()}')\nprint(f'size of offset vector: {offset.size()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: that unlike in all previous example, our network now accepts two parameters: data vector and offset vector, which are of different sizes. Similarly, our data loader also provides us with 3 values instead of 2: both text and offset vectors are provided as features. Therefore, we need to slightly adjust our training function to take care of that:","metadata":{}},{"cell_type":"code","source":"net = EmbedClassifier(vocab_size,32,len(classes)).to(torch_device)\n\ndef train_epoch_emb(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n    optimizer  = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n    loss_fn = loss_fn.to(torch_device)\n    net.train()\n    total_loss, acc, count, i = 0,0,0,0\n    for labels, text,off in dataloader:\n        optimizer.zero_grad()\n        labels, text, off = labels.to(torch_device), text.to(torch_device), off.to(torch_device)\n        output = net(text, off)\n        loss = loss_fn(output, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss\n        _, predicted = torch.max(output,1) # torch.max() is used to get the max value of a tensor\n        acc += (predicted == labels).sum()\n        count += len(labels)\n        i += 1\n        if i%report_freq == 0:\n            print(f'iteration {count}, loss {total_loss.item()/count}, accuracy {acc.item()/count}')\n        if epoch_size and count >= epoch_size:\n            break\n    return total_loss.item()/count, acc.item()/count\n\ntrain_epoch_emb(net, train_loader, lr=4, epoch_size=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Semantic Embeddings: Word2Vec\n\nIn our previous example, the model embedding layer learnt to map words to vector representaion. However, this representaion did not have much semantical meaning. It would be nice to learn such vector representaion, that similar words ot symonims would correspond to vectors that are close to each other in terms of some vector distance (eg. euclidian distance).\n\nTo do that, we need to pre-train our embedding model on a large collection of text of in a specific way. One of the first ways to train semantic embeddings is called **Word2Vec**. It helps `map the probability of a word`, based on the contexts from texts in the sequence. It is based on two main architectures that are used to produce a distributed representation of words:\n* **Continuous Bag-of-Words** (CBOW) - in this architecture, we train the model to predict a word from surrounding context. Given the ngram ($W_{-2},W_{-1},W_{0},W_{1},W_{2}$), the goal of the model is to predict $W_{0}$ from ($W_{-2},W_{-1},W_{1},W_{2}$). For example, \"I like my hot dog on a _\". Here the predicted word would be \"bun\".\n* **Continuous Skip-Gram** is opposite to CBoW. The model uses surrounding window of context words to predict the current word. For example: you can predict `dog` to be more associated with the word `veterinary`.\n\nCBoW is faster, while skip-gram is slower, bu does a better job of representing infrequent words.\n\n![](https://hostux.social/system/media_attachments/files/110/755/104/068/247/744/original/157b705602d3e7c4.png)\n\nBoth CBOW and Skip-Grams are \"predictive\" embeddings, in that they only take local contexts into account. Word2Vec does not take advantage of global context.\n* **GloVe**(Global Vectors) - derives the relationship between words. It determines how often a particular word-pair occurs together in a text by leveraging the idea of co-occurence matrix and uses neural methods to decompose co-occurence matrix into more expressive and non-linear word vectors.\n* **FastText** - builds on Word2Vec by learning vector representations are then averaged into one vector at each training step. While this adds a lot of additional computation ro pre-training, it enables word embeddings to encode sub-word information.\n* **Gensim**(Generate Similar) - is an open source NLP Python library that provides a unified interface to build word vectors,  corpus, perform topic identification,and other NLP tasks.\n\nFastText and Glove are other word embedding techniques that predict the probably of words appearing together.\n\nIn our Word2Vec example, we'll using pre-trained semantic embeddings, but it is interesting to see how those embeddings can be trained using either FastText, CBoW or Skip-gram architectures. This exercise goes beyond this module, but those interested can reference Word Embeddings tutorials on PyTorch's website.\n\n\n## Genim\n\nThe `gensim` framework can be used with PyTorch to train most commonly used to embeddings in a few lines of code. To experiment with word2vec embedding pre-trained on Google News dataset, we can use the `gensim` library. Below we find the words that are most similar to `neural`\n\n**Note: When you first create word vectors, downloading them can take some time.**","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api\n\nw2v = api.load('word2vec-google-news-300')\nw2v","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at words that are similar to `dog`.","metadata":{}},{"cell_type":"code","source":"for w,p in w2v.most_similar('dog'):\n    print(f'{w} {p}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also extract vector embeddings from the word, to be used in training classification model(we'll only show first 20 components of the vector for clarity):","metadata":{}},{"cell_type":"code","source":"w2v.word_vec('play')[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The great thing about semantical embeddings is that you can manipulate vector encoding to change the semantics. For example, we can ask to find a word, whose vectot representation would be as close as possible to words `king` and `woman`, asn as far away from the word `man`:","metadata":{}},{"cell_type":"code","source":"w2v.most_similar(positive=['king','woman'],negative=['man'])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Pre-trained Embeddings in PyTorch\n\nWe can modify the example above to pre-populate the matrix in our embedding layer with semantical embeddings, such as Word2Vec. We need to take into account that vocabularies of pre-trained embedding are an addition to the existing text corpus that we already have so they will likely not match. As a result, we will initialize weights for the missing words with random values:","metadata":{}},{"cell_type":"code","source":"embed_size =len(w2v.get_vector('hello'))\nprint(f'Embedding size: {embed_size}')\n\nnet = EmbedClassifier(vocab_size, embed_size, len(classes)).to(torch_device)\n\nprint('Populating matrix, this will take some time...', end='')\nfound,not_found = 0,0\nfor i,w in enumerate(vocab.get_itos()):\n    try:\n        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n        found += 1\n    except:\n        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n        not_found += 1\n\nprint(f'Done, found {found} words, {not_found} words missing')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train our model. Note that the time it takes to train the model is significantly larger than in the provious example, due to larger embedding layer size, and thus much higher number of parameters. Also, because of this, we may need to train our model on more examples if we want to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"train_epoch_emb(net, train_loader, lr=4, epoch_size=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our csse, we do not see a huge increase in accuracy, which is likely due to the quite different vocabularies. To overcome the problem of different vocabularies, we can use one of the following solutions:\n\n* Re-train word2vec model on our vocabulary\n* Load our dataset with the vocabulary form the pre-trained word2vec model. Vocabulary used to load the dataset can be specified during loading.\n\nThe latter approach seems easier, especially because PyTorch `torchtext` framework contains built-in support for embeddings.\n\n\n# Glove Embeddings\n\nTo load our dataset wiht the covabulary from a pre-trained word2vec model,we use Glove embeddings. We'll start by instantiating Glove-based vocabulary in the following manner:\n\n\nLoaded vocabulary has the following basic operations:\n* `vocab.get_stoi()` dictionary allows us to convert word into its dictionary index\n* `vocab.get_itos()` does the opposite - converts number into word\n* `vocab.vectors` is the array of embedding vectors, so to get the embedding of a word we nee to use `vocab.vectors[vocab.get_stoi(s)]`\n\nHere is the example of manipulating embeddings to demonstrate the equation `kind-man+woman=queen`(the coefficient was tweak a bit to make it work):","metadata":{}},{"cell_type":"code","source":"vocab = torchtext.vocab.GloVe(name='6B', dim=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the vector corresponding to kind-man+woman\nqvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n# find the index of the closest embedding vector\nd = torch.sum((vocab.vectors-qvec)**2,dim=1)\nmin_idx = torch.argmin(d)\n# find the corresponding word\nvocab.itos[min_idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To train the classsifier using those embeedings, we first need to encode our dataset Glove vocabulary:","metadata":{}},{"cell_type":"code","source":"def offsetify(b):\n    # first, compute data tensot from all sequences\n    x = [torch.tensor(encode(t[1], voc=vocab)) for t in b] # pass the instance of vocab to encode function\n    # now, compute the offsets by accumulating the tensor of sequence lengths\n    o = [0] + [len(t) for t in x]\n    o = torch.tensor(o[:-1]).cumsum(dim=0)\n    return (\n        torch.LongTensor([t[0]-1 for t in b]), # labels\n        torch.cat(x), # text\n        o\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have seen above, all vector embeddings are stored in `vocab.vectors` matrix. It makes it super-easy to load those weights into weights of embedding layer using simple copying:","metadata":{}},{"cell_type":"code","source":"net = EmbedClassifier(len(vocab), len(vocab.vectors[0]), len(classes))\nnet.embedding.weight.data = vocab.vectors\nnet = net.to(torch_device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's train our model and see if we get better results:","metadata":{}},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=offsetify)\ntrain_epoch_emb(net, train_loader, lr=4, epoch_size=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of the reasons we are not seeing a significant increase in accuracy is due to the fact that some words from our dataset are missing in the pre-trained Glove vocabulary, and thus they are essentially ignored. To overcome this fact, we can train our own embeddings on our dataset.","metadata":{}},{"cell_type":"markdown","source":"# Contextual Embeddings\n\nOne key limitation of trafitional pretrained embedding representations such as Word2Vec is the problem of word sense and removing ambiguity by making them clear. While pretrained embeddings can capture some of the meaning of words in context, every possible meaning of a word is encoded into the same embedding. This can cause problems in downstream models, since many words such as the word `play` have different meanings depending on the context they are used in.\n\nFor example, the word `play` in these two different sentences have quite different meaning:\n* I went to `play` at the theatre\n* John wants to `play` with his friends\n\nThe pretrained embeddings above represent both meanings of the word 'play' in the same embedding. To overcome this limitation, we need to build embeddings based on the `language model`, which is trained on a large corpus of text, and knows how words can be put together in different contexts.","metadata":{}}]}