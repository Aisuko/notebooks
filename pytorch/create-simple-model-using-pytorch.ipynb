{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nAbout Tensors see [What are tensors?](https://www.kaggle.com/code/aisuko/what-are-tensors)\n\nWe are going to use the FashionMNIST dataset to train a new simple model and optimize it using PyTorch.\n\nPytorch has two primitives to work with data:\n\n* `torch.utils.data.DataLoader`\n* `torch.utils.data.Daset.Dataset`\n\nThey stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`.\n\n## TORCH.UTILS.DATA\n\n`torch.utils.data.DataLoader` class is the the heart of PyTorch data loading utility. It supports for:\n\n* map-style and iterable-style datasets\n* customizing data loading order\n* automatic batching\n* single-and multi-process data loading\n* automatic memory pining\n\nMore detail in the notebook [DataLoader in PyTorch](https://pytorch.org/docs/stable/data.html).\n\nPyTorch offers domain-specific libraries such as:\n* TorchText\n* TorchVision\n* TorchAudio\n\nAll of which include datasets. We wil be using a TorchVision dataset. The list of `torchvision.datasets` module contains in [here](https://pytorch.org/vision/stable/datasets.html).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Download the dataset\n\nEvery TorchVision `Dataset` includes two arguments:\n* `transform`\n* `target_transform`\n\nto modify the samples and lables respectively.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\ntraining_data=datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data=datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset\n\nWe pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, each element in the dataloader iterable will return a batch of 64 features and labels.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size=64\n\ntrain_dataloader=DataLoader(training_data, batch_size=batch_size)\ntest_dataloader=DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N,C,H,W]:{X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define a Model\n\nTo define a neural network in PyTorch, we create a class that inherits from [nn.Module](). We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward` function. To accelerate operations in the neural network, we move it to the GPU or MPS if avaliable.","metadata":{}},{"cell_type":"code","source":"device=(\"cuda\" if torch.cuda.is_avaliable() else \"cpu\")\ndevice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten=nn.Flatten()\n        self.linear_relu_stack=nn.Sequantial(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512,512),\n            nn.ReLU(),\n            nn.Linear(512,10)\n        )\n    \n    def forward(self, x):\n        x=self.flatten(x)\n        logits=self.linear_relu_stack(x)\n        return logits\nmodel=NeuralNetwork().to(device)\nmodel","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizing the Model Parameters\n\nTo train a model, we need a [loss function]() and [optimizer]().","metadata":{}},{"cell_type":"code","source":"loss_fn=nn.CrossEntropyLoss()\noptimizer=torch.optim.SGD(model.parameters(), lr=1e-3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model\n\nIn a single training loop, the model maskes predictions on the training dataset(fed to it in batches), and backpropagates the prediction error to adjust the model's parameters","metadata":{}},{"cell_type":"code","source":"def train(dataloader, model, loss_fn, optimizer):\n    size=len(dataloader.dataset)\n    model.train()\n    for batch, (X,y) in enumerate(dataloader):\n        X,y =X.to(device), y.to(device)\n        \n        # compute prediction error\n        pred=model(X)\n        loss=loss_fn(pred,y)\n\n        # backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        if batch%100==0:\n            loss.current=loss.item(), (batch+1) *len(X)\n            print(f\"loss:{loss:>7f} [{current:>5d}/{size:>5d}]\")\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the model","metadata":{}},{"cell_type":"code","source":"def test(dataloader, model, loss_fn):\n    size=len(dataloader.dataset)\n    num_batches=len(dataloader)\n    model.eval()\n    test_loss, correct=0,0\n    with torch.no_grad():\n        for X,y in dataloader:\n            X,y =X.to(device), y.to(device)\n            pred=model(X)\n            test_loss+=loss_fn(pred, y).item()\n            correct+=(pred.argmax(1)==y).type(torch.float).sum().item()\n    test_loss/=num_batches\n    correct/=size\n    print(f\"Test Error:\\n Accuracy: {(100*correct):>0.1f}%, Avg loss:{test_loss:>8f} \\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training process is conducted over several iterations(epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and loss at each spoch; we'd like to see the accuracy increase and the loss decrease with every epoch.","metadata":{}},{"cell_type":"code","source":"epochs=5\n\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict,\"simple_model.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loadinf models\n\nThe process of loading a model includes re-creating the model structure and loading the state dictionary into it.","metadata":{}},{"cell_type":"code","source":"import gc\n\ndef model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=NeuralNetwork().to(device)\nmodel.load_state_dict(torch.load(\"simple_model.pth\"))","metadata":{},"execution_count":null,"outputs":[]}]}