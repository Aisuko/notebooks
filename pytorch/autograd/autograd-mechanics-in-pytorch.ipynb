{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70af3b41",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006436,
     "end_time": "2024-02-01T04:24:21.184242",
     "exception": false,
     "start_time": "2024-02-01T04:24:21.177806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "> How the autograd works and records the operations.\n",
    "\n",
    "Autograd is a reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "Internally, autograd represents this graph as a graph of Function objects(really expression), which can be `apply()` ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient(the `.grad_fn` attribute of each `torch.Tensor` is an entry point into this graph). When the forward pass is completed, PyTorch evaluates this graph in the backwards passs to compute the gradients.\n",
    "\n",
    "The graph is recreated from scratch at every  iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don't have to encode all possible paths before you launch the training- what you run is what you differentiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e9a63a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:24:21.198637Z",
     "iopub.status.busy": "2024-02-01T04:24:21.198235Z",
     "iopub.status.idle": "2024-02-01T04:24:24.743852Z",
     "shell.execute_reply": "2024-02-01T04:24:24.742647Z"
    },
    "papermill": {
     "duration": 3.555918,
     "end_time": "2024-02-01T04:24:24.746655",
     "exception": false,
     "start_time": "2024-02-01T04:24:21.190737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.randn(5, requires_grad=True)\n",
    "y=x.pow(2)\n",
    "# y.grad_fn._saved_self refers to the same Tensor object as x.\n",
    "print(x.equal(y.grad_fn._saved_self))\n",
    "print(x is y.grad_fn._saved_self)\n",
    "\n",
    "# But that may not always be the case\n",
    "x=torch.randn(5, requires_grad=True)\n",
    "y=x.exp()\n",
    "print(y.equal(y.grad_fn._saved_result))\n",
    "print(y is y.grad_fn._saved_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f81893",
   "metadata": {
    "papermill": {
     "duration": 0.005781,
     "end_time": "2024-02-01T04:24:24.758495",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.752714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Under the hood, to prevent reference cycles, PyTorch has packed the tensor upon saving and `uppacked` it into a different tensor for reading. Here, the tensor we get from accessing `y.grad_fn._saved_result` is a different tensor object than `y` (but they sill share the same storage).\n",
    "\n",
    "Whether a tensor will be packed into a different tensor object depends on whether it is an outputs of its own `grad_fn`, which is an implementation detail subject to change and that users should not rely on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc68b9",
   "metadata": {
    "papermill": {
     "duration": 0.005572,
     "end_time": "2024-02-01T04:24:24.769979",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.764407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setting requires_grad\n",
    "\n",
    "`requires_grad` is a flag, defaulting to false unless wrapped in a `nn.Parameter`, that allows for fine-grained exclusion of subgraphs from gradient computation. It takes effect in both the forward and backward passes:\n",
    "\n",
    "During the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. During the backward pass `backward()`, only leaf tensors with `requires_grad=True` will have gradients accumulated into their `.grad` fields.\n",
    "\n",
    "It it important to note that even though every tensor has this flag, setting it only make sense for **leaf tensors**(tensors that do not have `grad_fn`, e.g., a `nn.Module's` parameters). **Non-leaf tensors(tensors that do have `grad_fn`) are tensors that have a backward graph associated with them.** Thus their gradients will be needed as an intermediary result to compute the gradient for a leaf tensor that requires grad. From this definition, it is clear that all non-leaf tensors will automatically have `require_grad=True.`\n",
    "\n",
    "Setting `requires_grad` should be the main way you control which parts if the model are part of the gradient computation, for example, if you need to freeze parts of your pretrained model during model fine-tuning.\n",
    "\n",
    "To freeze parts of your model, simply apply `.requires_grad_(False)` to the parameters that you don't want updated. And as described above, since computations that use these parameters as inputs would not be recorded in the forward pass, they won't have their `.grad` fields updated in the backward pass because they won't be part of the backward graph in the first place, as desired.\n",
    "\n",
    "Because this is such as common pattern, `requires_grad` can also be set at the module level with `nn.Module.required_grad_()`. When applied to a module, `.requires_grad_()` takes effect on all of the module's parameters(which have `requires_grad=True` by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09bf03",
   "metadata": {
    "papermill": {
     "duration": 0.005556,
     "end_time": "2024-02-01T04:24:24.781360",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.775804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Locally disabling gradient computation\n",
    "\n",
    "There are several mechanisms avaliable from Python to locally diable gradient computation:\n",
    "\n",
    "To diable gradients across entire blocks of code, there are context managers like:\n",
    "* no-grade model\n",
    "* inference mode. \n",
    "\n",
    "For more fine-grained exclusion of subgraphs from gradient computation, there is setting the `requires_grad` field of a tensor.\n",
    "\n",
    "Below, inaddition to discussing the mechanisms above, we also describe evaluation mode `nn.Module.eval()`, a method that is not used to diable gradient computation but, because of its name, is often mixed up with the three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642803f7",
   "metadata": {
    "papermill": {
     "duration": 0.005497,
     "end_time": "2024-02-01T04:24:24.792651",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.787154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Grad Modes\n",
    "\n",
    "Apart from setting `required_grad` there are also three grad modes:\n",
    "\n",
    "* default(grade mode)\n",
    "* no-grad mode\n",
    "* inference mode\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/854/008/221/626/357/original/1d2fa9e43eb4c8dd.png\" width=\"60%\" heigh=\"60%\" alt=\"Grad Models under different context\"></div>\n",
    "\n",
    "\n",
    "## Default Mode(Grad Mode)\n",
    "\n",
    "It is the only mode in which `requires_grad` takes effect.\n",
    "\n",
    "\n",
    "## No-grad Mode\n",
    "\n",
    "Computations in no-grad mode are never recorded in the backward graph even if there are inputs that have `require_grad=True`. **Enable no-grad mode when you need to perform operations that should not be recorded by autograd, but you would sitll like to use the outputs of these computations in grad mode later.**\n",
    "\n",
    "\n",
    "## Inference Mode\n",
    "\n",
    "It is the extreme version of no-grad mode. Just like no-grad mode, computations in inference mode are not recorded in the backward graph, but enabling inference mode will allow PyTorch to speed up your model even more.\n",
    "\n",
    "\n",
    "## Evaluation Mode(nn.Module.eval())\n",
    "\n",
    "Evaluation mode is not a machanism to locally disable gradient computation. It is included here anyway because it it sometimes confused to be such a mechanism.\n",
    "\n",
    "Functionally, `module.eval()`(or equivalently `module.train(False)`) are completely orthogonal to no-grad mode and inference mode.\n",
    "\n",
    "It is recommended that you always use `model.train()` when training and `model.eval()` when evaluating model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98984695",
   "metadata": {
    "papermill": {
     "duration": 0.005592,
     "end_time": "2024-02-01T04:24:24.803997",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.798405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Examples with code\n",
    "\n",
    "When training neural network, the most frequently used algorithm is **back propgation**. In this algorithm, parameters(model weights) are adjusted according to the **gradient** of the loss function with respect to the given parameter.\n",
    "\n",
    "And the PyTorch build in differentiation engine `torch.autograd`, it supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "Consider the simplest one-layer neural network, which input x, parameters w and b, and some loss function. It can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27766f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:24:24.819444Z",
     "iopub.status.busy": "2024-02-01T04:24:24.818895Z",
     "iopub.status.idle": "2024-02-01T04:24:24.901664Z",
     "shell.execute_reply": "2024-02-01T04:24:24.900533Z"
    },
    "papermill": {
     "duration": 0.093521,
     "end_time": "2024-02-01T04:24:24.904181",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.810660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6917, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input tensor\n",
    "x=torch.ones(5)\n",
    "# expected output\n",
    "y=torch.zeros(3)\n",
    "\n",
    "w=torch.randn(5,3,requires_grad=True)\n",
    "b=torch.randn(3, requires_grad=True)\n",
    "z=torch.matmul(x,w)+b\n",
    "loss=torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e27e95",
   "metadata": {
    "papermill": {
     "duration": 0.005888,
     "end_time": "2024-02-01T04:24:24.916243",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.910355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tensors, Functions and Computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19238e7d",
   "metadata": {
    "papermill": {
     "duration": 0.00571,
     "end_time": "2024-02-01T04:24:24.927906",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.922196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The code defines the following **computational graph**:\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/854/136/166/407/621/original/ce241b35f174e9fe.png\" width=\"60%\" heigh=\"60%\" alt=\"Computational graph of the Code\"></div>\n",
    "\n",
    "\n",
    "In this network, `w` and `b` are parameters, which we need to optimize. Thus, we need to able to compute the gradients of loss function with respect to those variables. In order to do that, we set the `requires_grad` property of those tensors.\n",
    "\n",
    "A function that we apply to tensors to construct computational graph is in fact an object of class `Function`. This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in `grad_fn` property of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5223e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:24:24.941856Z",
     "iopub.status.busy": "2024-02-01T04:24:24.941394Z",
     "iopub.status.idle": "2024-02-01T04:24:24.946830Z",
     "shell.execute_reply": "2024-02-01T04:24:24.945765Z"
    },
    "papermill": {
     "duration": 0.016256,
     "end_time": "2024-02-01T04:24:24.950143",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.933887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z =<AddBackward0 object at 0x7e83efdb20e0>\n",
      "Gradient function for loss=<BinaryCrossEntropyWithLogitsBackward0 object at 0x7e83efdb3550>\n"
     ]
    }
   ],
   "source": [
    "print(f'Gradient function for z ={z.grad_fn}')\n",
    "print(f'Gradient function for loss={loss.grad_fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97412f1",
   "metadata": {
    "papermill": {
     "duration": 0.007208,
     "end_time": "2024-02-01T04:24:24.963623",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.956415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## More on Computational Graphs\n",
    "\n",
    "> Note: DAGs are dynamic in PyTorch an important thing to note is that the graph is recreated from scratch; after each `.backward()` call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\n",
    "\n",
    "Conceptually, autograd keeps a record of data(tensors) and all executed operations(along with the resulting new tensors) in a directed acyclic graph(DAG) consisting of function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "* run the requested operation to compute a resulting tensor\n",
    "* maintain the operation's gradient function in the DAG\n",
    "\n",
    "The backwaes pass kicks off when `.backward()` is called on the DAG root. `autograd` then:\n",
    "* computes the gradients from each `.grad_fn`\n",
    "* accumulates them in the respective tensor's `grad` attribute\n",
    "* using the chain rule, propagates all the way to the leaf tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a7e3d",
   "metadata": {
    "papermill": {
     "duration": 0.006148,
     "end_time": "2024-02-01T04:24:24.976077",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.969929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Computing Gradients\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "* We can only obtain the grad properties for the leaf nodes of the computational graph, wich have requires_grad property set to True. For all other nodes in our graph, gradients will not be avaliable.\n",
    "\n",
    "* We can only perform gradient calculations using backward onece on a given graph, for performance reasons.\n",
    "\n",
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $\\frac{\\partial loss}{\\partial w}$ and $\\frac{\\partial loss}{\\partial b}$ under some fixed values of `x` and `y`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137dc467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:24:24.992093Z",
     "iopub.status.busy": "2024-02-01T04:24:24.991687Z",
     "iopub.status.idle": "2024-02-01T04:24:25.018402Z",
     "shell.execute_reply": "2024-02-01T04:24:25.017275Z"
    },
    "papermill": {
     "duration": 0.036584,
     "end_time": "2024-02-01T04:24:25.020642",
     "exception": false,
     "start_time": "2024-02-01T04:24:24.984058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0828, 0.2673, 0.0525],\n",
      "        [0.0828, 0.2673, 0.0525],\n",
      "        [0.0828, 0.2673, 0.0525],\n",
      "        [0.0828, 0.2673, 0.0525],\n",
      "        [0.0828, 0.2673, 0.0525]])\n",
      "tensor([0.0828, 0.2673, 0.0525])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6069a8",
   "metadata": {
    "papermill": {
     "duration": 0.005961,
     "end_time": "2024-02-01T04:24:25.032880",
     "exception": false,
     "start_time": "2024-02-01T04:24:25.026919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Disabling Gradient Tracking\n",
    "\n",
    "By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to `forward` computations through the network. We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb80eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:24:25.048060Z",
     "iopub.status.busy": "2024-02-01T04:24:25.047651Z",
     "iopub.status.idle": "2024-02-01T04:24:25.055967Z",
     "shell.execute_reply": "2024-02-01T04:24:25.054554Z"
    },
    "papermill": {
     "duration": 0.01819,
     "end_time": "2024-02-01T04:24:25.058274",
     "exception": false,
     "start_time": "2024-02-01T04:24:25.040084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z=torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z=torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291489ab",
   "metadata": {
    "papermill": {
     "duration": 0.006351,
     "end_time": "2024-02-01T04:24:25.071083",
     "exception": false,
     "start_time": "2024-02-01T04:24:25.064732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Another way achieve the same result is to use the `detach()` method on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6f5bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-01T04:24:25.086409Z",
     "iopub.status.busy": "2024-02-01T04:24:25.085143Z",
     "iopub.status.idle": "2024-02-01T04:24:25.093271Z",
     "shell.execute_reply": "2024-02-01T04:24:25.092281Z"
    },
    "papermill": {
     "duration": 0.018068,
     "end_time": "2024-02-01T04:24:25.095544",
     "exception": false,
     "start_time": "2024-02-01T04:24:25.077476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.matmul(x,w)+b\n",
    "z_det=z.detach()\n",
    "z_det.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcfe9c",
   "metadata": {
    "papermill": {
     "duration": 0.006282,
     "end_time": "2024-02-01T04:24:25.108473",
     "exception": false,
     "start_time": "2024-02-01T04:24:25.102191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "* To mark some parameters in your neural networks as **frozen parameters**\n",
    "* To **speed up computations** when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.104721,
   "end_time": "2024-02-01T04:24:26.038000",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-01T04:24:17.933279",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
