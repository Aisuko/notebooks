{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/gaussian-error-linear-unit-gelu?scriptVersionId=164532090\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"524d7af7","metadata":{"papermill":{"duration":0.002495,"end_time":"2024-02-27T11:47:54.417811","exception":false,"start_time":"2024-02-27T11:47:54.415316","status":"completed"},"tags":[]},"source":["# Overview\n","\n","GELU stands for Gaussian Error Linear Unit. It's an activation function used in neural networks. The GELU function can be represented mathematically as:\n","\n","```math\n","GELU(x)=0.5*x*(1+tanh(sqrt(2/pi)*(x+0.044715*x^3)))\n","```\n","\n","\n","The GELU activation function is used to introduce non-linearity in the output of a neuron. For exmaple, here we are going to implement an Encoder model with two layer of GELU:\n","\n","```rust\n","#[derive(Module, Debug)]\n","pub struct AudioEncoder<B: Backend> {\n","    conv1: Conv1d<B>,\n","    gelu1: nn::GELU,\n","    conv2: Conv1d<B>,\n","    gelu2: nn::GELU,\n","    blocks: Vec<ResidualEncoderAttentionBlock<B>>,\n","    ln_post: nn::LayerNorm<B>,\n","    positional_embedding: Param<Tensor<B, 2>>,\n","    n_mels: usize,\n","    n_audio_ctx: usize,\n","}\n","```"]},{"cell_type":"markdown","id":"8ee93a1b","metadata":{"papermill":{"duration":0.00191,"end_time":"2024-02-27T11:47:54.422281","exception":false,"start_time":"2024-02-27T11:47:54.420371","status":"completed"},"tags":[]},"source":["# Benefit of using GELU\n","\n","What is the benefit of using GELU activation function over other activation functions?\n","\n","The GELU activation function has been found to perform better than other activation functions in some tasks, especially in transformer models. Here are some benefits of using GELU:\n","\n","\n","## Non Linearity\n","\n","Like other activation functions, GELU introduces non-linearity into the model, which allows the model to learn complex patterns.\n","\n","\n","## Smooth Gradient\n","\n","GELU is differentiable and has a smooth gradient everywhere, which can help improve the stability of the model's training.\n","\n","\n","## Better Performance\n","\n","Some studies have found that GELU can lead to better model performance compared to other activation functions, such as `ReLU` or `Leaky ReLU`, especially in transformer models.\n","\n","\n","## Gaussian Distribution\n","\n","The output of GELU follows a Gaussian distribution, which can be beneficial in some cases.\n","\n","\n","However, the choice of activation funciton can depend on the specific task and model architecture. It's often a good idea to experiment wirth different activation functions to see which one works best for the specific use case.\n","\n","# Implementing with Python"]},{"cell_type":"code","execution_count":1,"id":"f422231d","metadata":{"execution":{"iopub.execute_input":"2024-02-27T11:47:54.428396Z","iopub.status.busy":"2024-02-27T11:47:54.427637Z","iopub.status.idle":"2024-02-27T11:47:57.871195Z","shell.execute_reply":"2024-02-27T11:47:57.870324Z"},"papermill":{"duration":3.44928,"end_time":"2024-02-27T11:47:57.873647","exception":false,"start_time":"2024-02-27T11:47:54.424367","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","import math\n","\n","class GELU(torch.nn.Module):\n","    def forward(self,x):\n","        return 0.5*x*(1+torch.tanh(math.sqrt(2/math.pi)*(x+0.044715*torch.pow(x,3))))"]},{"cell_type":"markdown","id":"b47bdc66","metadata":{"papermill":{"duration":0.00186,"end_time":"2024-02-27T11:47:57.877733","exception":false,"start_time":"2024-02-27T11:47:57.875873","status":"completed"},"tags":[]},"source":["# Testing\n","\n","Here we creat a batch of 32 sequences, each of length 10, with 64 features"]},{"cell_type":"code","execution_count":2,"id":"d93b307e","metadata":{"execution":{"iopub.execute_input":"2024-02-27T11:47:57.884281Z","iopub.status.busy":"2024-02-27T11:47:57.883007Z","iopub.status.idle":"2024-02-27T11:47:57.964011Z","shell.execute_reply":"2024-02-27T11:47:57.963103Z"},"papermill":{"duration":0.0872,"end_time":"2024-02-27T11:47:57.96692","exception":false,"start_time":"2024-02-27T11:47:57.87972","status":"completed"},"tags":[]},"outputs":[],"source":["def test_forward():\n","    gelu=GELU()\n","    x=torch.randn(32,10,64)\n","    y=gelu(x)\n","    \n","    assert x.shape==y.shape\n","\n","test_forward()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":8.195074,"end_time":"2024-02-27T11:47:58.792766","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-27T11:47:50.597692","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}