{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/implement-stochastic-optimisation-with-pytorch?scriptVersionId=164626041\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"54d37274","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.003376,"end_time":"2024-02-28T03:09:18.58376","exception":false,"start_time":"2024-02-28T03:09:18.580384","status":"completed"},"tags":[]},"source":["# Overview\n","\n","Adam stands for A method for stochastic optimization. According to it's paper, it is an algorithm for first-order gradient-based optimization of stochastis objective functions, and it is based on adaptive estimates of lower-order moments. Furthermore, it is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the greadients, and is well suited for problems that are large in terms of data and/or parameters. And it is also appropriate for non-stationary objectices and problems with very noisy and/or sparse gradients. So, let's look at how this is implemented in PyTorch.\n","\n","\n","# Algorithm Description\n","\n","<div style=\"text-align: center\"><img src=\"https://hostux.social/system/media_attachments/files/111/533/123/322/407/873/original/c0b7ea2c4824b18d.png\" width=\"80%\" heigh=\"50%\" alt=\"multiplication_attention_scores\"><figure>Source: Paper:A method for Stochastic Optimization</figure></div>\n","\n","Adam update is,\n","\n","\\begin{align}\n","m_t &\\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g_t \\\\\n","v_t &\\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 \\\\\n","\\hat{m}_t &\\leftarrow \\frac{m_t}{1-\\beta_1^t} \\\\\n","\\hat{v}_t &\\leftarrow \\frac{v_t}{1-\\beta_2^t} \\\\\n","\\theta_t &\\leftarrow \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n","\\end{align}\n","\n","where $\\alpha$, $\\beta_1$, $\\beta_2$ and $\\epsilon$ are scalar hyper parameters.\n","$m_t$ and $v_t$ are first and second order moments.\n","$\\hat{m}_t$ and $\\hat{v}_t$ are biased corrected moments.\n","$\\epsilon$ is used as a fix for division by zero error, but also acts as a form of a gyper-parameter that acts against variance in gradients.\n","\n","\n","Effective step taken assuming $\\epsilon=0$ is,\n","$$\\Delta t= \\alpha \\cdot \\frac{\\hat{m}_t}{\\hat{v}_t}$$\n","\n","This is bounded by,\n","$$\\vert \\Delta t \\vert \\le \\alpha \\cdot \\frac{1-\\beta_1}{\\sqrt{1-\\beta_2}}$$\n","when $1 -\\beta_1 \\gt \\sqrt{1-\\beta_2}$\n","and\n","$$ \\vert \\Delta t\\vert \\le \\alpha$$\n","otherwise.\n","And in most common scenarios, $$\\vert \\Delta t \\vert \\approx \\alpha$$\n","\n"]},{"cell_type":"code","execution_count":1,"id":"eadb4fae","metadata":{"execution":{"iopub.execute_input":"2024-02-28T03:09:18.595288Z","iopub.status.busy":"2024-02-28T03:09:18.594129Z","iopub.status.idle":"2024-02-28T03:09:22.115434Z","shell.execute_reply":"2024-02-28T03:09:22.114111Z"},"papermill":{"duration":3.531294,"end_time":"2024-02-28T03:09:22.117964","exception":false,"start_time":"2024-02-28T03:09:18.58667","status":"completed"},"tags":[]},"outputs":[],"source":["from typing import List, Optional\n","from torch import Tensor\n","from torch.utils._foreach_utils import _group_tensors_by_device_and_dtype"]},{"cell_type":"markdown","id":"3dddc531","metadata":{"papermill":{"duration":0.002555,"end_time":"2024-02-28T03:09:22.123781","exception":false,"start_time":"2024-02-28T03:09:22.121226","status":"completed"},"tags":[]},"source":["# Implement\n","\n","Here are the description of the input parameters:\n","\n","* `params` is the list of parameters to optimize or dicts defining parameter groups\n","* `lr` is the learning rate $\\alpha$\n","* `betas` is a tuple of ($\\beta_1$, $\\beta_2$)\n","* `eps` is $\\hat{\\epsilon}$ or $\\epsilon$, term added to the denominator to improve numerical stability\n","* `weight_decay` weight decay(L2 penalty)\n","* `amsgrad` whether to use the AMSGrad variant of this algorithm form the paper On the Convergence of Adam and Beyond\n","* `foreach` whether foreach implementation of optimizer is used.\n","* `maximize` maximize the params based on the objective, instead of minimizing\n","* `capturable` whether this instance is safe to capture in a CUDA graph.\n","* `differentiable` whether autograd should occur through the optimizer step in training.\n","* `fused` whether the fused implementation is used."]},{"cell_type":"code","execution_count":2,"id":"b7c46ad9","metadata":{"execution":{"iopub.execute_input":"2024-02-28T03:09:22.131382Z","iopub.status.busy":"2024-02-28T03:09:22.130843Z","iopub.status.idle":"2024-02-28T03:09:22.156621Z","shell.execute_reply":"2024-02-28T03:09:22.155503Z"},"papermill":{"duration":0.032209,"end_time":"2024-02-28T03:09:22.158753","exception":false,"start_time":"2024-02-28T03:09:22.126544","status":"completed"},"tags":[]},"outputs":[],"source":["from torch.optim.optimizer import (Optimizer, _use_grad_for_differentiable, _get_value, _stack_if_compiling, _dispatch_sqrt, _default_to_fused_or_foreach)\n","\n","class Adam(Optimizer):\n","\n","    def __init__(\n","        self, params, lr=1e-3, betas=(0.9,0.999), \n","        eps=1e-8, weight_decay=0, amsgrad=False, *,\n","        foreach:Optional[bool]=None, maximize: bool=False, \n","        capturable:bool=False, \n","        differentiable:bool=False, fused:Optional[bool]=None):\n","        \n","        \n","        if not 0.0<=lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <=eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        if not 0.0 <= weight_decay:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        \n","        defaults=dict(\n","            lr=lr, betas=betas, eps=eps, \n","            weight_decat=weight_decay, amsgrad=amdgrad,\n","            maximize=maximize, foreach=foreach, capturable=capturable,\n","            differentiable=differentiable, fused=fused)\n","\n","        super().__init__(params, defaults)\n","        \n","        if fused:\n","            if differentiable:\n","                raise RuntimeError(\"`fused` does not support `differentiable`\")\n","            self._step_supports_amp_scaling=True\n","            if not all(p.is_cuda and torch.is_floating_point(p) \n","                       for pg in self.param_groups \n","                       for p in pg['params']):\n","                raise RuntimeError(\"`fused=True` requires all the params to be CUDA, floating point Tensor\")\n","            if foreach:\n","                raise RuntimeError(\"`fused` and `foreach` cannot be `True` together.\")\n","    \n","    def __setstate__(self, state):\n","        super().__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","            group.setdefault('maximize', False)\n","            group.setdefault('foreach', None)\n","            group.setdefault('capturable', False)\n","            group.setdefault('differentiable', False)\n","            group.setdefault('fused', None)\n","        state_values=list(self.state.values())\n","        step_is_tensor=(len(state_values)!=0 and torch.is_tensor(state_values[0]['step']))\n","        if not step_is_tensor:\n","            for s in state_values:\n","                s['step']=torch.tensor(float(s['step']))\n","    \n","    \n","    def _init_group(\n","        self, group, params_with_grad, \n","        grads, exp_avgs, exp_avg_sqs, \n","        max_exp_avg_sqs, state_steps):\n","        \n","\n","        for p in group['params']:\n","            if p.grad is not None:\n","                params_with_grad.append(p)\n","                if p.grad.is_sparse:\n","                    raise RuntimeError(\n","                        'Adam does not support sparse gradients, please consider SparseAdam instead')\n","                grads.append(p.grad)\n","                \n","                state=self.state[p]\n","                \n","                # Lazy state initialization\n","                if len(state)==0:\n","                    state['step']=(\n","                        torch.zeros((1,), dtype=torch.float, device=p.device)\n","                        if group['capturable'] or group['fused'] else torch.tensor(0.)\n","                    )\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] =torch.zeros_like(p, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq']=torch.zeros_like(p, memory_format=torch.preserve_format)\n","                    if group['amsgrad']:\n","                        # Maintains max of all exp, moving avg of sq. grad. values\n","                        state['max_exp_avg_sq']=torch.zeros_like(\n","                            p, \n","                            memory_format=torch.preserve_format)\n","                \n","                exp_avgs.append(state['exp_avg'])\n","                exp_avg_sqs.append(state['exp_avg_sq'])\n","                \n","                if group['amsgrad']:\n","                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n","                if group['differentiable'] and state['step'].requires_grad:\n","                    raise RuntimeError('`requires_grad` is not supported for `step` in differentiable mode')\n","                state_steps.append(state['step'])\n","    \n","    @_use_grad_for_differentiable\n","    def step(self, closure=None):\n","        \"\"\"\n","        Performs a single optimization step.\n","        \n","        Args:\n","            closure (Callable, optional): A closure that revaluates the model and return the loss\n","        \"\"\"\n","        \n","        self._cuda_graph_capture_health_check()\n","        \n","        loss=None\n","        if closure is not None:\n","            with torch.enable_grad():\n","                loss=closure()\n","                \n","        for group in self.param_groups:\n","            params_with_grad=[]\n","            grads=[]\n","            exp_avgs=[]\n","            exp_avg_sqs=[]\n","            max_exp_avg_sqs=[]\n","            state_steps=[]\n","            beta1, beta2=group['betas']\n","            \n","            self._init_group(\n","                group,\n","                params_with_grad,\n","                grads,\n","                exp_avgs,\n","                exp_avg_sqs,\n","                max_exp_avg_sqs,\n","                state_steps\n","            )\n","            \n","            adam(\n","                params_with_grad,\n","                grads,\n","                exp_avgs,\n","                exp_avg_sqs,\n","                max_exp_avg_sqs,\n","                state_steps,\n","                amsgrad=group['amsgrad'],\n","                beta1=beta1,\n","                beta2=beta2,\n","                lr=group['lr'],\n","                weight_decay=group['weight_decay'],\n","                eps=group['eps'],\n","                maximize=group['maximize'],\n","                foreach=group['foreach'],\n","                capturable=group['capturable'],\n","                differentiable=group['differentiable'],\n","                fused=group['fused'],\n","                grad_scale=getattr(self, \"grad_scale\", None),\n","                found_inf=getattr(self, \"found_inf\", None),\n","            )\n","            \n","        return loss\n"]},{"cell_type":"code","execution_count":3,"id":"6f7b96ea","metadata":{"execution":{"iopub.execute_input":"2024-02-28T03:09:22.16662Z","iopub.status.busy":"2024-02-28T03:09:22.166263Z","iopub.status.idle":"2024-02-28T03:09:22.176188Z","shell.execute_reply":"2024-02-28T03:09:22.175234Z"},"papermill":{"duration":0.016598,"end_time":"2024-02-28T03:09:22.178502","exception":false,"start_time":"2024-02-28T03:09:22.161904","status":"completed"},"tags":[]},"outputs":[],"source":["def adam(params: List[Tensor],\n","         grads: List[Tensor],\n","         exp_avgs: List[Tensor],\n","         exp_avg_sqs: List[Tensor],\n","         max_exp_avg_sqs: List[Tensor],\n","         state_steps: List[Tensor],\n","         foreach: Optional[bool] = None,\n","         capturable: bool = False,\n","         differentiable: bool = False,\n","         fused: Optional[bool] = None,\n","         grad_scale: Optional[Tensor] = None,\n","         found_inf: Optional[Tensor] = None,\n","         *,\n","         amsgrad: bool,\n","         beta1: float,\n","         beta2: float,\n","         lr: float,\n","         weight_decay: float,\n","         eps: float,\n","         maximize: bool\n","        ):\n","    \n","    if fused is None and foreach is None:\n","        _, foreach = _default_to_fused_or_foreach(params, differentiable, use_fused=False)\n","    if fused is None:\n","        fused = False\n","    if foreach is None:\n","        foreach = False\n","\n","    if not all(isinstance(t, torch.Tensor) for t in state_steps):\n","        raise RuntimeError(\"API has changed, `state_steps` argument must contain a list of singleton tensors\")\n","\n","    if foreach and torch.jit.is_scripting():\n","        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n","\n","    if fused and not torch.jit.is_scripting():\n","        func = _fused_adam\n","    elif foreach and not torch.jit.is_scripting():\n","        func = _multi_tensor_adam\n","    else:\n","        func = _single_tensor_adam\n","\n","    func(params,\n","         grads,\n","         exp_avgs,\n","         exp_avg_sqs,\n","         max_exp_avg_sqs,\n","         state_steps,\n","         amsgrad=amsgrad,\n","         beta1=beta1,\n","         beta2=beta2,\n","         lr=lr,\n","         weight_decay=weight_decay,\n","         eps=eps,\n","         maximize=maximize,\n","         capturable=capturable,\n","         differentiable=differentiable,\n","         grad_scale=grad_scale,\n","         found_inf=found_inf)"]},{"cell_type":"code","execution_count":4,"id":"dc7602bb","metadata":{"execution":{"iopub.execute_input":"2024-02-28T03:09:22.186122Z","iopub.status.busy":"2024-02-28T03:09:22.185726Z","iopub.status.idle":"2024-02-28T03:09:22.200489Z","shell.execute_reply":"2024-02-28T03:09:22.199727Z"},"papermill":{"duration":0.021072,"end_time":"2024-02-28T03:09:22.202545","exception":false,"start_time":"2024-02-28T03:09:22.181473","status":"completed"},"tags":[]},"outputs":[],"source":["def _single_tensor_adam(params: List[Tensor],\n","                        grads: List[Tensor],\n","                        exp_avgs: List[Tensor],\n","                        exp_avg_sqs: List[Tensor],\n","                        max_exp_avg_sqs: List[Tensor],\n","                        state_steps: List[Tensor],\n","                        grad_scale: Optional[Tensor],\n","                        found_inf: Optional[Tensor],\n","                        *,\n","                        amsgrad: bool,\n","                        beta1: float,\n","                        beta2: float,\n","                        lr: float,\n","                        weight_decay: float,\n","                        eps: float,\n","                        maximize: bool,\n","                        capturable: bool,\n","                        differentiable: bool):\n","\n","    assert grad_scale is None and found_inf is None\n","\n","    for i, param in enumerate(params):\n","\n","        grad = grads[i] if not maximize else -grads[i]\n","        exp_avg = exp_avgs[i]\n","        exp_avg_sq = exp_avg_sqs[i]\n","        step_t = state_steps[i]\n","\n","        if capturable:\n","            assert param.is_cuda and step_t.is_cuda, \"If capturable=True, params and state_steps must be CUDA tensors.\"\n","\n","        # update step\n","        step_t += 1\n","\n","        if weight_decay != 0:\n","            grad = grad.add(param, alpha=weight_decay)\n","\n","        if torch.is_complex(param):\n","            grad = torch.view_as_real(grad)\n","            exp_avg = torch.view_as_real(exp_avg)\n","            exp_avg_sq = torch.view_as_real(exp_avg_sq)\n","            param = torch.view_as_real(param)\n","\n","        # Decay the first and second moment running average coefficient\n","        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n","\n","        if capturable or differentiable:\n","            step = step_t\n","\n","            # 1 - beta1 ** step can't be captured in a CUDA graph, even if step is a CUDA tensor\n","            # (incurs \"RuntimeError: CUDA error: operation not permitted when stream is capturing\")\n","            bias_correction1 = 1 - torch.pow(beta1, step)\n","            bias_correction2 = 1 - torch.pow(beta2, step)\n","\n","            step_size = lr / bias_correction1\n","            step_size_neg = step_size.neg()\n","\n","            bias_correction2_sqrt = bias_correction2.sqrt()\n","\n","            if amsgrad:\n","                # Maintains the maximum of all 2nd moment running avg. till now\n","                if differentiable:\n","                    max_exp_avg_sqs_i = max_exp_avg_sqs[i].clone()\n","                else:\n","                    max_exp_avg_sqs_i = max_exp_avg_sqs[i]\n","                max_exp_avg_sqs[i].copy_(torch.maximum(max_exp_avg_sqs_i, exp_avg_sq))\n","                # Uses the max. for normalizing running avg. of gradient\n","                # Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write\n","                # (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)\n","                denom = (max_exp_avg_sqs[i].sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(eps / step_size_neg)\n","            else:\n","                denom = (exp_avg_sq.sqrt() / (bias_correction2_sqrt * step_size_neg)).add_(eps / step_size_neg)\n","\n","            param.addcdiv_(exp_avg, denom)\n","        else:\n","            step = _get_value(step_t)\n","\n","            bias_correction1 = 1 - beta1 ** step\n","            bias_correction2 = 1 - beta2 ** step\n","\n","            step_size = lr / bias_correction1\n","\n","            bias_correction2_sqrt = _dispatch_sqrt(bias_correction2)\n","\n","            if amsgrad:\n","                # Maintains the maximum of all 2nd moment running avg. till now\n","                torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n","                # Use the max. for normalizing running avg. of gradient\n","                denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n","            else:\n","                denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n","\n","            param.addcdiv_(exp_avg, denom, value=-step_size)"]},{"cell_type":"code","execution_count":5,"id":"33fa0ec0","metadata":{"execution":{"iopub.execute_input":"2024-02-28T03:09:22.211251Z","iopub.status.busy":"2024-02-28T03:09:22.20991Z","iopub.status.idle":"2024-02-28T03:09:22.233093Z","shell.execute_reply":"2024-02-28T03:09:22.231968Z"},"papermill":{"duration":0.029937,"end_time":"2024-02-28T03:09:22.235581","exception":false,"start_time":"2024-02-28T03:09:22.205644","status":"completed"},"tags":[]},"outputs":[],"source":["def _multi_tensor_adam(params: List[Tensor],\n","                       grads: List[Tensor],\n","                       exp_avgs: List[Tensor],\n","                       exp_avg_sqs: List[Tensor],\n","                       max_exp_avg_sqs: List[Tensor],\n","                       state_steps: List[Tensor],\n","                       grad_scale: Optional[Tensor],\n","                       found_inf: Optional[Tensor],\n","                       *,\n","                       amsgrad: bool,\n","                       beta1: float,\n","                       beta2: float,\n","                       lr: float,\n","                       weight_decay: float,\n","                       eps: float,\n","                       maximize: bool,\n","                       capturable: bool,\n","                       differentiable: bool):\n","    if len(params) == 0:\n","        return\n","\n","    if capturable:\n","        assert all(p.is_cuda and step.is_cuda for p, step in zip(params, state_steps)), \\\n","            \"If capturable=True, params and state_steps must be CUDA tensors.\"\n","\n","    assert grad_scale is None and found_inf is None\n","\n","    assert not differentiable, \"_foreach ops don't support autograd\"\n","\n","    grouped_tensors = _group_tensors_by_device_and_dtype([params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps])\n","    for (device_params, device_grads, device_exp_avgs, device_exp_avg_sqs,\n","         device_max_exp_avg_sqs, device_state_steps) in grouped_tensors.values():\n","\n","        if maximize:\n","            device_grads = torch._foreach_neg(tuple(device_grads))  # type: ignore[assignment]\n","\n","        # Handle complex parameters\n","        device_grads = [torch.view_as_real(x) if torch.is_complex(x) else x for x in device_grads]\n","        device_exp_avgs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in device_exp_avgs]\n","        device_exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in device_exp_avg_sqs]\n","        params_ = [torch.view_as_real(x) if torch.is_complex(x) else x for x in device_params]\n","\n","        # update steps\n","        torch._foreach_add_(device_state_steps, 1)\n","\n","        if weight_decay != 0:\n","            device_grads = torch._foreach_add(device_grads, device_params, alpha=weight_decay)\n","\n","        # Decay the first and second moment running average coefficient\n","        torch._foreach_mul_(device_exp_avgs, beta1)\n","        torch._foreach_add_(device_exp_avgs, device_grads, alpha=1 - beta1)\n","\n","        torch._foreach_mul_(device_exp_avg_sqs, beta2)\n","        torch._foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, 1 - beta2)\n","\n","        if capturable:\n","            # TODO: use foreach_pow if/when foreach_pow is added\n","            bias_correction1 = [torch.pow(beta1, step) for step in device_state_steps]\n","            bias_correction2 = [torch.pow(beta2, step) for step in device_state_steps]\n","            # foreach_sub doesn't allow a scalar as the first arg\n","            torch._foreach_sub_(bias_correction1, 1)\n","            torch._foreach_sub_(bias_correction2, 1)\n","            torch._foreach_neg_(bias_correction1)\n","            torch._foreach_neg_(bias_correction2)\n","\n","            # foreach_div doesn't allow a scalar as the first arg\n","            step_size = torch._foreach_div(bias_correction1, lr)\n","            torch._foreach_reciprocal_(step_size)\n","            torch._foreach_neg_(step_size)\n","\n","            bias_correction2_sqrt = torch._foreach_sqrt(bias_correction2)\n","\n","            if amsgrad:\n","                # Maintains the maximum of all 2nd moment running avg. till now\n","                torch._foreach_maximum_(device_max_exp_avg_sqs, device_exp_avg_sqs)  # type: ignore[assignment]\n","\n","                # Use the max. for normalizing running avg. of gradient\n","                max_exp_avg_sq_sqrt = torch._foreach_sqrt(device_max_exp_avg_sqs)\n","                # Folds in (admittedly ugly) 1-elem step_size math here to avoid extra param-set-sized read+write\n","                # (can't fold it into addcdiv_ below because addcdiv_ requires value is a Number, not a Tensor)\n","                torch._foreach_div_(max_exp_avg_sq_sqrt, torch._foreach_mul(bias_correction2_sqrt, step_size))\n","                eps_over_step_size = torch._foreach_div(step_size, eps)\n","                torch._foreach_reciprocal_(eps_over_step_size)\n","                denom = torch._foreach_add(max_exp_avg_sq_sqrt, eps_over_step_size)\n","            else:\n","                exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n","                torch._foreach_div_(exp_avg_sq_sqrt, torch._foreach_mul(bias_correction2_sqrt, step_size))\n","                eps_over_step_size = torch._foreach_div(step_size, eps)\n","                torch._foreach_reciprocal_(eps_over_step_size)\n","                denom = torch._foreach_add(exp_avg_sq_sqrt, eps_over_step_size)\n","\n","            torch._foreach_addcdiv_(params_, device_exp_avgs, denom)\n","        else:\n","            bias_correction1 = [1 - beta1 ** _get_value(step) for step in device_state_steps]\n","            bias_correction2 = [1 - beta2 ** _get_value(step) for step in device_state_steps]\n","\n","            step_size = _stack_if_compiling([(lr / bc) * -1 for bc in bias_correction1])\n","\n","            bias_correction2_sqrt = [_dispatch_sqrt(bc) for bc in bias_correction2]\n","\n","            if amsgrad:\n","                # Maintains the maximum of all 2nd moment running avg. till now\n","                torch._foreach_maximum_(device_max_exp_avg_sqs, device_exp_avg_sqs)\n","\n","                # Use the max. for normalizing running avg. of gradient\n","                max_exp_avg_sq_sqrt = torch._foreach_sqrt(device_max_exp_avg_sqs)\n","                torch._foreach_div_(max_exp_avg_sq_sqrt, bias_correction2_sqrt)\n","                denom = torch._foreach_add(max_exp_avg_sq_sqrt, eps)\n","            else:\n","                exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n","                torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n","                denom = torch._foreach_add(exp_avg_sq_sqrt, eps)\n","\n","            torch._foreach_addcdiv_(params_, device_exp_avgs, denom, step_size)"]},{"cell_type":"code","execution_count":6,"id":"f0403f82","metadata":{"execution":{"iopub.execute_input":"2024-02-28T03:09:22.244167Z","iopub.status.busy":"2024-02-28T03:09:22.243703Z","iopub.status.idle":"2024-02-28T03:09:22.259744Z","shell.execute_reply":"2024-02-28T03:09:22.258264Z"},"papermill":{"duration":0.023136,"end_time":"2024-02-28T03:09:22.26221","exception":false,"start_time":"2024-02-28T03:09:22.239074","status":"completed"},"tags":[]},"outputs":[],"source":["def _fused_adam(\n","    params: List[Tensor],\n","    grads: List[Tensor],\n","    exp_avgs: List[Tensor],\n","    exp_avg_sqs: List[Tensor],\n","    max_exp_avg_sqs: List[Tensor],\n","    state_steps: List[Tensor],\n","    grad_scale: Optional[Tensor],\n","    found_inf: Optional[Tensor],\n","    *,\n","    amsgrad: bool,\n","    beta1: float,\n","    beta2: float,\n","    lr: float,\n","    weight_decay: float,\n","    eps: float,\n","    maximize: bool,\n","    capturable: bool,  # Needed for consistency.\n","    differentiable: bool,\n",") -> None:\n","    grouped_tensors = _group_tensors_by_device_and_dtype([params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps])\n","    grad_scale_dict = {grad_scale.device: grad_scale} if grad_scale is not None else None\n","    found_inf_dict = {found_inf.device: found_inf} if found_inf is not None else None\n","    grouped_tensors = _group_tensors_by_device_and_dtype([params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps])\n","    for (device, dtype) in grouped_tensors:\n","        (\n","            device_params,\n","            device_grads,\n","            device_exp_avgs,\n","            device_exp_avg_sqs,\n","            device_max_exp_avg_sqs,\n","            device_state_steps,\n","        ) = grouped_tensors[(device, dtype)]\n","        if grad_scale is not None and found_inf is not None:\n","            if device not in grad_scale_dict:\n","                grad_scale_dict[device] = grad_scale.to(device, non_blocking=True)\n","            if found_inf not in found_inf_dict:\n","                found_inf_dict[device] = found_inf.to(device, non_blocking=True)\n","            device_grad_scale = grad_scale_dict[device]\n","            device_found_inf = found_inf_dict[device]\n","        else:\n","            device_grad_scale = None\n","            device_found_inf = None\n","        torch._foreach_add_(device_state_steps, 1)\n","        torch._fused_adam_(\n","            device_params,\n","            device_grads,\n","            device_exp_avgs,\n","            device_exp_avg_sqs,\n","            device_max_exp_avg_sqs,\n","            device_state_steps,\n","            amsgrad=amsgrad,\n","            lr=lr,\n","            beta1=beta1,\n","            beta2=beta2,\n","            weight_decay=weight_decay,\n","            eps=eps,\n","            maximize=maximize,\n","            grad_scale=device_grad_scale,\n","            found_inf=device_found_inf,\n","        )\n","        if device_found_inf is not None:\n","            torch._foreach_sub_(device_state_steps, [device_found_inf] * len(device_state_steps))"]},{"cell_type":"markdown","id":"da421f9f","metadata":{"papermill":{"duration":0.002526,"end_time":"2024-02-28T03:09:22.267829","exception":false,"start_time":"2024-02-28T03:09:22.265303","status":"completed"},"tags":[]},"source":["# Citation List\n","\n","* https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/optimizer_schedules#transformers.AdamW\n","* https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW\n","* https://nn.labml.ai/optimizers/adam.html\n","* https://arxiv.org/pdf/1412.6980.pdf\n","* https://arxiv.org/abs/1711.05101"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":8.199361,"end_time":"2024-02-28T03:09:23.293917","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-28T03:09:15.094556","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}