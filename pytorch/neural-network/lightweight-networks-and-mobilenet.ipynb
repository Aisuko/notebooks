{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/lightweight-networks-and-mobilenet?scriptVersionId=164483767\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"079c8379","metadata":{"papermill":{"duration":0.004932,"end_time":"2024-02-27T05:15:34.875216","exception":false,"start_time":"2024-02-27T05:15:34.870284","status":"completed"},"tags":[]},"source":["# Overview\n","\n","We have seen that complex network require significant resources, such as GPU, for training, and also for fast inference. However, it turns out that a model with significantly smaller number of parameters in most cases can still be trained to perform reasonably well. In other worlds, increase in the model complexity typically results in small(non-proportional) increase in the model performance. Increasing the number of CNN layer and/or number of neurons in the classifier allowed us to gain a few percents of accuracy at most. We have notebooks talk about this see [Converlutional Neural Network](https://www.kaggle.com/code/aisuko/convolutional-neural-network) and [Multilayer Dense Convilutional Neural Network](https://www.kaggle.com/code/aisuko/multilayer-dense-convolutional-neural-network)\n","\n","This leads us to the idea that we can experiment with `Lightweight network architectures` in order to train faster models. This is especially important if we want to be able to execute our models on mobile devices. This module will rely on the Cats and Dogs dataset."]},{"cell_type":"code","execution_count":null,"id":"dd9121d9","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:10:40.673361Z","iopub.status.busy":"2024-02-27T05:10:40.672928Z","iopub.status.idle":"2024-02-27T05:10:44.155978Z","shell.execute_reply":"2024-02-27T05:10:44.154772Z","shell.execute_reply.started":"2024-02-27T05:10:40.673327Z"},"papermill":{"duration":0.003735,"end_time":"2024-02-27T05:15:34.883137","exception":false,"start_time":"2024-02-27T05:15:34.879402","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"id":"00728b4e","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:34.892848Z","iopub.status.busy":"2024-02-27T05:15:34.892362Z","iopub.status.idle":"2024-02-27T05:15:38.317903Z","shell.execute_reply":"2024-02-27T05:15:38.316927Z"},"papermill":{"duration":3.432995,"end_time":"2024-02-27T05:15:38.320023","exception":false,"start_time":"2024-02-27T05:15:34.887028","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import os\n","import torch\n","import warnings\n","\n","if torch.cuda.is_available():\n","    torch_device = 'cuda'\n","else:\n","    torch_device = 'cpu'\n","\n","warnings.filterwarnings('ignore')\n","\n","print(torch_device)"]},{"cell_type":"code","execution_count":2,"id":"8aacdcea","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:38.329829Z","iopub.status.busy":"2024-02-27T05:15:38.329286Z","iopub.status.idle":"2024-02-27T05:15:44.74746Z","shell.execute_reply":"2024-02-27T05:15:44.746334Z"},"papermill":{"duration":6.425564,"end_time":"2024-02-27T05:15:44.7498","exception":false,"start_time":"2024-02-27T05:15:38.324236","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-02-27 05:15:39--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\r\n","Resolving download.microsoft.com (download.microsoft.com)... 104.123.44.196, 2a02:26f0:6d00:3b6::317f, 2a02:26f0:6d00:39f::317f\r\n","Connecting to download.microsoft.com (download.microsoft.com)|104.123.44.196|:443... connected.\r\n","HTTP request sent, awaiting response... 200 OK\r\n","Length: 824887076 (787M) [application/octet-stream]\r\n","Saving to: ‘data/kagglecatsanddogs_5340.zip’\r\n","\r\n","kagglecatsanddogs_5 100%[===================>] 786.67M   147MB/s    in 5.4s    \r\n","\r\n","2024-02-27 05:15:44 (147 MB/s) - ‘data/kagglecatsanddogs_5340.zip’ saved [824887076/824887076]\r\n","\r\n"]}],"source":["if not os.path.exists('data/kagglecatsanddogs_5340.zip'):\n","    !wget -P data https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip"]},{"cell_type":"code","execution_count":3,"id":"30423607","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:44.764974Z","iopub.status.busy":"2024-02-27T05:15:44.764613Z","iopub.status.idle":"2024-02-27T05:15:55.264293Z","shell.execute_reply":"2024-02-27T05:15:55.263462Z"},"papermill":{"duration":10.509865,"end_time":"2024-02-27T05:15:55.266583","exception":false,"start_time":"2024-02-27T05:15:44.756718","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Corrupt image: data/PetImages/Cat/666.jpg\n","Corrupt image: data/PetImages/Dog/11702.jpg\n"]}],"source":["import glob\n","import zipfile\n","import torchvision\n","from PIL import Image\n","\n","\n","def check_image(fn):\n","    try:\n","        im = Image.open(fn)\n","        im.verify()\n","        return True\n","    except:\n","        return False\n","\n","    \n","def check_image_dir(path):\n","    for fn in glob.glob(path):\n","        if not check_image(fn):\n","            print(\"Corrupt image: {}\".format(fn))\n","            os.remove(fn)\n","\n","            \n","def common_transform():\n","    # torchvision.transforms.Normalize is used to normalize a tensor image with mean and standard deviation.\n","    std_normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                                     std=[0.229, 0.224, 0.225])\n","    # torchvision.transforms.Compose is used to compose several transforms together in order to do data augmentation.\n","    trans = torchvision.transforms.Compose([\n","        torchvision.transforms.Resize(256), # resize the image to 256x256\n","        torchvision.transforms.CenterCrop(224), # crop the image to 224x224 about the center\n","        torchvision.transforms.ToTensor(), # convert the image to a tensor with pixel values in the range [0, 1]\n","        std_normalize])\n","    return trans\n","\n","\n","def load_cats_dogs_dataset():\n","    if not os.path.exists('data/PetImages'):\n","        with zipfile.ZipFile('data/kagglecatsanddogs_5340.zip', 'r') as zip_ref:\n","            zip_ref.extractall('data')\n","    \n","    check_image_dir('data/PetImages/Cat/*.jpg')\n","    check_image_dir('data/PetImages/Dog/*.jpg')\n","\n","    dataset = torchvision.datasets.ImageFolder('data/PetImages', transform=common_transform())\n","    trainset, testset = torch.utils.data.random_split(dataset, [20000, len(dataset) - 20000])\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) # num_workers: how many subprocesses to use for data loading\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n","    return dataset, trainloader, testloader\n","\n","dataset, trainloader, testloader = load_cats_dogs_dataset()"]},{"cell_type":"markdown","id":"57ab317f","metadata":{"papermill":{"duration":0.010514,"end_time":"2024-02-27T05:15:55.284277","exception":false,"start_time":"2024-02-27T05:15:55.273763","status":"completed"},"tags":[]},"source":["# MobileNet\n","\n","In the previous notebook, we habve seen [ResNet architecture](https://www.kaggle.com/code/aisuko/pre-trained-models-and-transfer-learning) for image classification. More lightweight analog of ResNet is **MobileNet**, which uses so-called *Inverted Residual Blocks*. Let's load pre-trained mobilenet and see how it works:"]},{"cell_type":"code","execution_count":4,"id":"0d2b51a5","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:55.302272Z","iopub.status.busy":"2024-02-27T05:15:55.301883Z","iopub.status.idle":"2024-02-27T05:15:57.455654Z","shell.execute_reply":"2024-02-27T05:15:57.454623Z"},"papermill":{"duration":2.164343,"end_time":"2024-02-27T05:15:57.457795","exception":false,"start_time":"2024-02-27T05:15:55.293452","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","100%|██████████| 13.6M/13.6M [00:00<00:00, 86.9MB/s]\n"]},{"data":{"text/plain":["MobileNetV2(\n","  (features): Sequential(\n","    (0): Conv2dNormActivation(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","    (1): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (7): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (8): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (9): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (10): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (11): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (12): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (13): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (14): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (15): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (16): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (17): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (18): Conv2dNormActivation(\n","      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.2, inplace=False)\n","    (1): Linear(in_features=1280, out_features=1000, bias=True)\n","  )\n",")"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n","model.to(torch_device).eval()"]},{"cell_type":"code","execution_count":5,"id":"3eab8fd6","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:57.474272Z","iopub.status.busy":"2024-02-27T05:15:57.473913Z","iopub.status.idle":"2024-02-27T05:15:58.138612Z","shell.execute_reply":"2024-02-27T05:15:58.137638Z"},"papermill":{"duration":0.675554,"end_time":"2024-02-27T05:15:58.141061","exception":false,"start_time":"2024-02-27T05:15:57.465507","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(281, device='cuda:0')\n"]}],"source":["# Apply the model to the dataset and visualize the results\n","sample_image = dataset[0][0].unsqueeze(0).to(torch_device) # unsqueeze(0): add a dimension of size 1 at the 0th position\n","res = model(sample_image) # apply the model to the sample image\n","print(res[0].argmax()) # get the index of the highest probability"]},{"cell_type":"markdown","id":"1994791a","metadata":{"papermill":{"duration":0.007616,"end_time":"2024-02-27T05:15:58.156551","exception":false,"start_time":"2024-02-27T05:15:58.148935","status":"completed"},"tags":[]},"source":["# Transfer Learning(Fine-tuning)"]},{"cell_type":"code","execution_count":6,"id":"3ea372bf","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:58.173979Z","iopub.status.busy":"2024-02-27T05:15:58.173321Z","iopub.status.idle":"2024-02-27T05:15:58.178859Z","shell.execute_reply":"2024-02-27T05:15:58.177957Z"},"papermill":{"duration":0.016566,"end_time":"2024-02-27T05:15:58.180755","exception":false,"start_time":"2024-02-27T05:15:58.164189","status":"completed"},"tags":[]},"outputs":[],"source":["# Frezze the original weights\n","for x in model.parameters():\n","    x.requires_grad = False"]},{"cell_type":"code","execution_count":7,"id":"2444f4ad","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:58.197434Z","iopub.status.busy":"2024-02-27T05:15:58.197164Z","iopub.status.idle":"2024-02-27T05:15:58.267302Z","shell.execute_reply":"2024-02-27T05:15:58.266419Z"},"papermill":{"duration":0.080885,"end_time":"2024-02-27T05:15:58.269421","exception":false,"start_time":"2024-02-27T05:15:58.188536","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["====================================================================================================\n","Layer (type:depth-idx)                             Output Shape              Param #\n","====================================================================================================\n","MobileNetV2                                        [1, 2]                    --\n","├─Sequential: 1-1                                  [1, 1280, 7, 7]           --\n","│    └─Conv2dNormActivation: 2-1                   [1, 32, 112, 112]         --\n","│    │    └─Conv2d: 3-1                            [1, 32, 112, 112]         (864)\n","│    │    └─BatchNorm2d: 3-2                       [1, 32, 112, 112]         (64)\n","│    │    └─ReLU6: 3-3                             [1, 32, 112, 112]         --\n","│    └─InvertedResidual: 2-2                       [1, 16, 112, 112]         --\n","│    │    └─Sequential: 3-4                        [1, 16, 112, 112]         (896)\n","│    └─InvertedResidual: 2-3                       [1, 24, 56, 56]           --\n","│    │    └─Sequential: 3-5                        [1, 24, 56, 56]           (5,136)\n","│    └─InvertedResidual: 2-4                       [1, 24, 56, 56]           --\n","│    │    └─Sequential: 3-6                        [1, 24, 56, 56]           (8,832)\n","│    └─InvertedResidual: 2-5                       [1, 32, 28, 28]           --\n","│    │    └─Sequential: 3-7                        [1, 32, 28, 28]           (10,000)\n","│    └─InvertedResidual: 2-6                       [1, 32, 28, 28]           --\n","│    │    └─Sequential: 3-8                        [1, 32, 28, 28]           (14,848)\n","│    └─InvertedResidual: 2-7                       [1, 32, 28, 28]           --\n","│    │    └─Sequential: 3-9                        [1, 32, 28, 28]           (14,848)\n","│    └─InvertedResidual: 2-8                       [1, 64, 14, 14]           --\n","│    │    └─Sequential: 3-10                       [1, 64, 14, 14]           (21,056)\n","│    └─InvertedResidual: 2-9                       [1, 64, 14, 14]           --\n","│    │    └─Sequential: 3-11                       [1, 64, 14, 14]           (54,272)\n","│    └─InvertedResidual: 2-10                      [1, 64, 14, 14]           --\n","│    │    └─Sequential: 3-12                       [1, 64, 14, 14]           (54,272)\n","│    └─InvertedResidual: 2-11                      [1, 64, 14, 14]           --\n","│    │    └─Sequential: 3-13                       [1, 64, 14, 14]           (54,272)\n","│    └─InvertedResidual: 2-12                      [1, 96, 14, 14]           --\n","│    │    └─Sequential: 3-14                       [1, 96, 14, 14]           (66,624)\n","│    └─InvertedResidual: 2-13                      [1, 96, 14, 14]           --\n","│    │    └─Sequential: 3-15                       [1, 96, 14, 14]           (118,272)\n","│    └─InvertedResidual: 2-14                      [1, 96, 14, 14]           --\n","│    │    └─Sequential: 3-16                       [1, 96, 14, 14]           (118,272)\n","│    └─InvertedResidual: 2-15                      [1, 160, 7, 7]            --\n","│    │    └─Sequential: 3-17                       [1, 160, 7, 7]            (155,264)\n","│    └─InvertedResidual: 2-16                      [1, 160, 7, 7]            --\n","│    │    └─Sequential: 3-18                       [1, 160, 7, 7]            (320,000)\n","│    └─InvertedResidual: 2-17                      [1, 160, 7, 7]            --\n","│    │    └─Sequential: 3-19                       [1, 160, 7, 7]            (320,000)\n","│    └─InvertedResidual: 2-18                      [1, 320, 7, 7]            --\n","│    │    └─Sequential: 3-20                       [1, 320, 7, 7]            (473,920)\n","│    └─Conv2dNormActivation: 2-19                  [1, 1280, 7, 7]           --\n","│    │    └─Conv2d: 3-21                           [1, 1280, 7, 7]           (409,600)\n","│    │    └─BatchNorm2d: 3-22                      [1, 1280, 7, 7]           (2,560)\n","│    │    └─ReLU6: 3-23                            [1, 1280, 7, 7]           --\n","├─Linear: 1-2                                      [1, 2]                    2,562\n","====================================================================================================\n","Total params: 2,226,434\n","Trainable params: 2,562\n","Non-trainable params: 2,223,872\n","Total mult-adds (M): 299.53\n","====================================================================================================\n","Input size (MB): 0.60\n","Forward/backward pass size (MB): 106.85\n","Params size (MB): 8.91\n","Estimated Total Size (MB): 116.36\n","===================================================================================================="]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn as nn\n","from torchinfo import summary\n","\n","# Replace the final classifier\n","model.classifier = nn.Linear(1280,2)  # change the last layer to a linear layer with 2 outputs\n","model = model.to(torch_device)\n","summary(model, input_size=(1, 3, 224, 224))"]},{"cell_type":"markdown","id":"5cd22087","metadata":{"papermill":{"duration":0.007633,"end_time":"2024-02-27T05:15:58.284936","exception":false,"start_time":"2024-02-27T05:15:58.277303","status":"completed"},"tags":[]},"source":["# Training"]},{"cell_type":"code","execution_count":8,"id":"f97e32a9","metadata":{"execution":{"iopub.execute_input":"2024-02-27T05:15:58.301731Z","iopub.status.busy":"2024-02-27T05:15:58.301415Z","iopub.status.idle":"2024-02-27T05:16:58.523484Z","shell.execute_reply":"2024-02-27T05:16:58.522186Z"},"papermill":{"duration":60.232969,"end_time":"2024-02-27T05:16:58.525549","exception":false,"start_time":"2024-02-27T05:15:58.29258","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, iter 0, loss=0.024, acc=0.438\n","Epoch 0, iter 90, loss=0.007, acc=0.925\n","Epoch 0, iter 180, loss=0.005, acc=0.940\n","Epoch 0, iter 270, loss=0.006, acc=0.941\n","Epoch 0, iter 360, loss=0.006, acc=0.945\n","Epoch 0, iter 450, loss=0.006, acc=0.949\n","Epoch 0, iter 540, loss=0.006, acc=0.949\n","Epoch 0, val_loss=0.008, val_acc=0.944\n"]}],"source":["def validate(net, dataloader, loss_fn=nn.NLLLoss()):\n","    net.eval() # put the network into evaluation mode to deactivate the dropout layers\n","    count,acc,loss =0,0,0\n","    with torch.no_grad(): # deactivate autograd to save memory and speed up computations\n","        for features, labels in dataloader:\n","            features,labels = features.to(torch_device), labels.to(torch_device)\n","            out=net(features) # forward pass of the mini-batch through the network to obtain the outputs\n","            loss += loss_fn(out,labels) # compute the loss\n","            preds=torch.max(out,dim=1)[1] # compute the predictions to obtain the accuracy\n","            acc+=(preds==labels).sum() # accumulate the correct predictions\n","            count+=len(labels) # accumulate the total number of examples\n","    return loss.item()/count, acc.item()/count # return the loss and accuracy\n","\n","def train_long(net, train_loader, test_loader, epochs=5, lr=0.01, optimizer=None, loss_fn=nn.NLLLoss(), print_freq=10):\n","    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr) # use Adam optimizer if not provided\n","    for epoch in range(epochs):\n","        net.train() # put the network into training mode make sure the parameters are trainable\n","        total_loss,acc,count =0,0,0\n","        for i, (features, labels) in enumerate(train_loader):\n","            lbls = labels.to(torch_device)\n","            optimizer.zero_grad() # reset the gradients to zero before each batch to avoid accumulation\n","            out=net(features.to(torch_device)) # forward pass of the mini-batch through the network to obtain the outputs\n","            loss = loss_fn(out, lbls) # compute the loss\n","            loss.backward() # compute the gradients of the loss with respect to all the parameters of the network\n","            optimizer.step() # update the parameters of the network using the gradients to minimize the loss\n","            total_loss+=loss # accumulate the loss for inspection\n","            _,preds=torch.max(out,dim=1) # compute the predictions to obtain the accuracy\n","            acc+=(preds==lbls).sum() # accumulate the correct predictions\n","            count+=len(lbls) # accumulate the total number of examples\n","            if i%print_freq==0:\n","                print(f'Epoch {epoch}, iter {i}, loss={total_loss.item()/count:.3f}, acc={acc.item()/count:.3f}')\n","        vl, va = validate(net, test_loader, loss_fn=loss_fn)\n","        print(f'Epoch {epoch}, val_loss={vl:.3f}, val_acc={va:.3f}')\n","\n","train_long(model, trainloader, testloader, loss_fn=torch.nn.CrossEntropyLoss(),epochs=1, print_freq=90)"]},{"cell_type":"markdown","id":"8a8ecd1f","metadata":{"papermill":{"duration":0.008014,"end_time":"2024-02-27T05:16:58.5423","exception":false,"start_time":"2024-02-27T05:16:58.534286","status":"completed"},"tags":[]},"source":["# Summary\n","\n","Notice that MobileNet results in almost the same accuracy as VGG-16, and just slightly lower than full-scale ResNet. The main advantage of small models, such as MobileNet or ResNet-18 is that they can be used on mobile devices, "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30528,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":95.510331,"end_time":"2024-02-27T05:17:00.374165","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-27T05:15:24.863834","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}