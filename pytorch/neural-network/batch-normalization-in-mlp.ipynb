{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f5955c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005933,
     "end_time": "2024-07-30T04:46:53.760413",
     "exception": false,
     "start_time": "2024-07-30T04:46:53.754480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "Batch normalization came out in 2015 from a team at google and it was an extremely impactful paper because it made possible to train every deep neural Nets quite reliably.\n",
    "\n",
    "In notebook [\"Kaiming init\" calculating the init scale](https://www.kaggle.com/code/aisuko/kaiming-init-calculating-the-init-scale), we want our hidden states value to be roughly unit gaussian, a unit or one standard deviation at least at initialization.\n",
    "\n",
    "We will implement normalize our hidden network stats to be unit gaussian by following paper [Batch Normalization](https://arxiv.org/pdf/1502.03167).\n",
    "\n",
    "We follow the Algorithm1 in the paper `Batch normalizing`.\n",
    "\n",
    "* Mini-batch mean\n",
    "* mini-batch variance\n",
    "* Normalize\n",
    "* Scale ans shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983d59c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:46:53.772263Z",
     "iopub.status.busy": "2024-07-30T04:46:53.771865Z",
     "iopub.status.idle": "2024-07-30T04:47:13.846188Z",
     "shell.execute_reply": "2024-07-30T04:47:13.844739Z"
    },
    "papermill": {
     "duration": 20.083287,
     "end_time": "2024-07-30T04:47:13.848775",
     "exception": false,
     "start_time": "2024-07-30T04:46:53.765488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 04:47:00.239847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 04:47:00.240002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 04:47:00.409438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n",
      "11897\n",
      "      0/ 200000: 3.3179\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# read in all the words\n",
    "with open('/kaggle/input/character-lm-without-framework/names.txt', 'r', encoding='utf-8') as f:\n",
    "    words=f.read()\n",
    "\n",
    "words=words.splitlines()\n",
    "\n",
    "# build the vocabulary of characters and \n",
    "chars=sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi={s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "itos={i:s for s,i in stoi.items()}\n",
    "vocab_size=len(itos)\n",
    "\n",
    "\n",
    "block_size=3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X,Y=[],[]\n",
    "    \n",
    "    for w in words:\n",
    "        context=[0]*block_size\n",
    "        for ch in w+'.':\n",
    "            ix=stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context=context[1:]+[ix] # crop and append\n",
    "            \n",
    "    X=torch.tensor(X)\n",
    "    Y=torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1=int(0.8*len(words))\n",
    "n2=int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr=build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev=build_dataset(words[n1:n2]) # 10%\n",
    "Xte, Yte=build_dataset(words[n2:])     # 10%\n",
    "\n",
    "\n",
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "parameters=[C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact=embcat@W1+b1 # hidden layer pre-activation\n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c5a5c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:13.861582Z",
     "iopub.status.busy": "2024-07-30T04:47:13.860238Z",
     "iopub.status.idle": "2024-07-30T04:47:13.869935Z",
     "shell.execute_reply": "2024-07-30T04:47:13.868445Z"
    },
    "papermill": {
     "duration": 0.019881,
     "end_time": "2024-07-30T04:47:13.873678",
     "exception": false,
     "start_time": "2024-07-30T04:47:13.853797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the hidden layer, we have 32 examples by 200 neurons\n",
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1e2441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:13.886461Z",
     "iopub.status.busy": "2024-07-30T04:47:13.885948Z",
     "iopub.status.idle": "2024-07-30T04:47:13.896192Z",
     "shell.execute_reply": "2024-07-30T04:47:13.894459Z"
    },
    "papermill": {
     "duration": 0.019836,
     "end_time": "2024-07-30T04:47:13.898778",
     "exception": false,
     "start_time": "2024-07-30T04:47:13.878942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross zero dimension and keep them as true\n",
    "\n",
    "# We doing the mean over all the elements in the batch \n",
    "hpreact.mean(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef2ce20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:13.911232Z",
     "iopub.status.busy": "2024-07-30T04:47:13.910790Z",
     "iopub.status.idle": "2024-07-30T04:47:13.921772Z",
     "shell.execute_reply": "2024-07-30T04:47:13.920638Z"
    },
    "papermill": {
     "duration": 0.020619,
     "end_time": "2024-07-30T04:47:13.924733",
     "exception": false,
     "start_time": "2024-07-30T04:47:13.904114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the standard deviation of these activations\n",
    "hpreact.std(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e30b6",
   "metadata": {
    "papermill": {
     "duration": 0.005068,
     "end_time": "2024-07-30T04:47:13.935369",
     "exception": false,
     "start_time": "2024-07-30T04:47:13.930301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How we standardize these values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36906c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:13.947763Z",
     "iopub.status.busy": "2024-07-30T04:47:13.947308Z",
     "iopub.status.idle": "2024-07-30T04:47:13.976853Z",
     "shell.execute_reply": "2024-07-30T04:47:13.975513Z"
    },
    "papermill": {
     "duration": 0.038952,
     "end_time": "2024-07-30T04:47:13.979507",
     "exception": false,
     "start_time": "2024-07-30T04:47:13.940555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n",
      "      0/ 200000: 3.3147\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "parameters=[C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact=embcat@W1+b1 # hidden layer pre-activation\n",
    "    \n",
    "    \n",
    "    # algorithm: we mini-batch =mean/standard_deviation\n",
    "    hpreact = (hpreact-hpreact.mean(0, keepdim=True))/ hpreact.std(0, keepdim=True)\n",
    "    \n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82460ae0",
   "metadata": {
    "papermill": {
     "duration": 0.005179,
     "end_time": "2024-07-30T04:47:13.990071",
     "exception": false,
     "start_time": "2024-07-30T04:47:13.984892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to do the normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb11fc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:14.002823Z",
     "iopub.status.busy": "2024-07-30T04:47:14.002406Z",
     "iopub.status.idle": "2024-07-30T04:47:14.021995Z",
     "shell.execute_reply": "2024-07-30T04:47:14.020902Z"
    },
    "papermill": {
     "duration": 0.028898,
     "end_time": "2024-07-30T04:47:14.024419",
     "exception": false,
     "start_time": "2024-07-30T04:47:13.995521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.3147\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "bngain=torch.ones((1, n_hidden))\n",
    "bnbias=torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters=[C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact=embcat@W1+b1 # hidden layer pre-activation\n",
    "    \n",
    "    \n",
    "    # algorithm: we mini-batch =mean/standard_deviation\n",
    "    hpreact = bngain*(hpreact-hpreact.mean(0, keepdim=True))/ hpreact.std(0, keepdim=True)+bnbias\n",
    "    \n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e759478",
   "metadata": {
    "papermill": {
     "duration": 0.00526,
     "end_time": "2024-07-30T04:47:14.035232",
     "exception": false,
     "start_time": "2024-07-30T04:47:14.029972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How do we do when the neural net in a forward pass estimates the statistics of the mean energy standard deviation of a batch?\n",
    "\n",
    "We would like to have a step after training that calculates and sets the batheoom mean and standard deviation a single time over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f17b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:14.047782Z",
     "iopub.status.busy": "2024-07-30T04:47:14.047396Z",
     "iopub.status.idle": "2024-07-30T04:47:14.569729Z",
     "shell.execute_reply": "2024-07-30T04:47:14.568684Z"
    },
    "papermill": {
     "duration": 0.531666,
     "end_time": "2024-07-30T04:47:14.572285",
     "exception": false,
     "start_time": "2024-07-30T04:47:14.040619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "# we won't call .backward below\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb=C[Xtr] # take the training set\n",
    "    embcat=emb.view(emb.shape[0], -1)\n",
    "    hpreact=embcat @ W1+b1 # get the pre-activations for every single training examples\n",
    "    # one single training time---measure the mean/std over the entire training set\n",
    "    bnmean=hpreact.mean(0, keepdim=True)\n",
    "    bnstd=hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136c2a64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:14.585124Z",
     "iopub.status.busy": "2024-07-30T04:47:14.584443Z",
     "iopub.status.idle": "2024-07-30T04:47:15.275354Z",
     "shell.execute_reply": "2024-07-30T04:47:15.273772Z"
    },
    "papermill": {
     "duration": 0.700457,
     "end_time": "2024-07-30T04:47:15.278297",
     "exception": false,
     "start_time": "2024-07-30T04:47:14.577840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3.268287420272827\n",
      "val 3.267815113067627\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y={\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb=C[x] # (N, block_size, n_embd)\n",
    "    embcat=emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    \n",
    "    hpreact=embcat@W1+b1\n",
    "#     hpreact=bngain*(hpreact=hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)+bnbias\n",
    "    hpreact=bngain*(hpreact-bnmean)/bnstd+bnbias\n",
    "    h=torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits=h@W2+b2 #(N, vocab_size)\n",
    "    loss=F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09802155",
   "metadata": {
    "papermill": {
     "duration": 0.005749,
     "end_time": "2024-07-30T04:47:15.291710",
     "exception": false,
     "start_time": "2024-07-30T04:47:15.285961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "Here is another idea from the paper that we don't want to define further step after training. We can define them at the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27565b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:47:15.305409Z",
     "iopub.status.busy": "2024-07-30T04:47:15.305002Z",
     "iopub.status.idle": "2024-07-30T04:51:26.246034Z",
     "shell.execute_reply": "2024-07-30T04:51:26.245029Z"
    },
    "papermill": {
     "duration": 250.950998,
     "end_time": "2024-07-30T04:51:26.248876",
     "exception": false,
     "start_time": "2024-07-30T04:47:15.297878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n",
      "      0/ 200000: 3.3239\n",
      "  10000/ 200000: 2.0322\n",
      "  20000/ 200000: 2.5675\n",
      "  30000/ 200000: 2.0125\n",
      "  40000/ 200000: 2.2446\n",
      "  50000/ 200000: 1.8897\n",
      "  60000/ 200000: 2.0785\n",
      "  70000/ 200000: 2.3681\n",
      "  80000/ 200000: 2.2918\n",
      "  90000/ 200000: 2.0238\n",
      " 100000/ 200000: 2.3673\n",
      " 110000/ 200000: 2.3132\n",
      " 120000/ 200000: 1.6414\n",
      " 130000/ 200000: 1.9311\n",
      " 140000/ 200000: 2.2231\n",
      " 150000/ 200000: 2.0027\n",
      " 160000/ 200000: 2.0997\n",
      " 170000/ 200000: 2.4949\n",
      " 180000/ 200000: 2.0199\n",
      " 190000/ 200000: 2.1707\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "# batch normalization has itself bias, so we remove b1\n",
    "# b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "bngain=torch.ones((1, n_hidden))\n",
    "bnbias=torch.zeros((1, n_hidden))\n",
    "bnmean_running=torch.zeros((1, n_hidden))\n",
    "bnstd_running=torch.ones((1, n_hidden))\n",
    "\n",
    "parameters=[C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact=embcat@W1 # +b1 # hidden layer pre-activation\n",
    "    \n",
    "    \n",
    "    bnmeani=hpreact.mean(0, keepdim=True)\n",
    "    bnstdi=hpreact.std(0, keepdim=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # algorithm: we mini-batch =mean/standard_deviation\n",
    "    hpreact = bngain*(hpreact-bnmeani)/ bnstdi+bnbias\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bnmean_running=0.999*bnmean_running+0.001*bnmeani\n",
    "        bnstd_running=0.999*bnstd_running+0.001*bnstdi\n",
    "    \n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "#     break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0847c21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:51:26.264633Z",
     "iopub.status.busy": "2024-07-30T04:51:26.264062Z",
     "iopub.status.idle": "2024-07-30T04:51:26.647554Z",
     "shell.execute_reply": "2024-07-30T04:51:26.646400Z"
    },
    "papermill": {
     "duration": 0.394401,
     "end_time": "2024-07-30T04:51:26.650437",
     "exception": false,
     "start_time": "2024-07-30T04:51:26.256036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "# we won't call .backward below\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb=C[Xtr] # take the training set\n",
    "    embcat=emb.view(emb.shape[0], -1)\n",
    "    hpreact=embcat @ W1 #+b1 # get the pre-activations for every single training examples\n",
    "    # one single training time---measure the mean/std over the entire training set\n",
    "    bnmean=hpreact.mean(0, keepdim=True)\n",
    "    bnstd=hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78cd5872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T04:51:26.666506Z",
     "iopub.status.busy": "2024-07-30T04:51:26.666081Z",
     "iopub.status.idle": "2024-07-30T04:51:27.294939Z",
     "shell.execute_reply": "2024-07-30T04:51:27.293500Z"
    },
    "papermill": {
     "duration": 0.640138,
     "end_time": "2024-07-30T04:51:27.297518",
     "exception": false,
     "start_time": "2024-07-30T04:51:26.657380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0677990913391113\n",
      "val 2.105621099472046\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y={\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb=C[x] # (N, block_size, n_embd)\n",
    "    embcat=emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    \n",
    "    hpreact=embcat@W1+b1\n",
    "#     hpreact=bngain*(hpreact=hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)+bnbias\n",
    "    hpreact=bngain*(hpreact-bnmean)/bnstd+bnbias\n",
    "    h=torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits=h@W2+b2 #(N, vocab_size)\n",
    "    loss=F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e32db",
   "metadata": {
    "papermill": {
     "duration": 0.006404,
     "end_time": "2024-07-30T04:51:27.310893",
     "exception": false,
     "start_time": "2024-07-30T04:51:27.304489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "* https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2443s\n",
    "* https://arxiv.org/pdf/1502.03167"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 187064505,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 279.447495,
   "end_time": "2024-07-30T04:51:30.034072",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-30T04:46:50.586577",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
