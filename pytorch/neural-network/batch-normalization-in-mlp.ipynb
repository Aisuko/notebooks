{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf86623",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005043,
     "end_time": "2024-07-30T05:29:31.142321",
     "exception": false,
     "start_time": "2024-07-30T05:29:31.137278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "Batch normalization came out in 2015 from a team at google and it was an extremely impactful paper because it made possible to train every deep neural Nets quite reliably.\n",
    "\n",
    "In notebook [\"Kaiming init\" calculating the init scale](https://www.kaggle.com/code/aisuko/kaiming-init-calculating-the-init-scale), we want our hidden states value to be roughly unit gaussian, a unit or one standard deviation at least at initialization.\n",
    "\n",
    "We will implement normalize our hidden network stats to be unit gaussian by following paper [Batch Normalization](https://arxiv.org/pdf/1502.03167).\n",
    "\n",
    "We follow the Algorithm1 in the paper `Batch normalizing`.\n",
    "\n",
    "* Mini-batch mean\n",
    "* mini-batch variance\n",
    "* Normalize\n",
    "* Scale ans shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b07fdfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:31.153635Z",
     "iopub.status.busy": "2024-07-30T05:29:31.153205Z",
     "iopub.status.idle": "2024-07-30T05:29:48.622988Z",
     "shell.execute_reply": "2024-07-30T05:29:48.621842Z"
    },
    "papermill": {
     "duration": 17.478101,
     "end_time": "2024-07-30T05:29:48.625288",
     "exception": false,
     "start_time": "2024-07-30T05:29:31.147187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 05:29:36.632986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 05:29:36.633132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 05:29:36.768535: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n",
      "11897\n",
      "      0/ 200000: 3.3179\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# read in all the words\n",
    "with open('/kaggle/input/character-lm-without-framework/names.txt', 'r', encoding='utf-8') as f:\n",
    "    words=f.read()\n",
    "\n",
    "words=words.splitlines()\n",
    "\n",
    "# build the vocabulary of characters and \n",
    "chars=sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi={s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "itos={i:s for s,i in stoi.items()}\n",
    "vocab_size=len(itos)\n",
    "\n",
    "\n",
    "block_size=3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X,Y=[],[]\n",
    "    \n",
    "    for w in words:\n",
    "        context=[0]*block_size\n",
    "        for ch in w+'.':\n",
    "            ix=stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context=context[1:]+[ix] # crop and append\n",
    "            \n",
    "    X=torch.tensor(X)\n",
    "    Y=torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1=int(0.8*len(words))\n",
    "n2=int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr=build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev=build_dataset(words[n1:n2]) # 10%\n",
    "Xte, Yte=build_dataset(words[n2:])     # 10%\n",
    "\n",
    "\n",
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "parameters=[C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact=embcat@W1+b1 # hidden layer pre-activation\n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d40c4392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:48.636954Z",
     "iopub.status.busy": "2024-07-30T05:29:48.636323Z",
     "iopub.status.idle": "2024-07-30T05:29:48.644262Z",
     "shell.execute_reply": "2024-07-30T05:29:48.643143Z"
    },
    "papermill": {
     "duration": 0.016182,
     "end_time": "2024-07-30T05:29:48.646374",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.630192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the hidden layer, we have 32 examples by 200 neurons\n",
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb1c7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:48.658424Z",
     "iopub.status.busy": "2024-07-30T05:29:48.658065Z",
     "iopub.status.idle": "2024-07-30T05:29:48.665696Z",
     "shell.execute_reply": "2024-07-30T05:29:48.664609Z"
    },
    "papermill": {
     "duration": 0.016557,
     "end_time": "2024-07-30T05:29:48.667941",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.651384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross zero dimension and keep them as true\n",
    "\n",
    "# We doing the mean over all the elements in the batch \n",
    "hpreact.mean(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981f040a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:48.680363Z",
     "iopub.status.busy": "2024-07-30T05:29:48.679371Z",
     "iopub.status.idle": "2024-07-30T05:29:48.688922Z",
     "shell.execute_reply": "2024-07-30T05:29:48.687689Z"
    },
    "papermill": {
     "duration": 0.018096,
     "end_time": "2024-07-30T05:29:48.691103",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.673007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the standard deviation of these activations\n",
    "hpreact.std(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b2ecf",
   "metadata": {
    "papermill": {
     "duration": 0.004822,
     "end_time": "2024-07-30T05:29:48.701035",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.696213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How we standardize these values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195bc074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:48.713340Z",
     "iopub.status.busy": "2024-07-30T05:29:48.712383Z",
     "iopub.status.idle": "2024-07-30T05:29:48.737030Z",
     "shell.execute_reply": "2024-07-30T05:29:48.735746Z"
    },
    "papermill": {
     "duration": 0.033606,
     "end_time": "2024-07-30T05:29:48.739644",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.706038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n",
      "      0/ 200000: 3.3147\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "parameters=[C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact=embcat@W1+b1 # hidden layer pre-activation\n",
    "    \n",
    "    \n",
    "    # algorithm: we mini-batch =mean/standard_deviation\n",
    "    hpreact = (hpreact-hpreact.mean(0, keepdim=True))/ hpreact.std(0, keepdim=True)\n",
    "    \n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa8acb",
   "metadata": {
    "papermill": {
     "duration": 0.004884,
     "end_time": "2024-07-30T05:29:48.749708",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.744824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to do the normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b24390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:48.761388Z",
     "iopub.status.busy": "2024-07-30T05:29:48.761014Z",
     "iopub.status.idle": "2024-07-30T05:29:48.779275Z",
     "shell.execute_reply": "2024-07-30T05:29:48.778135Z"
    },
    "papermill": {
     "duration": 0.02702,
     "end_time": "2024-07-30T05:29:48.781717",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.754697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.3147\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "bngain=torch.ones((1, n_hidden))\n",
    "bnbias=torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters=[C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    hpreact=embcat@W1+b1 # hidden layer pre-activation\n",
    "    \n",
    "    \n",
    "    # algorithm: we mini-batch =mean/standard_deviation\n",
    "    hpreact = bngain*(hpreact-hpreact.mean(0, keepdim=True))/ hpreact.std(0, keepdim=True)+bnbias\n",
    "    \n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "    break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3105e",
   "metadata": {
    "papermill": {
     "duration": 0.004956,
     "end_time": "2024-07-30T05:29:48.792083",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.787127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How do we do when the neural net in a forward pass estimates the statistics of the mean energy standard deviation of a batch?\n",
    "\n",
    "We would like to have a step after training that calculates and sets the batheoom mean and standard deviation a single time over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22719130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:48.804592Z",
     "iopub.status.busy": "2024-07-30T05:29:48.804200Z",
     "iopub.status.idle": "2024-07-30T05:29:49.270510Z",
     "shell.execute_reply": "2024-07-30T05:29:49.269595Z"
    },
    "papermill": {
     "duration": 0.475793,
     "end_time": "2024-07-30T05:29:49.273062",
     "exception": false,
     "start_time": "2024-07-30T05:29:48.797269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "# we won't call .backward below\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb=C[Xtr] # take the training set\n",
    "    embcat=emb.view(emb.shape[0], -1)\n",
    "    hpreact=embcat @ W1+b1 # get the pre-activations for every single training examples\n",
    "    # one single training time---measure the mean/std over the entire training set\n",
    "    bnmean=hpreact.mean(0, keepdim=True)\n",
    "    bnstd=hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32834a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:49.284950Z",
     "iopub.status.busy": "2024-07-30T05:29:49.284569Z",
     "iopub.status.idle": "2024-07-30T05:29:49.968451Z",
     "shell.execute_reply": "2024-07-30T05:29:49.967332Z"
    },
    "papermill": {
     "duration": 0.692465,
     "end_time": "2024-07-30T05:29:49.970804",
     "exception": false,
     "start_time": "2024-07-30T05:29:49.278339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3.268287420272827\n",
      "val 3.267815113067627\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y={\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb=C[x] # (N, block_size, n_embd)\n",
    "    embcat=emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    \n",
    "    hpreact=embcat@W1+b1\n",
    "#     hpreact=bngain*(hpreact=hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)+bnbias\n",
    "    hpreact=bngain*(hpreact-bnmean)/bnstd+bnbias\n",
    "    h=torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits=h@W2+b2 #(N, vocab_size)\n",
    "    loss=F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fc6ea",
   "metadata": {
    "papermill": {
     "duration": 0.005244,
     "end_time": "2024-07-30T05:29:49.981343",
     "exception": false,
     "start_time": "2024-07-30T05:29:49.976099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "Here is another idea from the paper that we don't want to define further step after training. We can define them at the training process.\n",
    "\n",
    "We are using batch normalization to control the statistics of activations in the neural net. It is a common to sprinkle batch normalization layer across the neural net and usally we will place it **after layers** that have **multiplications**, like a **linear layer** or a convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9ec67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:29:49.994091Z",
     "iopub.status.busy": "2024-07-30T05:29:49.993711Z",
     "iopub.status.idle": "2024-07-30T05:33:38.037151Z",
     "shell.execute_reply": "2024-07-30T05:33:38.035526Z"
    },
    "papermill": {
     "duration": 228.054022,
     "end_time": "2024-07-30T05:33:38.040799",
     "exception": false,
     "start_time": "2024-07-30T05:29:49.986777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n",
      "      0/ 200000: 3.3239\n",
      "  10000/ 200000: 2.0322\n",
      "  20000/ 200000: 2.5675\n",
      "  30000/ 200000: 2.0125\n",
      "  40000/ 200000: 2.2446\n",
      "  50000/ 200000: 1.8897\n",
      "  60000/ 200000: 2.0785\n",
      "  70000/ 200000: 2.3681\n",
      "  80000/ 200000: 2.2918\n",
      "  90000/ 200000: 2.0238\n",
      " 100000/ 200000: 2.3673\n",
      " 110000/ 200000: 2.3132\n",
      " 120000/ 200000: 1.6414\n",
      " 130000/ 200000: 1.9311\n",
      " 140000/ 200000: 2.2231\n",
      " 150000/ 200000: 2.0027\n",
      " 160000/ 200000: 2.0997\n",
      " 170000/ 200000: 2.4949\n",
      " 180000/ 200000: 2.0199\n",
      " 190000/ 200000: 2.1707\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "n_embd=10 # the dimensionality of the character embedding vectors\n",
    "n_hidden=200 # the number of neurons in the hidden layer of the MLP\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi=[]\n",
    "writer=SummaryWriter()\n",
    "\n",
    "g=torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C=torch.randn((vocab_size, n_embd),           generator=g)\n",
    "W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n",
    "# batch normalization has itself bias, so we remove b1\n",
    "# b1=torch.randn(n_hidden,                      generator=g) * 0.01\n",
    "W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n",
    "b2=torch.randn(vocab_size,                    generator=g) * 0\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain=torch.ones((1, n_hidden))\n",
    "bnbias=torch.zeros((1, n_hidden))\n",
    "# two buffers, running means and running standard diviation of \n",
    "bnmean_running=torch.zeros((1, n_hidden))\n",
    "bnstd_running=torch.ones((1, n_hidden))\n",
    "\n",
    "parameters=[C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n",
    "    \n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb=C[Xb] # embed the characters into vectors\n",
    "    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    \n",
    "    # Linear layer\n",
    "    hpreact=embcat@W1 # +b1 # hidden layer pre-activation\n",
    "    \n",
    "    # BatchNorm Layer\n",
    "    # calculate the means and standard deviation of the activation that are feeding into the batchnorm layer\n",
    "    bnmeani=hpreact.mean(0, keepdim=True)\n",
    "    bnstdi=hpreact.std(0, keepdim=True)\n",
    "\n",
    "    # it's centering that batch to be unit gaussian\n",
    "    # and then it's offsetting and scaling it by the learned bias and gain\n",
    "    # algorithm: we mini-batch =mean/standard_deviation\n",
    "    hpreact = bngain*(hpreact-bnmeani)/ bnstdi+bnbias\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bnmean_running=0.999*bnmean_running+0.001*bnmeani\n",
    "        bnstd_running=0.999*bnstd_running+0.001*bnstdi\n",
    "    \n",
    "    \n",
    "    # If all of these outputs h are in the flat regions of -1 and 1, \n",
    "    # then the gradients that are flowing through the network will just get\n",
    "    # destroyed at this layer.\n",
    "    h=torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "    logits=h@W2+b2 # output layer\n",
    "    loss=F.cross_entropy(logits, Yb) # loss function\n",
    "    writer.add_scalar(\"Loss/train\", loss, i)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data+=-lr*p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i%10000 ==0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "#     break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4720e4a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:33:38.060378Z",
     "iopub.status.busy": "2024-07-30T05:33:38.059319Z",
     "iopub.status.idle": "2024-07-30T05:33:38.402505Z",
     "shell.execute_reply": "2024-07-30T05:33:38.401580Z"
    },
    "papermill": {
     "duration": 0.355722,
     "end_time": "2024-07-30T05:33:38.405082",
     "exception": false,
     "start_time": "2024-07-30T05:33:38.049360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "# we won't call .backward below\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb=C[Xtr] # take the training set\n",
    "    embcat=emb.view(emb.shape[0], -1)\n",
    "    hpreact=embcat @ W1 #+b1 # get the pre-activations for every single training examples\n",
    "    # one single training time---measure the mean/std over the entire training set\n",
    "    bnmean=hpreact.mean(0, keepdim=True)\n",
    "    bnstd=hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d2dab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T05:33:38.420128Z",
     "iopub.status.busy": "2024-07-30T05:33:38.419746Z",
     "iopub.status.idle": "2024-07-30T05:33:39.020848Z",
     "shell.execute_reply": "2024-07-30T05:33:39.019623Z"
    },
    "papermill": {
     "duration": 0.611482,
     "end_time": "2024-07-30T05:33:39.023096",
     "exception": false,
     "start_time": "2024-07-30T05:33:38.411614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0677990913391113\n",
      "val 2.105621099472046\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y={\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb=C[x] # (N, block_size, n_embd)\n",
    "    embcat=emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    \n",
    "    hpreact=embcat@W1+b1\n",
    "#     hpreact=bngain*(hpreact=hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)+bnbias\n",
    "    hpreact=bngain*(hpreact-bnmean)/bnstd+bnbias\n",
    "    h=torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits=h@W2+b2 #(N, vocab_size)\n",
    "    loss=F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a62e9",
   "metadata": {
    "papermill": {
     "duration": 0.006341,
     "end_time": "2024-07-30T05:33:39.036066",
     "exception": false,
     "start_time": "2024-07-30T05:33:39.029725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "* https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2443s\n",
    "* https://arxiv.org/pdf/1502.03167"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 187064505,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 253.121203,
   "end_time": "2024-07-30T05:33:41.623713",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-30T05:29:28.502510",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
