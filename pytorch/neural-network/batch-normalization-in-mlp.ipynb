{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/batch-normalization-in-mlp?scriptVersionId=190516942\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"f25c7864","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.006555,"end_time":"2024-07-31T00:25:17.644852","exception":false,"start_time":"2024-07-31T00:25:17.638297","status":"completed"},"tags":[]},"source":["# Overview\n","\n","Batch normalization came out in 2015 from a team at google and it was an extremely impactful paper because it made possible to train every deep neural Nets quite reliably.\n","\n","In notebook [\"Kaiming init\" calculating the init scale](https://www.kaggle.com/code/aisuko/kaiming-init-calculating-the-init-scale), we want our hidden states value to be roughly unit gaussian, a unit or one standard deviation at least at initialization.\n","\n","We will implement normalize our hidden network stats to be unit gaussian by following paper [Batch Normalization](https://arxiv.org/pdf/1502.03167).\n","\n","We follow the Algorithm1 in the paper `Batch normalizing`.\n","\n","* Mini-batch mean\n","* mini-batch variance\n","* Normalize\n","* Scale ans shift"]},{"cell_type":"code","execution_count":1,"id":"e495b63a","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:17.658964Z","iopub.status.busy":"2024-07-31T00:25:17.658582Z","iopub.status.idle":"2024-07-31T00:25:38.032071Z","shell.execute_reply":"2024-07-31T00:25:38.030599Z"},"papermill":{"duration":20.38273,"end_time":"2024-07-31T00:25:38.034698","exception":false,"start_time":"2024-07-31T00:25:17.651968","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-31 00:25:24.249562: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-31 00:25:24.249700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-31 00:25:24.418555: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([182625, 3]) torch.Size([182625])\n","torch.Size([22655, 3]) torch.Size([22655])\n","torch.Size([22866, 3]) torch.Size([22866])\n","11897\n","      0/ 200000: 3.3179\n"]}],"source":["import random\n","import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt # for making figures\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","# read in all the words\n","with open('/kaggle/input/character-lm-without-framework/names.txt', 'r', encoding='utf-8') as f:\n","    words=f.read()\n","\n","words=words.splitlines()\n","\n","# build the vocabulary of characters and \n","chars=sorted(list(set(''.join(words))))\n","\n","stoi={s:i+1 for i,s in enumerate(chars)}\n","stoi['.']=0\n","itos={i:s for s,i in stoi.items()}\n","vocab_size=len(itos)\n","\n","\n","block_size=3 # context length: how many characters do we take to predict the next one?\n","\n","\n","def build_dataset(words):\n","    X,Y=[],[]\n","    \n","    for w in words:\n","        context=[0]*block_size\n","        for ch in w+'.':\n","            ix=stoi[ch]\n","            X.append(context)\n","            Y.append(ix)\n","            context=context[1:]+[ix] # crop and append\n","            \n","    X=torch.tensor(X)\n","    Y=torch.tensor(Y)\n","    print(X.shape, Y.shape)\n","    return X,Y\n","\n","random.seed(42)\n","random.shuffle(words)\n","n1=int(0.8*len(words))\n","n2=int(0.9*len(words))\n","\n","Xtr, Ytr=build_dataset(words[:n1])     # 80%\n","Xdev, Ydev=build_dataset(words[n1:n2]) # 10%\n","Xte, Yte=build_dataset(words[n2:])     # 10%\n","\n","\n","# MLP\n","\n","n_embd=10 # the dimensionality of the character embedding vectors\n","n_hidden=200 # the number of neurons in the hidden layer of the MLP\n","max_steps=200000\n","batch_size=32\n","lossi=[]\n","writer=SummaryWriter()\n","\n","g=torch.Generator().manual_seed(2147483647) # for reproducibility\n","C=torch.randn((vocab_size, n_embd),           generator=g)\n","W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n","b1=torch.randn(n_hidden,                      generator=g) * 0.01\n","W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n","b2=torch.randn(vocab_size,                    generator=g) * 0\n","\n","parameters=[C, W1, b1, W2, b2]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","    p.requires_grad=True\n","    \n","\n","for i in range(max_steps):\n","    # minibatch construct\n","    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n","    \n","    # forward pass\n","    emb=C[Xb] # embed the characters into vectors\n","    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n","    hpreact=embcat@W1+b1 # hidden layer pre-activation\n","    \n","    # If all of these outputs h are in the flat regions of -1 and 1, \n","    # then the gradients that are flowing through the network will just get\n","    # destroyed at this layer.\n","    h=torch.tanh(hpreact) # hidden layer\n","\n","    logits=h@W2+b2 # output layer\n","    loss=F.cross_entropy(logits, Yb) # loss function\n","    writer.add_scalar(\"Loss/train\", loss, i)\n","    \n","    # backward pass\n","    for p in parameters:\n","        p.grad=None\n","    loss.backward()\n","    \n","    # update\n","    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n","    for p in parameters:\n","        p.data+=-lr*p.grad\n","    \n","    # track stats\n","    if i%10000 ==0:\n","        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","    \n","    break\n","\n","writer.flush()\n","writer.close()"]},{"cell_type":"code","execution_count":2,"id":"b309cf4e","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:38.048028Z","iopub.status.busy":"2024-07-31T00:25:38.046675Z","iopub.status.idle":"2024-07-31T00:25:38.055593Z","shell.execute_reply":"2024-07-31T00:25:38.054238Z"},"papermill":{"duration":0.017973,"end_time":"2024-07-31T00:25:38.05798","exception":false,"start_time":"2024-07-31T00:25:38.040007","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([32, 200])"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# in the hidden layer, we have 32 examples by 200 neurons\n","hpreact.shape"]},{"cell_type":"code","execution_count":3,"id":"a2f00e4f","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:38.070291Z","iopub.status.busy":"2024-07-31T00:25:38.069924Z","iopub.status.idle":"2024-07-31T00:25:38.078638Z","shell.execute_reply":"2024-07-31T00:25:38.077249Z"},"papermill":{"duration":0.017938,"end_time":"2024-07-31T00:25:38.081286","exception":false,"start_time":"2024-07-31T00:25:38.063348","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 200])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# cross zero dimension and keep them as true\n","\n","# We doing the mean over all the elements in the batch \n","hpreact.mean(0, keepdim=True).shape"]},{"cell_type":"code","execution_count":4,"id":"3261ba25","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:38.094274Z","iopub.status.busy":"2024-07-31T00:25:38.09389Z","iopub.status.idle":"2024-07-31T00:25:38.10341Z","shell.execute_reply":"2024-07-31T00:25:38.102348Z"},"papermill":{"duration":0.018995,"end_time":"2024-07-31T00:25:38.105937","exception":false,"start_time":"2024-07-31T00:25:38.086942","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 200])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# calculating the standard deviation of these activations\n","hpreact.std(0, keepdim=True).shape"]},{"cell_type":"markdown","id":"908356b1","metadata":{"papermill":{"duration":0.005196,"end_time":"2024-07-31T00:25:38.116687","exception":false,"start_time":"2024-07-31T00:25:38.111491","status":"completed"},"tags":[]},"source":["# How we standardize these values?"]},{"cell_type":"code","execution_count":5,"id":"6ab1937f","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:38.129521Z","iopub.status.busy":"2024-07-31T00:25:38.129131Z","iopub.status.idle":"2024-07-31T00:25:38.156073Z","shell.execute_reply":"2024-07-31T00:25:38.154912Z"},"papermill":{"duration":0.036191,"end_time":"2024-07-31T00:25:38.158475","exception":false,"start_time":"2024-07-31T00:25:38.122284","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["11897\n","      0/ 200000: 3.3147\n"]}],"source":["# MLP\n","\n","n_embd=10 # the dimensionality of the character embedding vectors\n","n_hidden=200 # the number of neurons in the hidden layer of the MLP\n","max_steps=200000\n","batch_size=32\n","lossi=[]\n","writer=SummaryWriter()\n","\n","g=torch.Generator().manual_seed(2147483647) # for reproducibility\n","C=torch.randn((vocab_size, n_embd),           generator=g)\n","W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n","b1=torch.randn(n_hidden,                      generator=g) * 0.01\n","W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n","b2=torch.randn(vocab_size,                    generator=g) * 0\n","\n","parameters=[C, W1, b1, W2, b2]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","    p.requires_grad=True\n","    \n","\n","for i in range(max_steps):\n","    # minibatch construct\n","    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n","    \n","    # forward pass\n","    emb=C[Xb] # embed the characters into vectors\n","    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n","    hpreact=embcat@W1+b1 # hidden layer pre-activation\n","    \n","    \n","    # algorithm: we mini-batch =mean/standard_deviation\n","    hpreact = (hpreact-hpreact.mean(0, keepdim=True))/ hpreact.std(0, keepdim=True)\n","    \n","    \n","    # If all of these outputs h are in the flat regions of -1 and 1, \n","    # then the gradients that are flowing through the network will just get\n","    # destroyed at this layer.\n","    h=torch.tanh(hpreact) # hidden layer\n","\n","    logits=h@W2+b2 # output layer\n","    loss=F.cross_entropy(logits, Yb) # loss function\n","    writer.add_scalar(\"Loss/train\", loss, i)\n","    \n","    # backward pass\n","    for p in parameters:\n","        p.grad=None\n","    loss.backward()\n","    \n","    # update\n","    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n","    for p in parameters:\n","        p.data+=-lr*p.grad\n","    \n","    # track stats\n","    if i%10000 ==0:\n","        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","    \n","    break\n","\n","writer.flush()\n","writer.close()"]},{"cell_type":"markdown","id":"d5beaa44","metadata":{"papermill":{"duration":0.005349,"end_time":"2024-07-31T00:25:38.169479","exception":false,"start_time":"2024-07-31T00:25:38.16413","status":"completed"},"tags":[]},"source":["# How to do the normalization?"]},{"cell_type":"code","execution_count":6,"id":"1f9d7120","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:38.18248Z","iopub.status.busy":"2024-07-31T00:25:38.182085Z","iopub.status.idle":"2024-07-31T00:25:38.202433Z","shell.execute_reply":"2024-07-31T00:25:38.201238Z"},"papermill":{"duration":0.029727,"end_time":"2024-07-31T00:25:38.204917","exception":false,"start_time":"2024-07-31T00:25:38.17519","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["12297\n","      0/ 200000: 3.3147\n"]}],"source":["# MLP\n","\n","n_embd=10 # the dimensionality of the character embedding vectors\n","n_hidden=200 # the number of neurons in the hidden layer of the MLP\n","max_steps=200000\n","batch_size=32\n","lossi=[]\n","writer=SummaryWriter()\n","\n","g=torch.Generator().manual_seed(2147483647) # for reproducibility\n","C=torch.randn((vocab_size, n_embd),           generator=g)\n","W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n","b1=torch.randn(n_hidden,                      generator=g) * 0.01\n","W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n","b2=torch.randn(vocab_size,                    generator=g) * 0\n","\n","bngain=torch.ones((1, n_hidden))\n","bnbias=torch.zeros((1, n_hidden))\n","\n","parameters=[C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","    p.requires_grad=True\n","    \n","\n","for i in range(max_steps):\n","    # minibatch construct\n","    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n","    \n","    # forward pass\n","    emb=C[Xb] # embed the characters into vectors\n","    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n","    hpreact=embcat@W1+b1 # hidden layer pre-activation\n","    \n","    \n","    # algorithm: we mini-batch =mean/standard_deviation\n","    hpreact = bngain*(hpreact-hpreact.mean(0, keepdim=True))/ hpreact.std(0, keepdim=True)+bnbias\n","    \n","    \n","    # If all of these outputs h are in the flat regions of -1 and 1, \n","    # then the gradients that are flowing through the network will just get\n","    # destroyed at this layer.\n","    h=torch.tanh(hpreact) # hidden layer\n","\n","    logits=h@W2+b2 # output layer\n","    loss=F.cross_entropy(logits, Yb) # loss function\n","    writer.add_scalar(\"Loss/train\", loss, i)\n","    \n","    # backward pass\n","    for p in parameters:\n","        p.grad=None\n","    loss.backward()\n","    \n","    # update\n","    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n","    for p in parameters:\n","        p.data+=-lr*p.grad\n","    \n","    # track stats\n","    if i%10000 ==0:\n","        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","    \n","    break\n","\n","writer.flush()\n","writer.close()"]},{"cell_type":"markdown","id":"684a8ad6","metadata":{"papermill":{"duration":0.005346,"end_time":"2024-07-31T00:25:38.21599","exception":false,"start_time":"2024-07-31T00:25:38.210644","status":"completed"},"tags":[]},"source":["# How do we do when the neural net in a forward pass estimates the statistics of the mean energy standard deviation of a batch?\n","\n","We would like to have a step after training that calculates and sets the batheoom mean and standard deviation a single time over the training set."]},{"cell_type":"code","execution_count":7,"id":"6371d540","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:38.229364Z","iopub.status.busy":"2024-07-31T00:25:38.228969Z","iopub.status.idle":"2024-07-31T00:25:38.741546Z","shell.execute_reply":"2024-07-31T00:25:38.740326Z"},"papermill":{"duration":0.522453,"end_time":"2024-07-31T00:25:38.744345","exception":false,"start_time":"2024-07-31T00:25:38.221892","status":"completed"},"tags":[]},"outputs":[],"source":["# calibrate the batch norm at the end of training\n","\n","# we won't call .backward below\n","with torch.no_grad():\n","    # pass the training set through\n","    emb=C[Xtr] # take the training set\n","    embcat=emb.view(emb.shape[0], -1)\n","    hpreact=embcat @ W1+b1 # get the pre-activations for every single training examples\n","    # one single training time---measure the mean/std over the entire training set\n","    bnmean=hpreact.mean(0, keepdim=True)\n","    bnstd=hpreact.std(0, keepdim=True)"]},{"cell_type":"code","execution_count":8,"id":"39270d07","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:38.757543Z","iopub.status.busy":"2024-07-31T00:25:38.757041Z","iopub.status.idle":"2024-07-31T00:25:39.442859Z","shell.execute_reply":"2024-07-31T00:25:39.441683Z"},"papermill":{"duration":0.695293,"end_time":"2024-07-31T00:25:39.445497","exception":false,"start_time":"2024-07-31T00:25:38.750204","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train 3.268287420272827\n","val 3.267815113067627\n"]}],"source":["@torch.no_grad()\n","def split_loss(split):\n","    x,y={\n","        'train': (Xtr, Ytr),\n","        'val': (Xdev, Ydev),\n","        'test': (Xte, Yte),\n","    }[split]\n","    emb=C[x] # (N, block_size, n_embd)\n","    embcat=emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","    \n","    hpreact=embcat@W1+b1\n","#     hpreact=bngain*(hpreact=hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)+bnbias\n","    hpreact=bngain*(hpreact-bnmean)/bnstd+bnbias\n","    h=torch.tanh(hpreact) # (N, n_hidden)\n","    logits=h@W2+b2 #(N, vocab_size)\n","    loss=F.cross_entropy(logits, y)\n","    print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')"]},{"cell_type":"markdown","id":"e3833734","metadata":{"papermill":{"duration":0.005356,"end_time":"2024-07-31T00:25:39.456719","exception":false,"start_time":"2024-07-31T00:25:39.451363","status":"completed"},"tags":[]},"source":["# Summary\n","\n","Here is another idea from the paper that we don't want to define further step after training. We can define them at the training process.\n","\n","We are using batch normalization to control the statistics of activations in the neural net. It is a common to sprinkle batch normalization layer across the neural net and usally we will place it **after layers** that have **multiplications**, like a **linear layer** or a convolutional layer."]},{"cell_type":"code","execution_count":9,"id":"afc620d7","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:25:39.469846Z","iopub.status.busy":"2024-07-31T00:25:39.46942Z","iopub.status.idle":"2024-07-31T00:29:45.185447Z","shell.execute_reply":"2024-07-31T00:29:45.184298Z"},"papermill":{"duration":245.725617,"end_time":"2024-07-31T00:29:45.188088","exception":false,"start_time":"2024-07-31T00:25:39.462471","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["12097\n","      0/ 200000: 3.3239\n","  10000/ 200000: 2.0322\n","  20000/ 200000: 2.5675\n","  30000/ 200000: 2.0125\n","  40000/ 200000: 2.2446\n","  50000/ 200000: 1.8897\n","  60000/ 200000: 2.0785\n","  70000/ 200000: 2.3681\n","  80000/ 200000: 2.2918\n","  90000/ 200000: 2.0238\n"," 100000/ 200000: 2.3673\n"," 110000/ 200000: 2.3132\n"," 120000/ 200000: 1.6414\n"," 130000/ 200000: 1.9311\n"," 140000/ 200000: 2.2231\n"," 150000/ 200000: 2.0027\n"," 160000/ 200000: 2.0997\n"," 170000/ 200000: 2.4949\n"," 180000/ 200000: 2.0199\n"," 190000/ 200000: 2.1707\n"]}],"source":["# MLP\n","\n","n_embd=10 # the dimensionality of the character embedding vectors\n","n_hidden=200 # the number of neurons in the hidden layer of the MLP\n","max_steps=200000\n","batch_size=32\n","lossi=[]\n","writer=SummaryWriter()\n","\n","g=torch.Generator().manual_seed(2147483647) # for reproducibility\n","C=torch.randn((vocab_size, n_embd),           generator=g)\n","W1=torch.randn((n_embd*block_size, n_hidden), generator=g) * ((5/3)/((n_embd*block_size)**0.5))\n","# batch normalization has itself bias, so we remove b1\n","# b1=torch.randn(n_hidden,                      generator=g) * 0.01\n","W2=torch.randn((n_hidden, vocab_size),        generator=g) * 0.01\n","b2=torch.randn(vocab_size,                    generator=g) * 0\n","\n","# BatchNorm parameters\n","bngain=torch.ones((1, n_hidden))\n","bnbias=torch.zeros((1, n_hidden))\n","# two buffers, running means and running standard diviation of \n","bnmean_running=torch.zeros((1, n_hidden))\n","bnstd_running=torch.ones((1, n_hidden))\n","\n","parameters=[C, W1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","    p.requires_grad=True\n","    \n","\n","for i in range(max_steps):\n","    # minibatch construct\n","    ix=torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","    Xb,Yb=Xtr[ix], Ytr[ix] # batch X,Y\n","    \n","    # forward pass\n","    emb=C[Xb] # embed the characters into vectors\n","    embcat=emb.view(emb.shape[0], -1) # concatenate the vectors\n","    \n","    # Linear layer\n","    hpreact=embcat@W1 # +b1 # hidden layer pre-activation\n","    \n","    # BatchNorm Layer\n","    # calculate the means and standard deviation of the activation that are feeding into the batchnorm layer\n","    bnmeani=hpreact.mean(0, keepdim=True)\n","    bnstdi=hpreact.std(0, keepdim=True)\n","\n","    # it's centering that batch to be unit gaussian\n","    # and then it's offsetting and scaling it by the learned bias and gain\n","    # algorithm: we mini-batch =mean/standard_deviation\n","    hpreact = bngain*(hpreact-bnmeani)/ bnstdi+bnbias\n","    \n","    with torch.no_grad():\n","        bnmean_running=0.999*bnmean_running+0.001*bnmeani\n","        bnstd_running=0.999*bnstd_running+0.001*bnstdi\n","    \n","    \n","    # If all of these outputs h are in the flat regions of -1 and 1, \n","    # then the gradients that are flowing through the network will just get\n","    # destroyed at this layer.\n","    h=torch.tanh(hpreact) # hidden layer\n","\n","    logits=h@W2+b2 # output layer\n","    loss=F.cross_entropy(logits, Yb) # loss function\n","    writer.add_scalar(\"Loss/train\", loss, i)\n","    \n","    # backward pass\n","    for p in parameters:\n","        p.grad=None\n","    loss.backward()\n","    \n","    # update\n","    lr=0.1 if i< 100000 else 0.01 # step learning rate decay\n","    for p in parameters:\n","        p.data+=-lr*p.grad\n","    \n","    # track stats\n","    if i%10000 ==0:\n","        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","    \n","#     break\n","\n","writer.flush()\n","writer.close()"]},{"cell_type":"code","execution_count":10,"id":"44cbee77","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:29:45.20398Z","iopub.status.busy":"2024-07-31T00:29:45.203553Z","iopub.status.idle":"2024-07-31T00:29:45.584898Z","shell.execute_reply":"2024-07-31T00:29:45.583532Z"},"papermill":{"duration":0.392492,"end_time":"2024-07-31T00:29:45.587621","exception":false,"start_time":"2024-07-31T00:29:45.195129","status":"completed"},"tags":[]},"outputs":[],"source":["# calibrate the batch norm at the end of training\n","\n","# we won't call .backward below\n","with torch.no_grad():\n","    # pass the training set through\n","    emb=C[Xtr] # take the training set\n","    embcat=emb.view(emb.shape[0], -1)\n","    hpreact=embcat @ W1 #+b1 # get the pre-activations for every single training examples\n","    # one single training time---measure the mean/std over the entire training set\n","    bnmean=hpreact.mean(0, keepdim=True)\n","    bnstd=hpreact.std(0, keepdim=True)"]},{"cell_type":"code","execution_count":11,"id":"e5f002c2","metadata":{"execution":{"iopub.execute_input":"2024-07-31T00:29:45.603325Z","iopub.status.busy":"2024-07-31T00:29:45.602924Z","iopub.status.idle":"2024-07-31T00:29:46.210838Z","shell.execute_reply":"2024-07-31T00:29:46.209258Z"},"papermill":{"duration":0.618893,"end_time":"2024-07-31T00:29:46.213562","exception":false,"start_time":"2024-07-31T00:29:45.594669","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["train 2.0677990913391113\n","val 2.105621099472046\n"]}],"source":["@torch.no_grad()\n","def split_loss(split):\n","    x,y={\n","        'train': (Xtr, Ytr),\n","        'val': (Xdev, Ydev),\n","        'test': (Xte, Yte),\n","    }[split]\n","    emb=C[x] # (N, block_size, n_embd)\n","    embcat=emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","    \n","    hpreact=embcat@W1+b1\n","#     hpreact=bngain*(hpreact=hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)+bnbias\n","    hpreact=bngain*(hpreact-bnmean)/bnstd+bnbias\n","    h=torch.tanh(hpreact) # (N, n_hidden)\n","    logits=h@W2+b2 #(N, vocab_size)\n","    loss=F.cross_entropy(logits, y)\n","    print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')"]},{"cell_type":"markdown","id":"d024b0d6","metadata":{"papermill":{"duration":0.006827,"end_time":"2024-07-31T00:29:46.227382","exception":false,"start_time":"2024-07-31T00:29:46.220555","status":"completed"},"tags":[]},"source":["# Summary\n","\n","It is important to understand the **activations** and the **gradient** and **their statistics in neural networks**. And it will become increasingly important especially as us make our neural networks bigger lager and deeper.\n","\n","We looked at the distributions basically at the output layer and we saw that if you have two confident mispredictions because the activations are too messed up at the last layer you can end up with these hockey stick losses. And if you fix this you can get a better loss at the end of training because your training is not doing wasteful work.\n","\n","And we also saw we need to control the activations we don't want them to squash to zero or explore to infinity and because that you can run into a lot of trouble with all of these non-linearities in these neural nets and basicallt you want everything to be fairly homogeneous throughout the neural net **you want roughly gaussian activations through the neural net**.\n","\n","\n","## How do we scale these weight matrices and biases during initialization of the neural net?\n","\n","In the ideal senario, we have the roughly gaussian activations through the neural net so everything is as controlled as possible. This will give us a large boost in improvement. However that strategy isn't actually possible for much much deeper neural nets because when you have much deeper neural nets with lots of different types of layers. It becomes really really hard to precisely set the weights and the biases in such a way that the activations are roughly uniform thorughout the neural net.\n","\n","So, we will use the normalization layer. There are many normalization layers:\n","* Batch normalization\n","* Layer normalization(more common)\n","* Constant normalization\n","* Group normalization(more common)\n","\n","Here we use batch normalization. We can saw how batch normalization works. It is a layer that we can sprinkle throughout our deep neural net and the basic idea is if you want roughly gaussian activations, then take your activations and take the mean understand deviation and center your data. \n","\n","You can do that because the centering operation is differentiable and on top of that we actaully had to add a lot of bells and whistles and that gave us a sense of the complexities of the patch normalization layer because now we're centering the data that's great but suddenly we need th gain and the bias and now those are trainable.\n","\n","## How do we do the inference after we coupling all the training examples?\n","\n","After we apply the batch normalization, we were coupling all the training examples suddenly. How to do the inference we needed to?\n","\n","We need to estimate those mean and standard deviation once or the entire training set and then use those at inference but then no one likes to do stage two. **Instead we fold everything into the batch normalization layer during the training and try to estimate these in the running manner, so that everything is simpler.**"]},{"cell_type":"markdown","id":"144b503b","metadata":{"papermill":{"duration":0.006822,"end_time":"2024-07-31T00:29:46.241384","exception":false,"start_time":"2024-07-31T00:29:46.234562","status":"completed"},"tags":[]},"source":["# Acknowledgements\n","\n","* https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2443s\n","* https://arxiv.org/pdf/1502.03167"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":187064505,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":274.293108,"end_time":"2024-07-31T00:29:48.926592","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-31T00:25:14.633484","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}