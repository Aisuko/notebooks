{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9441706,"sourceType":"datasetVersion","datasetId":5737742}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mimic interp-net data preprocessing demo\n\nThe purpose of this note is to demo a preprocessing interp-net data. \n\n","metadata":{}},{"cell_type":"markdown","source":"## Load data from dataset\n\nAs a demo, only use some of the values in vitals_records, not all","metadata":{}},{"cell_type":"code","source":"import pickle\nimport copy\nimport numpy as np\n\n\nwith open('/kaggle/input/mimic-interp-net-data-demo/vitals_records_10000.p', 'rb') as file:\n    vitals = pickle.load(file)\nprint(len(vitals))\nwith open('/kaggle/input/mimic-interp-net-data-demo/adm_type_los_mortality.p', 'rb') as file:\n    adm_info = pickle.load(file)\nprint(len(adm_info))\n#print(adm_info[:1])","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:00.375027Z","iopub.execute_input":"2024-09-20T17:19:00.375588Z","iopub.status.idle":"2024-09-20T17:19:04.876980Z","shell.execute_reply.started":"2024-09-20T17:19:00.375539Z","shell.execute_reply":"2024-09-20T17:19:04.875738Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"5000\n58976\n","output_type":"stream"}]},{"cell_type":"code","source":"# This step is about to filter vitals result by choose adm_info's 4th value larger than 48.  Only keep the corresponding vitals values\n# Here is the original codes, correct me if I messed the logic. adm_id_needed is confusing\n\n# adm_id = [record[0] for record in adm_info]\n# adm_id_needed = [record[0] for record in adm_info if record[2] >= 48]\n# vitals_dict = {}\n# for i in range(len(adm_id)):\n#     vitals_dict[adm_id[i]] = vitals[i]\n# vitals = [vitals_dict[x] for x in adm_id_needed]\n# label = [rec[3] for x in adm_id_needed for rec in adm_info if x == rec[0]]\n\nbatch_size = 10000\nbatch_idx = 1\n\nstart_point= batch_size*(batch_idx-1)\n\nprint(start_point)\n\nlabel = []\nfor record in adm_info:\n    if record[2] >= 48:\n        label.append(record[3])\n\nprint(len(label))\n\nvitals_new = []\nfor i in range(len(vitals)):\n    if adm_info[start_point+i][2] >= 48:\n        vitals_new.append(vitals[i])\nprint(len(vitals_new))\n\nvitals = vitals_new","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:04.879457Z","iopub.execute_input":"2024-09-20T17:19:04.879917Z","iopub.status.idle":"2024-09-20T17:19:05.592423Z","shell.execute_reply.started":"2024-09-20T17:19:04.879871Z","shell.execute_reply":"2024-09-20T17:19:05.591000Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"0\n53211\n4531\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(vitals))\nprint(len(label))","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:05.594667Z","iopub.execute_input":"2024-09-20T17:19:05.595126Z","iopub.status.idle":"2024-09-20T17:19:05.602548Z","shell.execute_reply.started":"2024-09-20T17:19:05.595079Z","shell.execute_reply":"2024-09-20T17:19:05.601065Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"4531\n53211\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Trim lossing","metadata":{}},{"cell_type":"code","source":"# Original code: https://github.com/mlds-lab/interp-net/blob/master/src/mimic_preprocessing.py#L25\n\nimport numpy as np\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef trim_los_parallel(data_chunk, length_of_stay):\n    num_features = 12  # final features (excluding EtCO2)\n    max_length = 2881  # maximum length of time stamp\n    a = np.full((len(data_chunk), num_features, max_length), -100, dtype=float)  # initialize array with -100 (missing data)\n    timestamps = []\n\n    for i in range(len(data_chunk)):\n        # Process temperature conversion in a vectorized way\n        if data_chunk[i][7]:\n            temp_array = np.array([elem[1] for elem in data_chunk[i][7] if elem[1] is not None])\n            data_chunk[i][6] += [(elem[0], temp * 1.8 + 32) for elem, temp in zip(data_chunk[i][7], temp_array)]\n\n        # Combine data[9] with data[10] and data[11]\n        data_chunk[i][9].extend(data_chunk[i][10] + data_chunk[i][11])\n\n        # Remove unwanted elements (EtCO2 data)\n        del data_chunk[i][5:7]\n        del data_chunk[i][8]\n\n        # Collect unique timestamps across all features\n        all_timestamps = sorted(set([elem[0] for j in range(num_features) for elem in data_chunk[i][j]]))\n\n        # Extract first 48-hour data\n        first_ts = all_timestamps[0] if all_timestamps else None\n        TS = [ts for ts in all_timestamps if (ts - first_ts).total_seconds() / 3600 <= length_of_stay]\n\n        timestamps.append(TS)\n\n        for j in range(num_features):\n            feature_data = data_chunk[i][j]\n            feature_dict = {entry[0]: entry[1] for entry in feature_data}  # Convert list to dictionary for fast lookup\n\n            for k, ts in enumerate(TS):\n                if ts in feature_dict:\n                    value = feature_dict[ts]\n                    if value is None or value in ('Other/Remarks', 'Comment'):\n                        a[i, j, k] = -100\n                    elif value in ('Normal <3 secs', 'Normal <3 Seconds', 'Brisk'):\n                        a[i, j, k] = 1\n                    elif value in ('Abnormal >3 secs', 'Abnormal >3 Seconds', 'Delayed'):\n                        a[i, j, k] = 2\n                    else:\n                        a[i, j, k] = value\n                else:\n                    a[i, j, k] = -100  # missing data\n\n    return a, timestamps\n\n# Function to split the data and run in parallel\ndef run_trim_los_in_parallel(data, length_of_stay, num_workers=4):\n    # Split data into chunks for parallel processing\n    chunk_size = len(data) // num_workers\n    data_chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n\n    results = []\n    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(trim_los_parallel, chunk, length_of_stay) for chunk in data_chunks]\n        for future in futures:\n            results.append(future.result())\n\n    # Combine results from all chunks\n    all_a = np.concatenate([result[0] for result in results], axis=0)\n    all_timestamps = sum([result[1] for result in results], [])\n    \n    return all_a, all_timestamps\n\n# Example usage\nhours_look_ahead = 48\nnum_workers = 15  # Use the available 15 cores for parallel processing\nvitals, timestamps = run_trim_los_in_parallel(vitals, hours_look_ahead, num_workers)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:05.606955Z","iopub.execute_input":"2024-09-20T17:19:05.607493Z","iopub.status.idle":"2024-09-20T17:19:28.038490Z","shell.execute_reply.started":"2024-09-20T17:19:05.607417Z","shell.execute_reply":"2024-09-20T17:19:28.036659Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"print(len(vitals))\nprint(len(timestamps))","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:28.040576Z","iopub.execute_input":"2024-09-20T17:19:28.041081Z","iopub.status.idle":"2024-09-20T17:19:28.048714Z","shell.execute_reply.started":"2024-09-20T17:19:28.041032Z","shell.execute_reply":"2024-09-20T17:19:28.047316Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"4531\n4531\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fixing input format\n\nReturn the input in the proper format\n\n* x: observed values\n* M: masking, 0 indicates missing values\n* delta: time points of observation","metadata":{}},{"cell_type":"code","source":"def fix_input_format(x, T):\n    \"\"\"Return the input in the proper format\n    x: observed values\n    M: masking, 0 indicates missing values\n    delta: time points of observation\n    \"\"\"\n    timestamp = 200\n    num_features = 12\n\n    # trim time stamps higher than 200\n    for i in range(len(T)):\n        if len(T[i]) > timestamp:\n            T[i] = T[i][:timestamp]\n\n    x = x[:, :, :timestamp]\n    M = np.zeros_like(x)\n    delta = np.zeros_like(x)\n    print(x.shape, len(T))\n\n    for t in T:\n        for i in range(1, len(t)):\n            t[i] = (t[i] - t[0]).total_seconds()/3600.0\n        if len(t) != 0:\n            t[0] = 0\n\n    # count outliers and negative values as missing values\n    # M = 0 indicates missing value\n    # M = 1 indicates observed value\n    # now since we have mask variable, we don't need -100\n    M[x > 500] = 0\n    x[x > 500] = 0.0\n    M[x < 0] = 0\n    x[x < 0] = 0.0\n    M[x > 0] = 1\n\n    for i in range(num_features):\n        for j in range(x.shape[0]):\n            for k in range(len(T[j])):\n                delta[j, i, k] = T[j][k]\n\n    return x, M, delta\n\n\nx, M, delta = fix_input_format(vitals, timestamps)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:28.050785Z","iopub.execute_input":"2024-09-20T17:19:28.051274Z","iopub.status.idle":"2024-09-20T17:19:29.861587Z","shell.execute_reply.started":"2024-09-20T17:19:28.051212Z","shell.execute_reply":"2024-09-20T17:19:29.859724Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"(4531, 12, 200) 4531\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Mean inputation","metadata":{}},{"cell_type":"code","source":"def mean_imputation(vitals, mask):\n    \"\"\"For the time series missing entirely, our interpolation network \n    assigns the starting point (time t=0) value of the time series to \n    the global mean before applying the two-layer interpolation network.\n    In such cases, the first interpolation layer just outputs the global\n    mean for that channel, but the second interpolation layer performs \n    a more meaningful interpolation using the learned correlations from\n    other channels.\"\"\"\n    counts = np.sum(np.sum(mask, axis=2), axis=0)\n    mean_values = np.sum(np.sum(vitals*mask, axis=2), axis=0)/counts\n    for i in range(mask.shape[0]):\n        for j in range(mask.shape[1]):\n            if np.sum(mask[i, j]) == 0:\n                mask[i, j, 0] = 1\n                vitals[i, j, 0] = mean_values[j]\n    return\n\n\nmean_imputation(x, M)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:29.863687Z","iopub.execute_input":"2024-09-20T17:19:29.864195Z","iopub.status.idle":"2024-09-20T17:19:30.348048Z","shell.execute_reply.started":"2024-09-20T17:19:29.864139Z","shell.execute_reply":"2024-09-20T17:19:30.346720Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"def hold_out(mask, perc=0.2):\n    \"\"\"To implement the autoencoder component of the loss, we introduce a set\n    of masking variables mr (and mr1) for each data point. If drop_mask = 0,\n    then we removecthe data point as an input to the interpolation network,\n    and includecthe predicted value at this time point when assessing\n    the autoencoder loss. In practice, we randomly select 20% of the\n    observed data points to hold out from\n    every input time series.\"\"\"\n    drop_mask = np.ones_like(mask)\n    drop_mask *= mask\n    for i in range(mask.shape[0]):\n        for j in range(mask.shape[1]):\n            count = np.sum(mask[i, j], dtype='int')\n            if int(0.20*count) > 1:\n                index = 0\n                r = np.ones((count, 1))\n                b = np.random.choice(count, int(0.20*count), replace=False)\n                r[b] = 0\n                for k in range(mask.shape[2]):\n                    if mask[i, j, k] > 0:\n                        drop_mask[i, j, k] = r[index,0]\n                        index += 1\n    return drop_mask\n\ndrop_mask=hold_out(M)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:30.349984Z","iopub.execute_input":"2024-09-20T17:19:30.350570Z","iopub.status.idle":"2024-09-20T17:19:35.994279Z","shell.execute_reply.started":"2024-09-20T17:19:30.350515Z","shell.execute_reply":"2024-09-20T17:19:35.988892Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"x = np.concatenate((x, M, delta, drop_mask), axis=1)\nprint(x.shape)\ny= np.array(label)\nprint(y.shape)\n\nnp.savez('preprocessed_data.npz', array1=x, array2=y)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T17:19:36.001179Z","iopub.execute_input":"2024-09-20T17:19:36.001919Z","iopub.status.idle":"2024-09-20T17:19:38.954055Z","shell.execute_reply.started":"2024-09-20T17:19:36.001862Z","shell.execute_reply":"2024-09-20T17:19:38.952225Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"(4531, 48, 200)\n(53211,)\n","output_type":"stream"}]}]}