{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/fine-tuning-t5-small-with-lora?scriptVersionId=165006015\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\n**Note: All the images are from the Reference lists section or the internet**\n\nWe are going to fine-tuning a t5-small quantized model for English to French translation-related tasks using the LoRA method. We use the peft and transformers libraries for training.\n\n\n# About quantization\n\nPlease check the list below:\n\n* [Quantization Technologies](https://www.kaggle.com/code/aisuko/quantization-technologies)\n* [Zero Degradation matrix multiplication](https://www.kaggle.com/code/aisuko/zero-degradation-matrix-multiplication)\n* [Lighter models on GPU for inference](https://www.kaggle.com/code/aisuko/lighter-models-on-gpu-for-inference)\n\n\n# About LoRA(Low Rank Adaptation)\n\n> A technique that accelerates the fine-tuning of large models while consuming less memory.\n\n\n**The idea is to freeze the original pre-trained weights(Matrices) and introduce new updata matrices**. These new matrics are trained on new data while keeping the overall number of changes low. The original weights matrix doesn't receive any adjustments. And finally, both the original and the adapted weights are combined.\n\n![](https://files.mastodon.social/media_attachments/files/111/702/004/494/881/797/original/a26697e010f0096b.webp)\n\nLoRA makes fine-tuning more efficient by drastically reducing the number of **trainable parameters**. In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, **in Transformer models LoRA is typically applied to attention blocks only**. **The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined manily by the rank `r` and the shape of the original weight matrix**.\n\n\nThe differences between QLoRA and LoRA in real word case see notebook [fine-tuning llama2 with QLoRA](https://www.kaggle.com/code/aisuko/fine-tuning-llama2-with-qlora?scriptVersionId=158763163&cellId=1).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.36.2\n!pip install bitsandbytes==0.41.3\n!pip install accelerate==0.25.0\n!pip install datasets==2.15.0\n!pip install evaluate==0.4.1\n!pip install peft==0.7.1","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:46:49.791937Z","iopub.execute_input":"2024-03-01T12:46:49.792347Z","iopub.status.idle":"2024-03-01T12:48:20.622973Z","shell.execute_reply.started":"2024-03-01T12:46:49.792311Z","shell.execute_reply":"2024-03-01T12:48:20.62176Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning t5-small-on-opus100\"\nos.environ[\"WANDB_NAME\"] = \"ft-t5-small-on-opus100\"","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:48:20.624871Z","iopub.execute_input":"2024-03-01T12:48:20.625148Z","iopub.status.idle":"2024-03-01T12:48:21.504585Z","shell.execute_reply.started":"2024-03-01T12:48:20.625121Z","shell.execute_reply":"2024-03-01T12:48:21.503613Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Dataset\n\nWe are going to use the optus 100 dataset for training which gives us access to more than 100 different languages.","metadata":{}},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nconfigs=get_dataset_config_names(\"opus100\")\nprint(configs)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:48:21.507196Z","iopub.execute_input":"2024-03-01T12:48:21.507524Z","iopub.status.idle":"2024-03-01T12:48:51.828988Z","shell.execute_reply.started":"2024-03-01T12:48:21.507483Z","shell.execute_reply":"2024-03-01T12:48:51.828003Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/65.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc06d46a66c34493a85fbed43bb15dd0"}},"metadata":{}},{"name":"stdout","text":"['af-en', 'am-en', 'an-en', 'ar-de', 'ar-en', 'ar-fr', 'ar-nl', 'ar-ru', 'ar-zh', 'as-en', 'az-en', 'be-en', 'bg-en', 'bn-en', 'br-en', 'bs-en', 'ca-en', 'cs-en', 'cy-en', 'da-en', 'de-en', 'de-fr', 'de-nl', 'de-ru', 'de-zh', 'dz-en', 'el-en', 'en-eo', 'en-es', 'en-et', 'en-eu', 'en-fa', 'en-fi', 'en-fr', 'en-fy', 'en-ga', 'en-gd', 'en-gl', 'en-gu', 'en-ha', 'en-he', 'en-hi', 'en-hr', 'en-hu', 'en-hy', 'en-id', 'en-ig', 'en-is', 'en-it', 'en-ja', 'en-ka', 'en-kk', 'en-km', 'en-kn', 'en-ko', 'en-ku', 'en-ky', 'en-li', 'en-lt', 'en-lv', 'en-mg', 'en-mk', 'en-ml', 'en-mn', 'en-mr', 'en-ms', 'en-mt', 'en-my', 'en-nb', 'en-ne', 'en-nl', 'en-nn', 'en-no', 'en-oc', 'en-or', 'en-pa', 'en-pl', 'en-ps', 'en-pt', 'en-ro', 'en-ru', 'en-rw', 'en-se', 'en-sh', 'en-si', 'en-sk', 'en-sl', 'en-sq', 'en-sr', 'en-sv', 'en-ta', 'en-te', 'en-tg', 'en-th', 'en-tk', 'en-tr', 'en-tt', 'en-ug', 'en-uk', 'en-ur', 'en-uz', 'en-vi', 'en-wa', 'en-xh', 'en-yi', 'en-yo', 'en-zh', 'en-zu', 'fr-nl', 'fr-ru', 'fr-zh', 'nl-ru', 'nl-zh', 'ru-zh']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We will use the \"en-fr\" data for language translation. Let's download and load the dataset though the `load_dataset`.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset=load_dataset(\"opus100\", \"en-fr\", split=\"train[:1000]\")\ndataset=dataset.train_test_split(test_size=0.2)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:48:51.831815Z","iopub.execute_input":"2024-03-01T12:48:51.83241Z","iopub.status.idle":"2024-03-01T12:49:06.185311Z","shell.execute_reply.started":"2024-03-01T12:48:51.832372Z","shell.execute_reply":"2024-03-01T12:49:06.184528Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690da3fd737e4a70908b8cf62a648e81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/327k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f606aeb9c19443808f7e5b87e63dec82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fca104a33584bf3934a2b60f069a221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/334k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ee6df0d20543c3b87b9e284de597ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab117b96a79640379dbaac3c9a9af183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5bd807c0dd42258cdb0ffb62dd28fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b694f33e3d540f9b6a6e3821673f0ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08b678330514870a3b3fd28789b606c"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 200\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']['translation'][:5]","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:06.186389Z","iopub.execute_input":"2024-03-01T12:49:06.186688Z","iopub.status.idle":"2024-03-01T12:49:06.205637Z","shell.execute_reply.started":"2024-03-01T12:49:06.186661Z","shell.execute_reply":"2024-03-01T12:49:06.204818Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[{'en': \"She's getting away!\", 'fr': \"Elle s'échappe!\"},\n {'en': 'Stop the car!', 'fr': 'Arrêtez la voiture !'},\n {'en': ']', 'fr': ']'},\n {'en': '(25)', 'fr': '(52)'},\n {'en': 'The tariff was bad mercantilism: it robbed more in profits and unionized jobs from steel-consuming industries than it gave to steel-making industries, and the former were at least as well-organized and vocal in Washington as the latter.',\n  'fr': \"Et une mauvaise décision commerciale : elles ont dépouillé les industries grandes consommatrices d'acier de bénéfices et d'emplois syndiqués qu'elles n'en ont préservé dans l'industrie de l'acier alors que ces deux secteurs industriels et leurs lobbies étaient aussi bien organisés et visibles à Washington l'un que l'autre.\"}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name=\"t5-small\"\nprompt=\"My name is Kaggle, nice to see you.\"\n\ntokenizer=AutoTokenizer.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:06.207143Z","iopub.execute_input":"2024-03-01T12:49:06.207577Z","iopub.status.idle":"2024-03-01T12:49:11.792902Z","shell.execute_reply.started":"2024-03-01T12:49:06.20754Z","shell.execute_reply":"2024-03-01T12:49:11.791786Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6b7992fc2644e5c9dfb71fee4820d42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdf2d03037a4b0e85fc008d60c1d881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26baddf18218470f9726b64b5cfbef81"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer(prompt, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:11.794178Z","iopub.execute_input":"2024-03-01T12:49:11.794645Z","iopub.status.idle":"2024-03-01T12:49:11.83595Z","shell.execute_reply.started":"2024-03-01T12:49:11.794618Z","shell.execute_reply":"2024-03-01T12:49:11.834997Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 499,  564,   19, 2209,  122, 3537,    6, 1245,   12,  217,   25,    5,\n            1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_func(data):\n    inputs=[ex['en'] for ex in data['translation']]\n    targets=[ex['fr'] for ex in data['translation']]\n    \n    # tokenize each row of inputs and outputs\n    model_inputs=tokenizer(inputs, truncation=True)\n    labels=tokenizer(targets, truncation=True)\n    \n    model_inputs[\"labels\"]=labels[\"input_ids\"]\n    return model_inputs\n\n# We tokenize the entire dataset\ntokenized_dataset=dataset.map(preprocess_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:11.837094Z","iopub.execute_input":"2024-03-01T12:49:11.837395Z","iopub.status.idle":"2024-03-01T12:49:12.017287Z","shell.execute_reply.started":"2024-03-01T12:49:11.837368Z","shell.execute_reply":"2024-03-01T12:49:12.01634Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23af67299a234404af3000a388b3dd2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c587530fdd50402f81ade1a7652c1366"}},"metadata":{}}]},{"cell_type":"code","source":"# # use a sample of around 2000 instead of the complete dataset as training dataset\n# train_dataset=tokenized_dataset['train'].shuffle(seed=42).select(range(2000))\n\n# # as evaluation dataset\n# eval_dataset=tokenized_dataset['validation']","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:12.01832Z","iopub.execute_input":"2024-03-01T12:49:12.018629Z","iopub.status.idle":"2024-03-01T12:49:12.022771Z","shell.execute_reply.started":"2024-03-01T12:49:12.018602Z","shell.execute_reply":"2024-03-01T12:49:12.021912Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the model\n\nFirst, we will load the model with 8 bit(quantization the model). And then, we are using the LoRA. Here are the description of the LoraConfig:\n\n* **r**: the rank of the update matrices, expressed in **int**. Lower rank results in smaller update matrices with fewer trainable parameters.\n* **target_modules**: **The modules(for example, attention blocks)** to apply the LoRA update matrices.\n* **alpha**: LoRA scaling factor\n* **bias**: Specifies if the bias parameters should be trained. Can be 'none','all' or 'lora_only'.\n* **module_to_save**: List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. These typically include model's custome head that is randomly initialized for the fine-tuning task.\n* **layer_to_transform**: List of layers to be transformed by LoRA. If not specified, all layers in `target_modules` are transformed.\n* **layers_pattern**: Pattern to match layer names in `target_modules`, if `layer_to_transform` is specified. By default `PeftModel` will look at common layer pattern(`layers`,`h`, `blocks`, etc.), use it for exotic and custom models.\n* **rank_pattern**: The mapping from layer names or regexp expression to ranks which are different from the default tank specified by `r`.\n* **alpha_pattern**: The mapping from layer names or regexp expression to alphas which are different from the default alpha specified by `lora_alpha`.","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel, prepare_model_for_kbit_training, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom transformers import BitsAndBytesConfig\nfrom transformers import AutoModelForSeq2SeqLM\n\nbnb_config=BitsAndBytesConfig(\n    load_in_8bit=True\n)\n\nmodel=AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:12.026212Z","iopub.execute_input":"2024-03-01T12:49:12.026583Z","iopub.status.idle":"2024-03-01T12:49:15.048108Z","shell.execute_reply.started":"2024-03-01T12:49:12.026546Z","shell.execute_reply":"2024-03-01T12:49:15.046971Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d85da7e32f84a958b8b2470f6fb143b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef388d282cf4a4b85f79692085d05e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d4ba3230e754e699371d77e6e75891d"}},"metadata":{}}]},{"cell_type":"code","source":"# Freeze the original parameters\nmodel=prepare_model_for_kbit_training(model)\n\npeft_config=LoraConfig(\n    # the task to train for (sequence-to-sequence language modeling in this case)\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    # the dimension of the low-rank matrices\n    r=4,\n    # the scaling factor for the low-rank matrices\n    lora_alpha=32,\n    # the dropout probability of the LoRA layers\n    lora_dropout=0.01,\n    target_modules=[\"k\",\"q\",\"v\"],\n)\n\npeft_model=get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:15.049617Z","iopub.execute_input":"2024-03-01T12:49:15.050032Z","iopub.status.idle":"2024-03-01T12:49:15.149468Z","shell.execute_reply.started":"2024-03-01T12:49:15.049991Z","shell.execute_reply":"2024-03-01T12:49:15.148498Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"trainable params: 221,184 || all params: 60,727,808 || trainable%: 0.36422193931320557\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\naccuracy=evaluate.load(\"accuracy\")\n\ndef compute_metrics(p):\n    predictions, labels=p\n    predictions=np.argmax(predictions, axis=1)\n    \n    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:15.150483Z","iopub.execute_input":"2024-03-01T12:49:15.150798Z","iopub.status.idle":"2024-03-01T12:49:26.874679Z","shell.execute_reply.started":"2024-03-01T12:49:15.150771Z","shell.execute_reply":"2024-03-01T12:49:26.873857Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60c5ab8485b43ce9f14744f12f78b50"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Ttraining","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# ignore tokenizer pad token in the loss\nlabel_pad_token_id=-100\n\n# padding the sentence of the entire datasets\ndata_collator=DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=peft_model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:49:26.875795Z","iopub.execute_input":"2024-03-01T12:49:26.876336Z","iopub.status.idle":"2024-03-01T12:49:26.881553Z","shell.execute_reply.started":"2024-03-01T12:49:26.876307Z","shell.execute_reply":"2024-03-01T12:49:26.88052Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer\nimport torch\n\ntraining_args=Seq2SeqTrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    auto_find_batch_size=True,\n    learning_rate=1e-3,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir=os.getenv(\"WANDB_NAME\")+\"/logs\",\n    logging_strategy=\"epoch\",\n    logging_steps=500,\n    save_strategy=\"epoch\",\n    push_to_hub=False,\n    report_to=\"wandb\",\n    run_name=os.getenv(\"WANDB_NAME\"),\n)\n\n\n# Create Trainer instance\ntrainer=Seq2SeqTrainer(\n    model=peft_model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)\n\npeft_model.config.use_cache=False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:50:56.929695Z","iopub.execute_input":"2024-03-01T12:50:56.930121Z","iopub.status.idle":"2024-03-01T12:50:57.936918Z","shell.execute_reply.started":"2024-03-01T12:50:56.930078Z","shell.execute_reply":"2024-03-01T12:50:57.93527Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m trainer\u001b[38;5;241m=\u001b[39mSeq2SeqTrainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mpeft_model,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m peft_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/memory.py:136\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1821\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1818\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1820\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1821\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1822\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:448\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2799\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2799\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2800\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2795\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2779\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2777\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2778\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2779\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2780\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2781\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2782\u001b[0m )\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:588\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    586\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m _query_table(table, key)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43m_query_table_with_indices_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa_subtable\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:75\u001b[0m, in \u001b[0;36m_query_table_with_indices_mapping\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _query_table(table, indices\u001b[38;5;241m.\u001b[39mcolumn(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto_pylist())\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Iterable):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_query_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m _raise_bad_key_type(key)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:100\u001b[0m, in \u001b[0;36m_query_table\u001b[0;34m(table, key)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mslice(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# don't use pyarrow.Table.take even for pyarrow >=1.0 (see https://issues.apache.org/jira/browse/ARROW-9773)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m _raise_bad_key_type(key)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/table.py:134\u001b[0m, in \u001b[0;36mIndexedTableMixin.fast_gather\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndices must be non-empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msearchsorted(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets, indices, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(\n\u001b[0;32m--> 134\u001b[0m     [\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batches[batch_idx]\u001b[38;5;241m.\u001b[39mslice(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets[batch_idx], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_indices, indices)\n\u001b[1;32m    137\u001b[0m     ],\n\u001b[1;32m    138\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema,\n\u001b[1;32m    139\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/table.py:135\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndices must be non-empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msearchsorted(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets, indices, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(\n\u001b[1;32m    134\u001b[0m     [\n\u001b[0;32m--> 135\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mslice(i \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets[batch_idx], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_indices, indices)\n\u001b[1;32m    137\u001b[0m     ],\n\u001b[1;32m    138\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema,\n\u001b[1;32m    139\u001b[0m )\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"code","source":"trainer.evaluate() # out of GPU memory","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:50:03.308971Z","iopub.status.idle":"2024-03-01T12:50:03.30938Z","shell.execute_reply.started":"2024-03-01T12:50:03.30919Z","shell.execute_reply":"2024-03-01T12:50:03.309209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs={\n    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n    'finetuned_from': model_name,\n    'tasks': 'Translation',\n#     'dataset_tags':'',\n    'dataset':'opus100'\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:50:03.311081Z","iopub.status.idle":"2024-03-01T12:50:03.311433Z","shell.execute_reply.started":"2024-03-01T12:50:03.311264Z","shell.execute_reply":"2024-03-01T12:50:03.311281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"peft_model.config.use_cache=True\ncontext=tokenizer([\"Floods in Australia 2024\"], return_tensors=\"pt\")\noutput=peft_model.generate(**context)\n\ntokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:50:03.312642Z","iopub.status.idle":"2024-03-01T12:50:03.312992Z","shell.execute_reply.started":"2024-03-01T12:50:03.312817Z","shell.execute_reply":"2024-03-01T12:50:03.312834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\nmodel = PeftModel.from_pretrained(model, \"aisuko/ft-t5-small-on-opus100\")\n\noutput=model.generate(**context)\ntokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T12:50:03.314863Z","iopub.status.idle":"2024-03-01T12:50:03.315223Z","shell.execute_reply.started":"2024-03-01T12:50:03.315053Z","shell.execute_reply":"2024-03-01T12:50:03.31507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References List\n\n* https://towardsdev.com/fine-tune-quantized-language-model-using-lora-with-peft-transformers-on-t4-gpu-287da2d5d7f1\n* https://huggingface.co/docs/peft/quicktour","metadata":{}}]}