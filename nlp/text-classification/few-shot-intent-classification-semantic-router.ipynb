{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThe Semantic Router algorithm converts all the training texts to embeddings using any off-the-shelf embedding models like Sentence Transformer. During inference, it converts the input texts to embeddings and finds the K Nearest Neighbots from the training data embeddings to classify the input text.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -q -U transformers==4.39.3\n!pip install -q -U datasets==2.18.0\n!pip install -q -U semantic-router==0.0.44","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:38:00.540357Z","iopub.execute_input":"2024-05-25T05:38:00.541205Z","iopub.status.idle":"2024-05-25T05:38:39.123167Z","shell.execute_reply.started":"2024-05-25T05:38:00.541171Z","shell.execute_reply":"2024-05-25T05:38:39.121945Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncohere 5.5.3 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntransformers 4.39.3 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"MODEL\"]=\"sentence-transformers/paraphrase-mpnet-base-v2\"\nos.environ[\"DATASET\"]=\"SetFit/amazon_massive_intent_en-US\"\nos.environ[\"FITMODEL\"]=\"semantic-router-mpnet-v2-amazon-mi\"","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:35:35.529240Z","iopub.execute_input":"2024-05-25T05:35:35.529989Z","iopub.status.idle":"2024-05-25T05:35:36.221088Z","shell.execute_reply.started":"2024-05-25T05:35:35.529947Z","shell.execute_reply":"2024-05-25T05:35:36.220199Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, load_dataset\n\nds=load_dataset(os.getenv(\"DATASET\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:35:36.222079Z","iopub.execute_input":"2024-05-25T05:35:36.222360Z","iopub.status.idle":"2024-05-25T05:35:39.161101Z","shell.execute_reply.started":"2024-05-25T05:35:36.222315Z","shell.execute_reply":"2024-05-25T05:35:39.160080Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 1.14M/1.14M [00:00<00:00, 9.35MB/s]\nDownloading data: 100%|██████████| 201k/201k [00:00<00:00, 3.06MB/s]\nDownloading data: 100%|██████████| 294k/294k [00:00<00:00, 3.54MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf34cc4e9a104c86ba0e3cb7d5c4d915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690c41192f484ed18617a616f1021590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f501877a15624e40a86e390b6ee89be5"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Pre-processing dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.DataFrame(ds[\"test\"])\n\n# Helper function to select random rows\ndef select_random_rows(group):\n    return group.sample(n=10, random_state=42)\n\n\n# find top classes with minimum 30 rows\nlabel_counts=df[\"label_text\"].value_counts()\nlabel_counts=label_counts[label_counts>30]\n\n# restruct df to top classes only\ndf=df[df[\"label_text\"].isin(label_counts.index)].reset_index(drop=True)\n\nassert set(df[\"label_text\"].value_counts().index.to_list())==set(label_counts.index.to_list()), \"Some labels were lost\"\n\n# select random row per unique value in label_text column\ntrain_df=df.groupby(\"label_text\", group_keys=False).apply(select_random_rows)\n\n# create eval dataframe by dropping the train data and selecting random rows\neval_df=(df.drop(train_df.index).groupby(\"label_text\",group_keys=False).apply(select_random_rows))\n\n# create test dataframe by dropping both train and eval data\ntest_df=df.drop(train_df.index.to_list()+eval_df.index.to_list())\n\n# reset the index\ncols_to_keep=[\"text\", \"label_text\"]\ntrain_df=train_df[cols_to_keep].reset_index(drop=True)\neval_df=eval_df[cols_to_keep].reset_index(drop=True)\ntest_df=test_df[cols_to_keep].reset_index(drop=True)\n\n# save the file\ntest_df.to_pickle(\"test_df.pkl\")\ntrain_df.to_pickle(\"train_df.pkl\")\neval_df.to_pickle(\"eval_df.pkl\")\n\ntrain_ds=Dataset.from_pandas(train_df)\neval_ds=Dataset.from_pandas(eval_df)\ntest_ds=Dataset.from_pandas(test_df)\n\nprint(train_df.shape, eval_df.shape, test_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:35:39.162472Z","iopub.execute_input":"2024-05-25T05:35:39.162935Z","iopub.status.idle":"2024-05-25T05:35:39.469163Z","shell.execute_reply.started":"2024-05-25T05:35:39.162907Z","shell.execute_reply":"2024-05-25T05:35:39.468279Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(350, 2) (350, 2) (1879, 2)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2096072177.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  train_df=df.groupby(\"label_text\", group_keys=False).apply(select_random_rows)\n/tmp/ipykernel_34/2096072177.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  eval_df=(df.drop(train_df.index).groupby(\"label_text\",group_keys=False).apply(select_random_rows))\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text   label_text\n0       do i have any alarms set for six am tomorrow  alarm_query\n1  what is the wake up time for my alarm i have s...  alarm_query\n2                  please tell me what alarms are on  alarm_query\n3                          please list all my alarms  alarm_query\n4                     what times do my alarms go off  alarm_query","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>do i have any alarms set for six am tomorrow</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what is the wake up time for my alarm i have s...</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>please tell me what alarms are on</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>please list all my alarms</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>what times do my alarms go off</td>\n      <td>alarm_query</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from semantic_router import Route\nfrom semantic_router.encoders import HuggingFaceEncoder\nfrom semantic_router.layer import RouteLayer\n\nroutes=[]\n\nfor topic in train_df[\"label_text\"].unique():\n    name=topic\n    utterances = train_df[train_df[\"label_text\"] == topic][\"text\"].values.tolist()\n    route = Route(name=name, utterances=utterances)\n    routes.append(route)\n\n# Step 2: Setup the Embedding model\nencoder = HuggingFaceEncoder(name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Step 3: Create the route layer to embed all utterences and assign their class\nrl = RouteLayer(encoder=encoder, routes=routes)   ","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:38:39.125384Z","iopub.execute_input":"2024-05-25T05:38:39.125698Z","iopub.status.idle":"2024-05-25T05:38:40.403153Z","shell.execute_reply.started":"2024-05-25T05:38:39.125670Z","shell.execute_reply":"2024-05-25T05:38:40.401743Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/semantic_router/encoders/huggingface.py:51\u001b[0m, in \u001b[0;36mHuggingFaceEncoder._initialize_hf_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     logging,\n\u001b[1;32m     49\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/dependency_versions_check.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 111\u001b[0m     \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n","\u001b[0;31mImportError\u001b[0m: tokenizers>=0.14,<0.19 is required for a normal functioning of this module, but found tokenizers==0.19.1.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     routes\u001b[38;5;241m.\u001b[39mappend(route)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 2: Setup the Embedding model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 3: Create the route layer to embed all utterences and assign their class\u001b[39;00m\n\u001b[1;32m     17\u001b[0m rl \u001b[38;5;241m=\u001b[39m RouteLayer(encoder\u001b[38;5;241m=\u001b[39mencoder, routes\u001b[38;5;241m=\u001b[39mroutes)   \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/semantic_router/encoders/huggingface.py:47\u001b[0m, in \u001b[0;36mHuggingFaceEncoder.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_hf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/semantic_router/encoders/huggingface.py:53\u001b[0m, in \u001b[0;36mHuggingFaceEncoder._initialize_hf_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install transformers to use HuggingFaceEncoder. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can install it with: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install semantic-router[local]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n","\u001b[0;31mImportError\u001b[0m: Please install transformers to use HuggingFaceEncoder. You can install it with: `pip install semantic-router[local]`"],"ename":"ImportError","evalue":"Please install transformers to use HuggingFaceEncoder. You can install it with: `pip install semantic-router[local]`","output_type":"error"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom tdqm import tdqm\n\n# Step 1: Run predictions to calculate class level metrics\npredictions = []\nfor text in tqdm(test_df['text'].values):\n    pred = rl(text).name\n\n    if pred:\n        predictions.append(pred)\n    else:\n        predictions.append(\"\")\n\ntest_df[\"semantic_router_predictions\"] = predictions\nprint(\"\\nSemantic Router Class Level Metrics:\")\nprint(\n    classification_report(test_df[\"label_text\"], test_df[\"semantic_router_predictions\"])\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:35:39.479881Z","iopub.status.idle":"2024-05-25T05:35:39.480291Z","shell.execute_reply.started":"2024-05-25T05:35:39.480077Z","shell.execute_reply":"2024-05-25T05:35:39.480093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://medium.com/towards-artificial-intelligence/few-shot-nlp-intent-classification-d29bf85548aa\n* https://github.com/aurelio-labs/semantic-router/tree/main","metadata":{}}]}