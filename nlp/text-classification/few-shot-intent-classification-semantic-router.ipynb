{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThe Semantic Router algorithm converts all the training texts to embeddings using any off-the-shelf embedding models like Sentence Transformer. During inference, it converts the input texts to embeddings and finds the K Nearest Neighbots from the training data embeddings to classify the input text.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -q -U transformers==4.39.3\n!pip install -q -U datasets==2.18.0\n!pip install -q -U semantic-router==0.0.44","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"MODEL\"]=\"sentence-transformers/paraphrase-mpnet-base-v2\"\nos.environ[\"DATASET\"]=\"SetFit/amazon_massive_intent_en-US\"\nos.environ[\"FITMODEL\"]=\"semantic-router-mpnet-v2-amazon-mi\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, load_dataset\n\nds=load_dataset(os.getenv(\"DATASET\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.DataFrame(ds[\"test\"])\n\n# Helper function to select random rows\ndef select_random_rows(group):\n    return group.sample(n=10, random_state=42)\n\n\n# find top classes with minimum 30 rows\nlabel_counts=df[\"label_text\"].value_counts()\nlabel_counts=label_counts[label_counts>30]\n\n# restruct df to top classes only\ndf=df[df[\"label_text\"].isin(label_counts.index)].reset_index(drop=True)\n\nassert set(df[\"label_text\"].value_counts().index.to_list())==set(label_counts.index.to_list()), \"Some labels were lost\"\n\n# select random row per unique value in label_text column\ntrain_df=df.groupby(\"label_text\", group_keys=False).apply(select_random_rows)\n\n# create eval dataframe by dropping the train data and selecting random rows\neval_df=(df.drop(train_df.index).groupby(\"label_text\",group_keys=False).apply(select_random_rows))\n\n# create test dataframe by dropping both train and eval data\ntest_df=df.drop(train_df.index.to_list()+eval_df.index.to_list())\n\n# reset the index\ncols_to_keep=[\"text\", \"label_text\"]\ntrain_df=train_df[cols_to_keep].reset_index(drop=True)\neval_df=eval_df[cols_to_keep].reset_index(drop=True)\ntest_df=test_df[cols_to_keep].reset_index(drop=True)\n\n# save the file\ntest_df.to_pickle(\"test_df.pkl\")\ntrain_df.to_pickle(\"train_df.pkl\")\neval_df.to_pickle(\"eval_df.pkl\")\n\ntrain_ds=Dataset.from_pandas(train_df)\neval_ds=Dataset.from_pandas(eval_df)\ntest_ds=Dataset.from_pandas(test_df)\n\nprint(train_df.shape, eval_df.shape, test_df.shape)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from semantic_router import Route\nfrom smentic_router.encoders import HuggingFaceEncoder\nfrom smentic_router.layer import RouteLayer\n\nroutes=[]\n\nfor topic train_df[\"label_text\"].unique():\n    name=topic\n    utterances = train_df[train_df[\"label_text\"] == topic][\"text\"].values.tolist()\n    route = Route(name=name, utterances=utterances)\n    routes.append(route)\n\n# Step 2: Setup the Embedding model\nencoder = HuggingFaceEncoder(name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Step 3: Create the route layer to embed all utterences and assign their class\nrl = RouteLayer(encoder=encoder, routes=routes)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom tdqm import tdqm\n\n# Step 1: Run predictions to calculate class level metrics\npredictions = []\nfor text in tqdm(test_df['text'].values):\n    pred = rl(text).name\n\n    if pred:\n        predictions.append(pred)\n    else:\n        predictions.append(\"\")\n\ntest_df[\"semantic_router_predictions\"] = predictions\nprint(\"\\nSemantic Router Class Level Metrics:\")\nprint(\n    classification_report(test_df[\"label_text\"], test_df[\"semantic_router_predictions\"])\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://medium.com/towards-artificial-intelligence/few-shot-nlp-intent-classification-d29bf85548aa\n* https://github.com/aurelio-labs/semantic-router/tree/main","metadata":{}}]}