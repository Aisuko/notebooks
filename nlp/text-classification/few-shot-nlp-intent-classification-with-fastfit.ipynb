{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThe FastFit algorithm uses a pre-trained ST as the base model. It fine-tunes the base model by using in-batch contrastive loss to embed both the texts and their class names into a shared embeddding space such that texts and their respective class names have a low distance.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -q -U transformers==4.39.3\n!pip install -q -U datasets==2.18.0\n!pip install -q -U fast-fit==1.2.1","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:20:49.235601Z","iopub.execute_input":"2024-05-25T05:20:49.236052Z","iopub.status.idle":"2024-05-25T05:21:27.901564Z","shell.execute_reply.started":"2024-05-25T05:20:49.236014Z","shell.execute_reply":"2024-05-25T05:21:27.900147Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"MODEL\"]=\"sentence-transformers/paraphrase-mpnet-base-v2\"\nos.environ[\"DATASET\"]=\"SetFit/amazon_massive_intent_en-US\"\nos.environ[\"FITMODEL\"]=\"fastfit-mpnet-v2-amazon-mi\"","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:21:27.903787Z","iopub.execute_input":"2024-05-25T05:21:27.904106Z","iopub.status.idle":"2024-05-25T05:21:28.876770Z","shell.execute_reply.started":"2024-05-25T05:21:27.904077Z","shell.execute_reply":"2024-05-25T05:21:28.875622Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, load_dataset\n\nds=load_dataset(os.getenv(\"DATASET\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:21:28.878022Z","iopub.execute_input":"2024-05-25T05:21:28.878321Z","iopub.status.idle":"2024-05-25T05:21:34.411690Z","shell.execute_reply.started":"2024-05-25T05:21:28.878294Z","shell.execute_reply":"2024-05-25T05:21:34.410569Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.14M/1.14M [00:00<00:00, 2.56MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201k/201k [00:00<00:00, 748kB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 294k/294k [00:00<00:00, 2.75MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bab5a9675884d09aa70836496af5735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd74316a3f264846ad70ca1f7af58613"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a600a4d58d48405285e8051d0d021a0d"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.DataFrame(ds[\"test\"])\n\n# Helper function to select random rows\ndef select_random_rows(group):\n    return group.sample(n=10, random_state=42)\n\n\n# find top classes with minimum 30 rows\nlabel_counts=df[\"label_text\"].value_counts()\nlabel_counts=label_counts[label_counts>30]\n\n# restruct df to top classes only\ndf=df[df[\"label_text\"].isin(label_counts.index)].reset_index(drop=True)\n\nassert set(df[\"label_text\"].value_counts().index.to_list())==set(label_counts.index.to_list()), \"Some labels were lost\"\n\n# select random row per unique value in label_text column\ntrain_df=df.groupby(\"label_text\", group_keys=False).apply(select_random_rows)\n\n# create eval dataframe by dropping the train data and selecting random rows\neval_df=(df.drop(train_df.index).groupby(\"label_text\",group_keys=False).apply(select_random_rows))\n\n# create test dataframe by dropping both train and eval data\ntest_df=df.drop(train_df.index.to_list()+eval_df.index.to_list())\n\n# reset the index\ncols_to_keep=[\"text\", \"label_text\"]\ntrain_df=train_df[cols_to_keep].reset_index(drop=True)\neval_df=eval_df[cols_to_keep].reset_index(drop=True)\ntest_df=test_df[cols_to_keep].reset_index(drop=True)\n\n# save the file\ntest_df.to_pickle(\"test_df.pkl\")\ntrain_df.to_pickle(\"train_df.pkl\")\neval_df.to_pickle(\"eval_df.pkl\")\n\ntrain_ds=Dataset.from_pandas(train_df)\neval_ds=Dataset.from_pandas(eval_df)\ntest_ds=Dataset.from_pandas(test_df)\n\nprint(train_df.shape, eval_df.shape, test_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:21:34.414468Z","iopub.execute_input":"2024-05-25T05:21:34.415053Z","iopub.status.idle":"2024-05-25T05:21:34.742337Z","shell.execute_reply.started":"2024-05-25T05:21:34.415018Z","shell.execute_reply":"2024-05-25T05:21:34.741294Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(350, 2) (350, 2) (1879, 2)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2096072177.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  train_df=df.groupby(\"label_text\", group_keys=False).apply(select_random_rows)\n/tmp/ipykernel_34/2096072177.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  eval_df=(df.drop(train_df.index).groupby(\"label_text\",group_keys=False).apply(select_random_rows))\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text   label_text\n0       do i have any alarms set for six am tomorrow  alarm_query\n1  what is the wake up time for my alarm i have s...  alarm_query\n2                  please tell me what alarms are on  alarm_query\n3                          please list all my alarms  alarm_query\n4                     what times do my alarms go off  alarm_query","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>do i have any alarms set for six am tomorrow</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what is the wake up time for my alarm i have s...</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>please tell me what alarms are on</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>please list all my alarms</td>\n      <td>alarm_query</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>what times do my alarms go off</td>\n      <td>alarm_query</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from fastfit import FastFit, FastFitTrainer\n\n# Load the base ST model and setup the trainer\ntrainer = FastFitTrainer(\n    model_name_or_path=os.getenv(\"MODEL\"),\n    label_column_name=\"label_text\",\n    text_column_name=\"text\",\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    max_text_length=128,\n    dataloader_drop_last=False,\n    num_repeats=1,\n    optim=\"adafactor\",\n    clf_loss_factor=0.1,\n    fp16=True,\n    train_dataset=train_ds,\n    validation_dataset=eval_ds,\n    test_dataset=test_ds,\n)\n\nmodel=trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:21:34.743441Z","iopub.execute_input":"2024-05-25T05:21:34.743712Z","iopub.status.idle":"2024-05-25T05:22:31.528719Z","shell.execute_reply.started":"2024-05-25T05:21:34.743688Z","shell.execute_reply":"2024-05-25T05:22:31.527305Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-05-25 05:21:41.900701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-25 05:21:41.900797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-25 05:21:42.041211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bffa9eb3c144fc1bc2ec8f339196644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0cfc9228aa4d44a62a3f779666afb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa3af78b730471ea42d902b44f9ae10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb58e3ff07c24a48a92546cd50f194f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"033c8a19550a44629485f502dd85386a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2bd5b12c3448699314e36646e7a49f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/350 [00:00<?,â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d37688374389411dba5f0eda06c74f0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/350 [00:00<?,â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a5f894dce443cdb5d2417f963368b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/1879 [00:00<?â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ccbb77584fd481fa20ed6db626f0391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0643969306874a448538f2fdad7e0406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8164ca34c5064a6292037bc800a799cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/1879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1988699faa3c447cb8ebb9d19cdfa9e8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/fastfit/train.py:879: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(self.data_args.metric_name, experiment_id=uuid.uuid4())\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e6e1f819c7421386e25edbb366d03e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33murakiny\u001b[0m (\u001b[33mcausal_language_trainer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240525_052211-f74w8fne</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/causal_language_trainer/huggingface/runs/f74w8fne' target=\"_blank\">fancy-snowball-2</a></strong> to <a href='https://wandb.ai/causal_language_trainer/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/causal_language_trainer/huggingface' target=\"_blank\">https://wandb.ai/causal_language_trainer/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/causal_language_trainer/huggingface/runs/f74w8fne' target=\"_blank\">https://wandb.ai/causal_language_trainer/huggingface/runs/f74w8fne</a>"},"metadata":{}},{"name":"stderr","text":"[WARNING|modeling_utils.py:1111] 2024-05-25 05:22:29,540 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11/11 00:01, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =        0GF\n  train_loss               =     2.7326\n  train_runtime            = 0:00:21.80\n  train_samples            =        350\n  train_samples_per_second =     16.055\n  train_steps_per_second   =      0.505\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate metrics on test dataset\neval_metrics = trainer.evaluate()\nprint(\"Eval Accuracy: {:.2f}\".format(eval_metrics[\"eval_accuracy\"] * 100))\n\ntest_metrics = trainer.test()\nprint(\"Test Accuracy: {:.2f}\".format(test_metrics[\"eval_accuracy\"] * 100))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:22:31.530336Z","iopub.execute_input":"2024-05-25T05:22:31.531046Z","iopub.status.idle":"2024-05-25T05:22:36.085335Z","shell.execute_reply.started":"2024-05-25T05:22:31.531010Z","shell.execute_reply":"2024-05-25T05:22:36.084221Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =        1.0\n  eval_accuracy           =     0.7343\n  eval_loss               =     4.5141\n  eval_runtime            = 0:00:00.73\n  eval_samples            =        350\n  eval_samples_per_second =    475.297\n  eval_steps_per_second   =      8.148\nEval Accuracy: 73.43\n***** test metrics *****\n  epoch                   =        1.0\n  eval_accuracy           =     0.6525\n  eval_loss               =     5.2223\n  eval_runtime            = 0:00:03.79\n  eval_samples_per_second =    495.108\n  eval_steps_per_second   =      7.905\n  test_samples            =       1879\nTest Accuracy: 65.25\n","output_type":"stream"}]},{"cell_type":"code","source":"# trainer.push_to_hub(\"aisuko/\"+os.getenv(\"FITMODEL\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:22:36.086812Z","iopub.execute_input":"2024-05-25T05:22:36.087177Z","iopub.status.idle":"2024-05-25T05:22:36.999005Z","shell.execute_reply.started":"2024-05-25T05:22:36.087144Z","shell.execute_reply":"2024-05-25T05:22:36.997550Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maisuko/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFITMODEL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: FastFitTrainer.push_to_hub() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"FastFitTrainer.push_to_hub() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"model.save_pretrained(\"aisuko/\"+os.getenv(\"FITMODEL\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:22:53.415869Z","iopub.execute_input":"2024-05-25T05:22:53.416707Z","iopub.status.idle":"2024-05-25T05:22:54.273435Z","shell.execute_reply.started":"2024-05-25T05:22:53.416676Z","shell.execute_reply":"2024-05-25T05:22:54.272393Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, pipeline\n\n# Step 1: Load a pre-trained model from disk\nmodel = FastFit.from_pretrained('aisuko/'+os.getenv(\"FITMODEL\"))\ntokenizer = AutoTokenizer.from_pretrained(os.getenv(\"MODEL\"))\nclassifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=\"cuda\")\n\n# Step 2: Run predictions to calculate class level metrics\npredictions = []\nfor row in tqdm(test_ds):\n    predictions.append(classifier(row[\"text\"])[0][\"label\"])\n\ntest_df[\"fastfit_predictions\"] = predictions\nprint(\"FastFit Class Level Metrics:\")\nprint(classification_report(test_df[\"label_text\"], test_df[\"fastfit_predictions\"]))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T05:23:00.725496Z","iopub.execute_input":"2024-05-25T05:23:00.725856Z","iopub.status.idle":"2024-05-25T05:23:47.373172Z","shell.execute_reply.started":"2024-05-25T05:23:00.725829Z","shell.execute_reply":"2024-05-25T05:23:47.370151Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"[ERROR|base.py:1052] 2024-05-25 05:23:03,252 >> The model 'FastFit' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n  0%|          | 8/1879 [00:00<00:52, 35.90it/s]--- Logging error ---\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n    msg = self.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n    return fmt.format(record)\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n    record.message = record.getMessage()\n  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_34/3077441312.py\", line 13, in <module>\n    predictions.append(classifier(row[\"text\"])[0][\"label\"])\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 156, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n    logger.warning_once(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n    self.warning(*args, **kwargs)\nMessage: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\nArguments: (<class 'UserWarning'>,)\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1879/1879 [00:44<00:00, 42.70it/s]","output_type":"stream"},{"name":"stdout","text":"FastFit Class Level Metrics:\n                          precision    recall  f1-score   support\n\n             alarm_query       0.64      0.50      0.56        14\n               alarm_set       0.58      0.86      0.69        21\n       audio_volume_mute       0.85      0.92      0.88        12\n          calendar_query       0.45      0.57      0.50       106\n         calendar_remove       0.69      0.96      0.80        47\n            calendar_set       0.80      0.43      0.56       189\n          cooking_recipe       0.80      0.92      0.86        52\n          datetime_query       0.81      0.71      0.76        68\n             email_query       0.66      0.92      0.77        99\n         email_sendemail       0.71      0.66      0.69        94\n          general_quirky       0.32      0.10      0.15       149\n              iot_coffee       0.83      0.94      0.88        16\n     iot_hue_lightchange       0.80      0.25      0.38        16\n        iot_hue_lightoff       0.67      0.96      0.79        23\n       lists_createoradd       0.59      0.68      0.63        19\n             lists_query       0.42      0.68      0.52        31\n            lists_remove       0.75      0.75      0.75        32\n          music_likeness       0.12      0.19      0.15        16\n             music_query       0.16      0.60      0.26        15\n              news_query       0.73      0.80      0.76       104\n          play_audiobook       0.64      0.67      0.65        21\n               play_game       0.43      0.87      0.58        15\n              play_music       0.83      0.65      0.73       156\n           play_podcasts       0.68      0.84      0.75        43\n              play_radio       1.00      0.62      0.76        52\n             qa_currency       0.79      1.00      0.88        19\n           qa_definition       0.61      0.73      0.67        37\n              qa_factoid       0.57      0.54      0.55       121\n   recommendation_events       0.34      0.70      0.46        23\nrecommendation_locations       0.36      0.91      0.51        11\n             social_post       0.69      0.66      0.67        61\n          takeaway_query       0.48      0.67      0.56        15\n         transport_query       0.65      0.65      0.65        31\n        transport_ticket       0.79      1.00      0.88        15\n           weather_query       0.93      0.93      0.93       136\n\n                accuracy                           0.65      1879\n               macro avg       0.63      0.71      0.64      1879\n            weighted avg       0.67      0.65      0.64      1879\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://medium.com/towards-artificial-intelligence/few-shot-nlp-intent-classification-d29bf85548aa\n* https://github.com/IBM/fastfit","metadata":{}}]}