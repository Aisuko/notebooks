{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nDue to the modularity of BERTopic, it can also be used on large datasets `Cohere/wikipedia-22-12` (>1_000_000) if we change some of the internal algorithms such that they can scale a bit better. And we can also enable GPU-accelrated machine learning.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture --no-stderr\n# !pip install bertopic==0.16.0\n\n!pip install sentence-transformers==2.3.1\n!pip install datasets==2.17.0","metadata":{"execution":{"iopub.status.busy":"2024-02-16T06:46:39.672522Z","iopub.execute_input":"2024-02-16T06:46:39.672915Z","iopub.status.idle":"2024-02-16T06:47:16.379759Z","shell.execute_reply.started":"2024-02-16T06:46:39.672880Z","shell.execute_reply":"2024-02-16T06:47:16.378640Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Sometimes, it might happen that you get the `NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968` error, if so make sure to run the following code","metadata":{}},{"cell_type":"code","source":"# import locale\n\n# _locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset\n\nWe are going to load in Wikipedia texts. Cohere has fortunately created a dataset split by paragraphs, which allows us to stay within token limit sizes. Let's load in 1 million texts from Wikipedia and see if we can extract topics from them.","metadata":{}},{"cell_type":"code","source":"data=load_dataset(f'Cohere/wikipedia-22-12', 'en', split='train[:10000]')\n# docs=[doc['text'] for doc in data if doc['id']!='1_000']\n# print(len(docs))","metadata":{"execution":{"iopub.status.busy":"2024-02-16T07:15:04.178687Z","iopub.execute_input":"2024-02-16T07:15:04.179071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\n\nif __name__ == '__main__':\n    lang='en'\n    data=load_dataset(f'Cohere/wikipedia-22-12', lang, split='train', streaming=True)\n    docs=[doc['text'] for doc in data if doc['id']!='1_000_0']\n    print(len(docs))\n    \n    encoder=SentenceTransformer('all-MiniLM-L6-v2')\n    \n    pool=mode.start_multi_process_pool()\n    \n    emb=encodr.encode_multi_process(docs, pool)\n    print(emb.shape)\n    \n    model.stop_multi_process_pool(pool)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Example\n\nThis example shows the minimum steps necessary for training a BERTopic model on large datasets. Do note though that memeory errors are still possible when tweaking parameters. After this section, some tips will be mentioned to demonstrate how we can further reduce memory or be more efficient with our training process.\n\n\n## Embeddings\n\nNext, we are going to pre-calculate the embeddings as input for our BERTopic model. The reason for doing this is that this input step can take quite some time to compute. If we pre-calculate them and save embeddings, we can skip over this step when we are iterating over our model.","metadata":{}},{"cell_type":"code","source":"# from sentence_transformers import SentenceTransformer\n\n# encoder=SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n# encoder.max_seq_length=256\n# encoder.to('cuda')\n# encoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings=encoder.encode(docs, show_progress_bar=True)\n# embeddings.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to compute embeddings with `multiple` GPUs and append normlization to it. See [Distribution compute of Quora questions embeddings](https://www.kaggle.com/code/aisuko/distribution-compute-of-quora-questions-embeddings)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nembedding_df=pd.DataFrame(embeddings,batch_size=128,chunk_size=1024)\nembedding_df.to_csv('embedding_of_wikipedia_22_12_without_normalization.csv')","metadata":{},"execution_count":null,"outputs":[]}]}