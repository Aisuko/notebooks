{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nNormally we use sentence-transformers model as the default encoder in ML project. However, as the recently research LLM2Vec can convert decoder-only LLMs into text encoders with three steps. So, let's see how to use it.","metadata":{}},{"cell_type":"code","source":"!pip install -U -q llm2vec==0.1.4","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T01:40:12.478188Z","iopub.execute_input":"2024-04-18T01:40:12.479069Z","iopub.status.idle":"2024-04-18T01:40:26.920452Z","shell.execute_reply.started":"2024-04-18T01:40:12.479039Z","shell.execute_reply":"2024-04-18T01:40:26.919402Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"model_name\"]=\"McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp\"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:40:26.922804Z","iopub.execute_input":"2024-04-18T01:40:26.923114Z","iopub.status.idle":"2024-04-18T01:40:26.927603Z","shell.execute_reply.started":"2024-04-18T01:40:26.923085Z","shell.execute_reply":"2024-04-18T01:40:26.926759Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom llm2vec import LLM2Vec\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom peft import PeftModel\n\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv('model_name'))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:40:26.928773Z","iopub.execute_input":"2024-04-18T01:40:26.929097Z","iopub.status.idle":"2024-04-18T01:40:34.909526Z","shell.execute_reply.started":"2024-04-18T01:40:26.929064Z","shell.execute_reply":"2024-04-18T01:40:34.908741Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/871 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81d55bf6e6a470581af8184bb0f9981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d6902c0a8b74ad2b59d532d3303c4eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98f8f152d6e748729d0314ddcdae583c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4acbc5c8a30443d2aa0db94a456438f1"}},"metadata":{}}]},{"cell_type":"code","source":"config=AutoConfig.from_pretrained(os.getenv('model_name'), trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:40:34.910806Z","iopub.execute_input":"2024-04-18T01:40:34.911439Z","iopub.status.idle":"2024-04-18T01:40:35.280843Z","shell.execute_reply.started":"2024-04-18T01:40:34.911405Z","shell.execute_reply":"2024-04-18T01:40:35.279978Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/753 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c2e2fedc76c4829aefa4d29d4f220af"}},"metadata":{}}]},{"cell_type":"code","source":"model=AutoModel.from_pretrained(os.getenv('model_name'), trust_remote_code=True, config=config, torch_dtype=torch.bfloat16, device_map=\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:40:35.283072Z","iopub.execute_input":"2024-04-18T01:40:35.283376Z","iopub.status.idle":"2024-04-18T01:40:54.752591Z","shell.execute_reply.started":"2024-04-18T01:40:35.283352Z","shell.execute_reply":"2024-04-18T01:40:54.751767Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cacbe76253b44eebae5066b3b116de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_llama_encoder.py:   0%|          | 0.00/8.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1685ba8a45a40c0b3d3673226741c54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"attn_mask_utils.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6816096c35d349c7a9ef7d5522f0f880"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp:\n- attn_mask_utils.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp:\n- modeling_llama_encoder.py\n- attn_mask_utils.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/5.38G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd372a5e3cd429c92267cb086cb02bf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of the model checkpoint at princeton-nlp/Sheared-LLaMA-1.3B were not used when initializing LlamaEncoderModel: ['lm_head.weight']\n- This IS expected if you are initializing LlamaEncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaEncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec6c21fa139466ea77a1140f90f64d1"}},"metadata":{}}]},{"cell_type":"code","source":"model=PeftModel.from_pretrained(\n    model,\n    os.getenv('model_name')\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:40:54.753770Z","iopub.execute_input":"2024-04-18T01:40:54.754041Z","iopub.status.idle":"2024-04-18T01:40:55.604866Z","shell.execute_reply.started":"2024-04-18T01:40:54.754019Z","shell.execute_reply":"2024-04-18T01:40:55.604034Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Wrapper for encoding and pooling operations\nl2v=LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:40:55.606001Z","iopub.execute_input":"2024-04-18T01:40:55.606302Z","iopub.status.idle":"2024-04-18T01:41:01.225208Z","shell.execute_reply.started":"2024-04-18T01:40:55.606270Z","shell.execute_reply":"2024-04-18T01:41:01.224114Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Queries","metadata":{}},{"cell_type":"code","source":"instruction=(\"Given a web search query, retrieve relevant passagers that answer the query:\")\n\nqueries=[\n    [instruction, \"How the weather today in Melbourne\"],\n    [instruction, \"Melbourne Parkville suburb\"],\n]\n\nq_reps=l2v.encode(queries)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:44:03.109841Z","iopub.execute_input":"2024-04-18T01:44:03.110209Z","iopub.status.idle":"2024-04-18T01:44:03.359942Z","shell.execute_reply.started":"2024-04-18T01:44:03.110181Z","shell.execute_reply":"2024-04-18T01:44:03.358630Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4e9845575a4fdbab79de1b442a97fb"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m instruction\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven a web search query, retrieve relevant passagers that answer the query:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m queries\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      4\u001b[0m     [instruction, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow the weather today in Melbourne\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     [instruction, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelbourne Parkville suburb\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 8\u001b[0m q_reps\u001b[38;5;241m=\u001b[39m\u001b[43ml2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llm2vec/llm2vec.py:302\u001b[0m, in \u001b[0;36mLLM2Vec.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mlen\u001b[39m(sentences),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar,\n\u001b[1;32m    298\u001b[0m     ):\n\u001b[1;32m    299\u001b[0m         sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[\n\u001b[1;32m    300\u001b[0m             start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size\n\u001b[1;32m    301\u001b[0m         ]\n\u001b[0;32m--> 302\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_to_numpy\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mappend(embeddings)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llm2vec/llm2vec.py:367\u001b[0m, in \u001b[0;36mLLM2Vec._encode\u001b[0;34m(self, sentences_batch, device, convert_to_numpy, multiprocessing)\u001b[0m\n\u001b[1;32m    364\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 367\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    369\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mcpu()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llm2vec/llm2vec.py:190\u001b[0m, in \u001b[0;36mLLM2Vec.forward\u001b[0;34m(self, sentence_feature)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sentence_feature:\n\u001b[1;32m    189\u001b[0m     embed_mask \u001b[38;5;241m=\u001b[39m sentence_feature\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 190\u001b[0m reps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m sentence_feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embed_mask\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pooling(sentence_feature, reps\u001b[38;5;241m.\u001b[39mlast_hidden_state)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:563\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    562\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp/813a65a03add82b52fb2664efc85b0a1ed8661d3/modeling_llama_encoder.py:132\u001b[0m, in \u001b[0;36mLlamaEncoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    128\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# output_attentions=True can not be supported when using SDPA, and we fall back on\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# the manual implementation that requires a 4D causal mask in all cases.\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# 4d mask is passed through the layers\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask(\n\u001b[1;32m    141\u001b[0m         attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n\u001b[1;32m    142\u001b[0m     )\n","File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp/813a65a03add82b52fb2664efc85b0a1ed8661d3/attn_mask_utils.py:62\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(attention_mask, input_shape, inputs_embeds, past_key_values_length, sliding_window)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         expanded_4d_mask \u001b[38;5;241m=\u001b[39m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unmask_unattended\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpanded_4d_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munmasked_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expanded_4d_mask\n","\u001b[0;31mTypeError\u001b[0m: AttentionMaskConverter._unmask_unattended() got an unexpected keyword argument 'unmasked_value'"],"ename":"TypeError","evalue":"AttentionMaskConverter._unmask_unattended() got an unexpected keyword argument 'unmasked_value'","output_type":"error"}]},{"cell_type":"markdown","source":"# Encoding Documents","metadata":{}},{"cell_type":"code","source":"documents=[\n    \"\"\"Melbourne is notorious for its fickle weather, often experiencing \"four seasons in one day.\" Summers (December to February) are warm to hot, with average temperatures ranging from 14°C to 25°C, but occasional heatwaves can push the mercury above 40°C. Autumn (March to May) brings milder temperatures and stunning foliage, while winters (June to August) are cool and wet, with average temperatures between 6°C and 14°C. Spring (September to November) is a delightful time with blossoming flowers and temperatures gradually warming up. It's always wise to be prepared for anything when heading outdoors in Melbourne!\"\"\"\n]\n\nd_reps=l2v.encode(documents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q_res_norm=torch.nn.functional.normalize(q_res, p=2, dim=1)\nd_res_norm=torch.nn.functional.normalize(d_res, p=2, dim=1)\ncos_sim=torch.mm(q_reps_norm, d_reps_norm.transpose(0,1))\ncos_sim","metadata":{"execution":{"iopub.status.busy":"2024-04-18T01:41:03.073593Z","iopub.status.idle":"2024-04-18T01:41:03.074360Z","shell.execute_reply.started":"2024-04-18T01:41:03.074089Z","shell.execute_reply":"2024-04-18T01:41:03.074117Z"},"trusted":true},"execution_count":null,"outputs":[]}]}