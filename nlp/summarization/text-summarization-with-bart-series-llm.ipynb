{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6004344,"sourceType":"datasetVersion","datasetId":3438844}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/text-summarization-with-bart-series-llm?scriptVersionId=163528181\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will fine-tune `facebook/bart-large-xsum` model on `SamSum` dataset.\n\nNote: There is a technique we did not mentioned in the previously notebook. It is `transfer learning`, we can also call it `fine-tuning`.\n\n\n# Evaluation Strategy\n\nEvaluating performance for language models can be quite tricky, especially when it comes to text summarization. The goal of our model is to produce a short sentence describing the content of a dialogue, while maintaining all the important information within that dialogue.\n\nOne of the quantitative metrics we can employ to evaluate performance is the `ROUGE Score`. It is considered one of the best metrics for text summarization and it evaluates performance by comparing the quality of a machine-generated summary to a human generated summary used for reference.\n\nThe similarities between both summaries are measured by analyzing the overlapping `n-grams`, either single words of sequences of words that are present in both summaries. These can be unigrams(ROUGE-1), where only the overlap of sole words is measured; biggrams(ROUGE-2), where we measure the overlap of two-word sequencesl trigrams(ROUGE-3), where we measrure the overlap of three-word sequences; etc. Besides that, we also have:\n\n\n**ROUGE-L**\n\nIt measures the *Longest Common Subsequence(LCS)* between the two summaries, which helps to capture content coverage of the machine-generated text. If both summaries have the sequence \"the apple is green\", we have a match regardless of where they appear in both texts.\n\n**ROUGE-S**\n\nIt avaluates the overlap of skip-bigrams, which are bigrams that permit gaps between words. This helps to measure the coherence of a machine-generated summary. For example, in the phrase \"this apple is absolutely green\", we find a match for the terms such as \"apple\" and \"green\", if that is what we are looking for.\n\nThese scores might typically range from 0 to 100, where 0 indicates no match and 100 indicates a perfect match between both summaries. Besides quantitative metrics, it is useful to use `human evaluation` to analyze the output of language models, since we are able to comprehend text in a wat that a machine does not.\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!nvidia-smi # Checking GPU","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:35:58.859714Z","iopub.execute_input":"2024-02-20T06:35:58.860204Z","iopub.status.idle":"2024-02-20T06:35:59.973417Z","shell.execute_reply.started":"2024-02-20T06:35:58.860154Z","shell.execute_reply":"2024-02-20T06:35:59.972337Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Tue Feb 20 06:35:59 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   39C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture --no-stderr\n!pip install transformers==4.37.2\n!pip install datasets==2.17.0\n!pip install evaluate==0.4.1\n!pip install rouge-score==0.1.2","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:35:59.975655Z","iopub.execute_input":"2024-02-20T06:35:59.976036Z","iopub.status.idle":"2024-02-20T06:37:34.659855Z","shell.execute_reply.started":"2024-02-20T06:35:59.976001Z","shell.execute_reply":"2024-02-20T06:37:34.658699Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ['MODEL']='facebook/bart-large-xsum'\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning BART Series LLMs\"\nos.environ[\"WANDB_NOTES\"] = \"\"\nos.environ[\"WANDB_NAME\"] = \"ft-facebook-bart-large-xsum-on-samsum\"","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:34.661445Z","iopub.execute_input":"2024-02-20T06:37:34.661773Z","iopub.status.idle":"2024-02-20T06:37:35.863335Z","shell.execute_reply.started":"2024-02-20T06:37:34.661742Z","shell.execute_reply":"2024-02-20T06:37:35.862123Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:35.86494Z","iopub.execute_input":"2024-02-20T06:37:35.865532Z","iopub.status.idle":"2024-02-20T06:37:35.870618Z","shell.execute_reply.started":"2024-02-20T06:37:35.865491Z","shell.execute_reply":"2024-02-20T06:37:35.869609Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Data Handling\nimport pandas as pd\n\ntrain=pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-train.csv')\ntest=pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv')\nval=pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv')\ntype(train)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:35.874169Z","iopub.execute_input":"2024-02-20T06:37:35.875246Z","iopub.status.idle":"2024-02-20T06:37:36.955251Z","shell.execute_reply.started":"2024-02-20T06:37:35.875207Z","shell.execute_reply":"2024-02-20T06:37:36.954211Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"pandas.core.frame.DataFrame"},"metadata":{}}]},{"cell_type":"markdown","source":"In the notebook [Visualisation and Statistic SamSum Dataset](https://www.kaggle.com/code/aisuko/visualisation-and-statistic-samsum-dataset), we can see that some tags in a few texts, such as `file_photo` in dialogue. Let's remove these tags from the texts.","metadata":{}},{"cell_type":"code","source":"print(train['dialogue'].iloc[14727])","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:36.956328Z","iopub.execute_input":"2024-02-20T06:37:36.956609Z","iopub.status.idle":"2024-02-20T06:37:36.965766Z","shell.execute_reply.started":"2024-02-20T06:37:36.956584Z","shell.execute_reply":"2024-02-20T06:37:36.964477Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Romeo: You are on my ‘People you may know’ list.\nGreta: Ah, maybe it is because of the changed number of somebody’s?\nGreta: I don’t know you?\nRomeo: This might be the beginning of a beautiful relationship\nRomeo: How about adding me on your friend list and talk a bit?\nGreta: No.\nRomeo: Okay I see.\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n\ndef clean_tags(text):\n    clean=re.compile('<.*?>') # compiling tags\n    clean=re.sub(clean, '', text) # replacing tags text by an empty string\n    \n    # removing empty dialogues\n    clean='\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n    return clean\n\ntest1=clean_tags(train['dialogue'].iloc[14727])\ntest2=clean_tags(test['dialogue'].iloc[0])\n\nprint(test1)\nprint('\\n'*3)\nprint(test2)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:36.967056Z","iopub.execute_input":"2024-02-20T06:37:36.967368Z","iopub.status.idle":"2024-02-20T06:37:36.989389Z","shell.execute_reply.started":"2024-02-20T06:37:36.967344Z","shell.execute_reply":"2024-02-20T06:37:36.988343Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Romeo: You are on my ‘People you may know’ list.\nGreta: Ah, maybe it is because of the changed number of somebody’s?\nGreta: I don’t know you?\nRomeo: This might be the beginning of a beautiful relationship\nRomeo: How about adding me on your friend list and talk a bit?\nGreta: No.\nRomeo: Okay I see.\n\n\n\n\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's define a function and apply `clean_tags` to the entire datasets. It's beneficial to conduct such data cleansing to eliminate noise-information.","metadata":{}},{"cell_type":"code","source":"def clean_df(df, cols):\n    for col in cols:\n        df[col]=df[col].fillna('').apply(clean_tags)\n    return df\n\ntrain=clean_df(train, ['dialogue','summary'])\ntest=clean_df(test, ['dialogue', 'summary'])\nval=clean_df(val, ['dialogue', 'summary'])\n\n# visualizing results\ntrain.tail(3)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:36.990554Z","iopub.execute_input":"2024-02-20T06:37:36.990836Z","iopub.status.idle":"2024-02-20T06:37:37.498584Z","shell.execute_reply.started":"2024-02-20T06:37:36.990813Z","shell.execute_reply":"2024-02-20T06:37:37.497646Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"             id                                           dialogue  \\\n14729  13819050  John: Every day some bad news. Japan will hunt...   \n14730  13828395  Jennifer: Dear Celia! How are you doing?\\r\\nJe...   \n14731  13729017  Georgia: are you ready for hotel hunting? We n...   \n\n                                                 summary  \n14729  Japan is going to hunt whales again. Island an...  \n14730  Celia couldn't make it to the afternoon with t...  \n14731  Georgia and Juliette are looking for a hotel i...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dialogue</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14729</th>\n      <td>13819050</td>\n      <td>John: Every day some bad news. Japan will hunt...</td>\n      <td>Japan is going to hunt whales again. Island an...</td>\n    </tr>\n    <tr>\n      <th>14730</th>\n      <td>13828395</td>\n      <td>Jennifer: Dear Celia! How are you doing?\\r\\nJe...</td>\n      <td>Celia couldn't make it to the afternoon with t...</td>\n    </tr>\n    <tr>\n      <th>14731</th>\n      <td>13729017</td>\n      <td>Georgia: are you ready for hotel hunting? We n...</td>\n      <td>Georgia and Juliette are looking for a hotel i...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Data Handling\nfrom datasets import Dataset\n\ntrain_ds=Dataset.from_pandas(train)\ntest_ds=Dataset.from_pandas(test)\nval_ds=Dataset.from_pandas(val)\n\nprint(train_ds)\nprint('\\n'*2)\nprint(test_ds)\nprint('\\n'*2)\nprint(val_ds)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:37.499738Z","iopub.execute_input":"2024-02-20T06:37:37.499993Z","iopub.status.idle":"2024-02-20T06:37:38.319079Z","shell.execute_reply.started":"2024-02-20T06:37:37.499971Z","shell.execute_reply":"2024-02-20T06:37:38.318101Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['id', 'dialogue', 'summary'],\n    num_rows: 14732\n})\n\n\n\nDataset({\n    features: ['id', 'dialogue', 'summary'],\n    num_rows: 819\n})\n\n\n\nDataset({\n    features: ['id', 'dialogue', 'summary'],\n    num_rows: 818\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:38.320449Z","iopub.execute_input":"2024-02-20T06:37:38.320933Z","iopub.status.idle":"2024-02-20T06:37:38.327496Z","shell.execute_reply.started":"2024-02-20T06:37:38.320904Z","shell.execute_reply":"2024-02-20T06:37:38.326535Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'id': '13818513',\n 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepprocess the data\n\nThe following `preprocess_func` can be directly copied from the Transformers documentation, and it serves well to preprocess data for several NLP tasks.","metadata":{}},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration # BERT Tokenizer and architecture\n\ntokenizer=BartTokenizer.from_pretrained(os.getenv('MODEL'))\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:38.328806Z","iopub.execute_input":"2024-02-20T06:37:38.329055Z","iopub.status.idle":"2024-02-20T06:37:48.847101Z","shell.execute_reply.started":"2024-02-20T06:37:38.329033Z","shell.execute_reply":"2024-02-20T06:37:48.846134Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33bbd6f4c3b4871be4fff683eba6e7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90259abcde4b4d449f7057bc3a16a77b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699d7ad187f44d6d8e79ac850664021c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6db9d0cc6fd64729a2a793ae3fdb4c6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ce6275820345a7ae86e0385321a319"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BartTokenizer(name_or_path='facebook/bart-large-xsum', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_func(example):\n    # Iterating over every `dialogue` in the datset and saving them as input to the model\n    inputs=[doc for doc in example['dialogue']]\n    # we use tokenizer convert the input dialogues into tokens that can be easily understood by the BART model.\n    # The truncation=True parameter ensures that all dialogues have a maximum number of 1024 tokens, as defined by the `max_length` parameter\n    model_inputs=tokenizer(inputs, max_length=1024, truncation=True)\n    \n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        # we tokenizes the target variable, which is our summaries. And we expect summaries to be a much shorter text than that of dialogues max_length=128\n        labels=tokenizer(example['summary'], max_length=128, truncation=True)\n    \n    # we adding the tokenized labels to the preprocessed dataset, alongside the tokenized inputs.\n    model_inputs['labels']=labels['input_ids']\n    return model_inputs\n\n\ntokenized_train= train_ds.map(preprocess_func, batched=True, remove_columns=['id', 'dialogue', 'summary'])\ntokenized_test=test_ds.map(preprocess_func, batched=True, remove_columns=['id', 'dialogue', 'summary'])\ntokenized_val=val_ds.map(preprocess_func, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n\nprint(tokenized_train)\nprint(tokenized_test)\nprint(tokenized_val)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:37:48.848191Z","iopub.execute_input":"2024-02-20T06:37:48.848706Z","iopub.status.idle":"2024-02-20T06:38:25.096684Z","shell.execute_reply.started":"2024-02-20T06:37:48.848668Z","shell.execute_reply":"2024-02-20T06:38:25.095696Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18b90e0bf185437b8c68525b306d6856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f19731ecce042ccad693757ee14c865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4167539012fd4889baadb5dc0824abd4"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 14732\n})\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 819\n})\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 818\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Checking sample\n\nOur tokenized datasets consist now of only three features, `input_ids`, `attention_mask` and `labels`. Let's print a sample from our tokenized train dataset to investigate further how the preprocess function altered the data.","metadata":{}},{"cell_type":"code","source":"sample=tokenized_train[0]\nprint(sample['input_ids'])\nprint(sample['attention_mask'])\nprint(sample['labels'])","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:38:25.098205Z","iopub.execute_input":"2024-02-20T06:38:25.098961Z","iopub.status.idle":"2024-02-20T06:38:25.105187Z","shell.execute_reply.started":"2024-02-20T06:38:25.098923Z","shell.execute_reply":"2024-02-20T06:38:25.104194Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[0, 10127, 5219, 35, 38, 17241, 1437, 15269, 4, 1832, 47, 236, 103, 116, 50121, 50118, 39237, 35, 9136, 328, 50121, 50118, 10127, 5219, 35, 38, 581, 836, 47, 3859, 48433, 2]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[0, 10127, 5219, 17241, 15269, 8, 40, 836, 6509, 103, 3859, 4, 2]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**input_ids**\n\nThere are the token IDs mapped to the dialogues. Each token represents a word or subword that can be perfectly understood by the BART model. For instance, the number 5219 could be a map to a word like hello in BARt's vocabulary. Each word has its unique token in this context.\n\n**attention_mask**\n\nThis mask indicates which tokens the model should pay attention to and which tokens should be ignored. This is often used in the context of padding - when some tokens are used to equalize the lengths of sentences - but most of these padding tokens do not hold any meaningful information, so the attention mask ensures the model does not focus on them. In the case of this specific sample, all toknes are masked as '1', meaning they are all relevant and none of them are used for padding.\n\n**labels**\n\nSimilarly to the first feature, these are token IDs obtained from the words and subwords in the summaries. These are the tokens that the model will be trained on to give as output.","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nsummarizer=pipeline('summarization', model=os.getenv('MODEL'))\n\nnews=\"summarize:Melbourne, Australia's cultural capital, pulsates with a vibrant energy. Grand Victorian architecture mingles with modern laneways bursting with street art, cafes, and independent shops. World-class museums and galleries showcase diverse collections, while renowned sporting events like the Australian Open electrify the atmosphere. Foodies delight in a multicultural culinary scene, from Michelin-starred restaurants to hidden gems serving up global flavors. Beyond the city, pristine beaches and lush national parks offer escapes into nature, making Melbourne a city that truly has it all.\"\n\nsummarizer(news)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:38:25.109127Z","iopub.execute_input":"2024-02-20T06:38:25.109449Z","iopub.status.idle":"2024-02-20T06:38:58.924602Z","shell.execute_reply.started":"2024-02-20T06:38:25.109421Z","shell.execute_reply":"2024-02-20T06:38:58.923615Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-02-20 06:38:28.479653: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-20 06:38:28.479777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-20 06:38:28.754605: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03ae8056c0243508354a610a1349a9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/309 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e4a315e3554c219507508318c97e29"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': \"Melbourne, Australia, is one of the world's most visited cities, according to Lonely Planet.\"}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading model\n\nIt is possible to see below that models consist of an encoder and a decoder, we can see the Linear Layers, as well as the activation functions, which use $GeLU$, instead of the more typical $ReLU$. It is also interesting to observe the output layer, **lm_head**, which shows us that this model is ideal for generating outputs with a vocabulary size - `out_features=50264` - this shows us that this architecture","metadata":{}},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration\n\nmodel=BartForConditionalGeneration.from_pretrained(os.getenv('MODEL'))\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:38:58.925842Z","iopub.execute_input":"2024-02-20T06:38:58.926616Z","iopub.status.idle":"2024-02-20T06:39:01.170669Z","shell.execute_reply.started":"2024-02-20T06:38:58.92657Z","shell.execute_reply":"2024-02-20T06:39:01.168916Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): Embedding(50264, 1024, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Batching the data\n\nWe must now use `DataCollatorForSeq2Seq` to batch the data. These data collators may also automatically apply some processing techniques, such as padding.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator= DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\nprint(data_collator)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:39:01.171951Z","iopub.execute_input":"2024-02-20T06:39:01.172318Z","iopub.status.idle":"2024-02-20T06:39:03.775432Z","shell.execute_reply.started":"2024-02-20T06:39:01.172264Z","shell.execute_reply":"2024-02-20T06:39:03.774128Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"DataCollatorForSeq2Seq(tokenizer=BartTokenizer(name_or_path='facebook/bart-large-xsum', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n}, model=BartForConditionalGeneration(\n  (model): BartModel(\n    (shared): Embedding(50264, 1024, padding_idx=1)\n    (encoder): BartEncoder(\n      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartEncoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): BartDecoder(\n      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x BartDecoderLayer(\n          (self_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): BartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading evaluation metrics","metadata":{}},{"cell_type":"code","source":"from datasets import load_metric\n\nmetric=load_metric('rouge')","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:39:03.777407Z","iopub.execute_input":"2024-02-20T06:39:03.778101Z","iopub.status.idle":"2024-02-20T06:39:05.048369Z","shell.execute_reply.started":"2024-02-20T06:39:03.778061Z","shell.execute_reply":"2024-02-20T06:39:05.047331Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b726dcabca468eb672cc5fa8d30d1e"}},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nimport numpy as np\n\n# this divides a text into a list of sentences\nnltk.download('punkt')\n\ndef compute_metrics(eval_pred):\n    predictions, labels=eval_pred # obtaining predictions and true labels\n    \n    # decoding predictions\n    decoded_preds=tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # obtaining the true labels tokens, while eliminating any possible masked token (i.e: label=-100)\n    labels=np.where(labels!=-100, labels, tokenizer.pad_token_id)\n    decoded_labels=tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # rouge expects a newline after each sentence\n    decoded_preds=['\\n'.join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels=['\\n'.join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # computing rouge score\n    result=metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    result={key: value.mid.fmeasure*100 for key, value in result.items()} # extracting some results\n    \n    # add mean-genrated length\n    prediction_lens=[np.count_nonzero(pred!=tokenizer.pad_token_id) for pred in predictions]\n    result['gen_len']=np.mean(prediction_lens)\n    return {k: round(v,4) for k,v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:39:05.050064Z","iopub.execute_input":"2024-02-20T06:39:05.050785Z","iopub.status.idle":"2024-02-20T06:39:05.368475Z","shell.execute_reply.started":"2024-02-20T06:39:05.050748Z","shell.execute_reply":"2024-02-20T06:39:05.367455Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\ntraining_args=Seq2SeqTrainingArguments(\n    output_dir=os.getenv('WANDB_NAME'),\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    seed=42,\n    learning_rate=2e-5,\n    max_steps=100,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=1, # only for testing\n    predict_with_generate=True,\n    fp16=True,\n    report_to='wandb',\n    run_name=os.getenv('WANDB_NAME')\n)\n\ntrainer=Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:39:05.369517Z","iopub.execute_input":"2024-02-20T06:39:05.369816Z","iopub.status.idle":"2024-02-20T06:51:52.3096Z","shell.execute_reply.started":"2024-02-20T06:39:05.369791Z","shell.execute_reply":"2024-02-20T06:51:52.308655Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33murakiny\u001b[0m (\u001b[33mcausal_language_trainer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240220_063911-8gdu1sbz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20BART%20Series%20LLMs/runs/8gdu1sbz' target=\"_blank\">ft-facebook-bart-large-xsum-on-samsum</a></strong> to <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20BART%20Series%20LLMs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20BART%20Series%20LLMs' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20BART%20Series%20LLMs</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20BART%20Series%20LLMs/runs/8gdu1sbz' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20BART%20Series%20LLMs/runs/8gdu1sbz</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 12:02, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>1.505537</td>\n      <td>49.551200</td>\n      <td>24.556800</td>\n      <td>40.703900</td>\n      <td>45.227400</td>\n      <td>26.423700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=1.5889302062988282, metrics={'train_runtime': 763.3675, 'train_samples_per_second': 4.192, 'train_steps_per_second': 0.131, 'total_flos': 2331956926414848.0, 'train_loss': 1.5889302062988282, 'epoch': 0.22})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluating","metadata":{}},{"cell_type":"code","source":"validation=trainer.evaluate(eval_dataset=tokenized_val)\nprint(validation)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:51:52.310881Z","iopub.execute_input":"2024-02-20T06:51:52.31121Z","iopub.status.idle":"2024-02-20T06:57:19.690034Z","shell.execute_reply.started":"2024-02-20T06:51:52.311182Z","shell.execute_reply":"2024-02-20T06:57:19.688981Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [103/103 05:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 1.4688810110092163, 'eval_rouge1': 50.9912, 'eval_rouge2': 25.7585, 'eval_rougeL': 41.4197, 'eval_rougeLsum': 46.5946, 'eval_gen_len': 26.8814, 'eval_runtime': 327.367, 'eval_samples_per_second': 2.499, 'eval_steps_per_second': 0.315, 'epoch': 0.22}\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GenerationConfig\nkwargs={\n    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n    'finetuned_from': f'{os.getenv(\"MODEL\")}',\n    'tasks': 'summarization'\n}\n\ntrainer.push_to_hub(**kwargs)\ntokenizer.push_to_hub(os.getenv('WANDB_NAME'))\n\ngeneration_config=GenerationConfig(\n    max_length=62, min_length=11, early_stopping=True, num_beams=6, no_repeat_ngram_size=3, forced_eos_token_id=2\n)\n\ngeneration_config.save_pretrained('aisuko/'+os.getenv('WANDB_NAME'), push_to_hub=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:57:19.691617Z","iopub.execute_input":"2024-02-20T06:57:19.691988Z","iopub.status.idle":"2024-02-20T06:58:05.260126Z","shell.execute_reply.started":"2024-02-20T06:57:19.691953Z","shell.execute_reply":"2024-02-20T06:58:05.258929Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 62, 'min_length': 11, 'early_stopping': True, 'num_beams': 6, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a0b32a955bd40859eb475258e89d861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae1da8e44c364b21be0d940b2bf4c1c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113c19ed28d24903802d3050d604c078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.70k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a48f5803cf04cde945083327bf2d850"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"aisuko/ft-facebook-bart-large-xsum-on-samsum\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"aisuko/ft-facebook-bart-large-xsum-on-samsum\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:58:05.261628Z","iopub.execute_input":"2024-02-20T06:58:05.261953Z","iopub.status.idle":"2024-02-20T06:58:06.516578Z","shell.execute_reply.started":"2024-02-20T06:58:05.261925Z","shell.execute_reply":"2024-02-20T06:58:06.514344Z"},"trusted":true},"execution_count":22,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maisuko/ft-facebook-bart-large-xsum-on-samsum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maisuko/ft-facebook-bart-large-xsum-on-samsum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:773\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 773\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1100\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1098\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1100\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1102\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:634\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    636\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:356\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         )\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mOSError\u001b[0m: aisuko/ft-facebook-bart-large-xsum-on-samsum does not appear to have a file named config.json. Checkout 'https://huggingface.co/aisuko/ft-facebook-bart-large-xsum-on-samsum/None' for available files."],"ename":"OSError","evalue":"aisuko/ft-facebook-bart-large-xsum-on-samsum does not appear to have a file named config.json. Checkout 'https://huggingface.co/aisuko/ft-facebook-bart-large-xsum-on-samsum/None' for available files.","output_type":"error"}]},{"cell_type":"code","source":"val_ds[35]['dialogue']","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:58:06.517505Z","iopub.status.idle":"2024-02-20T06:58:06.517952Z","shell.execute_reply.started":"2024-02-20T06:58:06.517728Z","shell.execute_reply":"2024-02-20T06:58:06.51775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summarizer(val_ds[35]['dialogue'])","metadata":{"execution":{"iopub.status.busy":"2024-02-20T06:58:06.518994Z","iopub.status.idle":"2024-02-20T06:58:06.519372Z","shell.execute_reply.started":"2024-02-20T06:58:06.519178Z","shell.execute_reply":"2024-02-20T06:58:06.519192Z"},"trusted":true},"execution_count":null,"outputs":[]}]}