{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6004344,"sourceType":"datasetVersion","datasetId":3438844}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/minimal-reproducer-for-issue-of-transformer-29128?scriptVersionId=164774587\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.37.2\n!pip install datasets==2.17.0\n!pip install evaluate==0.4.1\n!pip install rouge-score==0.1.2","metadata":{"execution":{"iopub.status.busy":"2024-02-21T03:13:32.952165Z","iopub.execute_input":"2024-02-21T03:13:32.952537Z","iopub.status.idle":"2024-02-21T03:14:49.349169Z","shell.execute_reply.started":"2024-02-21T03:13:32.952506Z","shell.execute_reply":"2024-02-21T03:14:49.348061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries\nimport os\nimport re\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom datasets import Dataset\nfrom datasets import load_metric\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom transformers import BartForConditionalGeneration\nfrom transformers import DataCollatorForSeq2Seq\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nos.environ['MODEL']='facebook/bart-large-xsum'\nos.environ[\"WANDB_NAME\"] = \"ft-facebook-bart-large-xsum-on-samsum\"\n\nwarnings.filterwarnings('ignore')\n\n# Loading and preprocessing data from https://www.kaggle.com/datasets/nileshmalode1/samsum-dataset-text-summarization\ntrain=pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-train.csv')\ntest=pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv')\nval=pd.read_csv('/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv')\n\ndef clean_tags(text):\n    clean=re.compile('<.*?>') # compiling tags\n    clean=re.sub(clean, '', text) # replacing tags text by an empty string\n    \n    # removing empty dialogues\n    clean='\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n    return clean\n\ndef clean_df(df, cols):\n    for col in cols:\n        df[col]=df[col].fillna('').apply(clean_tags)\n    return df\n\ntrain=clean_df(train, ['dialogue','summary'])\ntest=clean_df(test, ['dialogue', 'summary'])\nval=clean_df(val, ['dialogue', 'summary'])\n\ntrain_ds=Dataset.from_pandas(train)\ntest_ds=Dataset.from_pandas(test)\nval_ds=Dataset.from_pandas(val)\n\n# Tokenizer\ntokenizer=BartTokenizer.from_pretrained(os.getenv('MODEL'))\n\ndef preprocess_func(example):\n    # Iterating over every `dialogue` in the datset and saving them as input to the model\n    inputs=[doc for doc in example['dialogue']]\n    # we use tokenizer convert the input dialogues into tokens that can be easily understood by the BART model.\n    # The truncation=True parameter ensures that all dialogues have a maximum number of 1024 tokens, as defined by the `max_length` parameter\n    model_inputs=tokenizer(inputs, max_length=1024, truncation=True)\n    \n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        # we tokenizes the target variable, which is our summaries. And we expect summaries to be a much shorter text than that of dialogues max_length=128\n        labels=tokenizer(example['summary'], max_length=128, truncation=True)\n    \n    # we adding the tokenized labels to the preprocessed dataset, alongside the tokenized inputs.\n    model_inputs['labels']=labels['input_ids']\n    return model_inputs\n\n\ntokenized_train= train_ds.map(preprocess_func, batched=True, remove_columns=['id', 'dialogue', 'summary'])\ntokenized_test=test_ds.map(preprocess_func, batched=True, remove_columns=['id', 'dialogue', 'summary'])\ntokenized_val=val_ds.map(preprocess_func, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n\n# Loading the model\nmodel=BartForConditionalGeneration.from_pretrained(os.getenv('MODEL'))\n\n# Loading DataCollator\ndata_collator= DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# Customizing metrics\nmetric=load_metric('rouge')\n\nnltk.download('punkt')  # this divides a text into a list of sentences\n\ndef compute_metrics(eval_pred):\n    predictions, labels=eval_pred # obtaining predictions and true labels\n    \n    # decoding predictions\n    decoded_preds=tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # obtaining the true labels tokens, while eliminating any possible masked token (i.e: label=-100)\n    labels=np.where(labels!=-100, labels, tokenizer.pad_token_id)\n    decoded_labels=tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # rouge expects a newline after each sentence\n    decoded_preds=['\\n'.join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels=['\\n'.join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # computing rouge score\n    result=metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    result={key: value.mid.fmeasure*100 for key, value in result.items()} # extracting some results\n    \n    # add mean-genrated length\n    prediction_lens=[np.count_nonzero(pred!=tokenizer.pad_token_id) for pred in predictions]\n    result['gen_len']=np.mean(prediction_lens)\n    return {k: round(v,4) for k,v in result.items()}\n\n\n# Training\ntraining_args=Seq2SeqTrainingArguments(\n    output_dir=os.getenv('WANDB_NAME'),\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    seed=42,\n    learning_rate=2e-5,\n    max_steps=100,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=1, # only for testing\n    predict_with_generate=True,\n    fp16=True,\n    report_to='none',\n)\n\ntrainer=Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-21T03:14:49.351838Z","iopub.execute_input":"2024-02-21T03:14:49.352238Z","iopub.status.idle":"2024-02-21T03:27:46.299547Z","shell.execute_reply.started":"2024-02-21T03:14:49.352185Z","shell.execute_reply":"2024-02-21T03:27:46.29868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs={\n    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n    'finetuned_from': f'{os.getenv(\"MODEL\")}',\n    'tasks': 'summarization',\n    'dataset_tags':'text-summatization',\n    'dataset':'Samsum'\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{},"execution_count":null,"outputs":[]}]}