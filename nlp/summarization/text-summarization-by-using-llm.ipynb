{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will fine-tune `facebook/bart-large-xsum` model on `SamSum` dataset.\n\nNote: There is a technique we did not mentioned in the previously notebook. It is `transfer learning`, we can also call it `fine-tuning`.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Evaluation Metrics\n\nEvaluating performance for language models can be quite tricky, especially when it comes to text summarization. The goal of our model is to produce a short sentence describing the content of a dialogue, while maintaining all the important information within that dialogue.\n\nOne of the quantitative metrics we can employ to evaluate performance is the `ROUGE Score`. It is considered one of the best metrics for text summarization and it evaluates performance by comparing the quality of a machine-generated summary to a human generated summary used for reference.\n\nThe similarities between both summaries are measured by analyzing the overlapping `n-grams`, either single words of sequences of words that are present in both summaries. These can be unigrams(ROUGE-1), where only the overlap of sole words is measured; biggrams(ROUGE-2), where we measure the overlap of two-word sequencesl trigrams(ROUGE-3), where we measrure the overlap of three-word sequences; etc. Besides that, we also have:\n\n\n**ROUGE-L**\n\nIt measures the *Longest Common Subsequence(LCS)* between the two summaries, which helps to capture content coverage of the machine-generated text. If both summaries have the sequence \"the apple is green\", we have a match regardless of where they appear in both texts.\n\n**ROUGE-S**\n\nIt avaluates the overlap of skip-bigrams, which are bigrams that permit gaps between words. This helps to measure the coherence of a machine-generated summary. For example, in the phrase \"this apple is absolutely green\", we find a match for the terms such as \"apple\" and \"green\", if that is what we are looking for.\n\nThese scores might typically range from 0 to 100, where 0 indicates no match and 100 indicates a perfect match between both summaries. Besides quantitative metrics, it is useful to use `human evaluation` to analyze the output of language models, since we are able to comprehend text in a wat that a machine does not.\n\n","metadata":{}},{"cell_type":"code","source":"!nvidia-smi # Checking GPU","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.37.2\n!pip install datasets==2.17.0\n!pip install evaluate==0.4.1\n!pip install rouge-score==0.1.2\n# Installing library to save zip archives\n!pip install py7zr==0.20.8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarning.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuring Pandas to exhibit larger columns\n\nimport pandas as pd\n\npd.set_option('display.max_colwidth', 1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Credit\n\n* https://www.kaggle.com/code/lusfernandotorres/text-summarization-with-large-language-models/notebook","metadata":{}}]}