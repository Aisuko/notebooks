{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn the notebooks [Semantic Textual Similarity](https://www.kaggle.com/code/aisuko/semantic-textual-similarity), [Semantic Search](https://www.kaggle.com/code/aisuko/semantic-search), [Similar Questions Retrieval](https://www.kaggle.com/code/aisuko/similar-questions-retrieval), [Semantic Search in Publications](https://www.kaggle.com/code/aisuko/semantic-search-in-publications), [Wikipedia Q&A Retrieval Semantic Search](https://www.kaggle.com/code/aisuko/wikipedia-q-a-retrieval-semantic-search). We have shown how to compute the embeddings of queries, sentences, paragraphs, articles, and how to use semantic search function.\n\nIn this notebook, we are going to show a complex search tasks, for example, for question answering retrieval, the search can significantly be improved by using **Retrieve & Re_Rank**.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Retrieve & Re-Rank Pipeline\n\nA pipeline for information retrieval / question answering retrieval that works well is the following. All components are provided and explained in here:\n\n<div style=\"text-align: center\"><img src=\"https://hostux.social/system/media_attachments/files/111/898/410/559/698/272/original/c090c2ca83c51a40.png\" width=\"100%\" heigh=\"100%\" alt=\"Retrieve&Re-Rank pipeline\"></div>\n\n\nFrom the picture above. For a query, we first use a **retrieval system** that retrieves a large list of e.g. 100 possible hits which are potentially relevant for the query. For the retrieval, we can use either lexical search, e.g, with Elasticsearch, or we can use dense retrieval with a semantic model.\n\nHowever, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, we use a **re-ranker** based on a **cross-encoder** that scores that relevancy of all candidates for the given search query.\n\nThe output will be a ranked list of hits we can present to the user.","metadata":{}},{"cell_type":"markdown","source":"## Retrieval System: Bi-Encoder\n\nFor the retrieval of the candidate set, we can either use lexical search (e.g. Elasticsearch), or we can use a semantic model. However, lexical search looks for literal matches of the query words in our document collection. It will not recognize synonyms, acronyms or spelling variations. In contrast, semantic search (or dense retrieval) encodes the search query into vector space and retrieves the document embeddings that are close in vector space.\n\n<div style=\"text-align: center\"><img src=\"https://hostux.social/system/media_attachments/files/111/888/433/770/763/125/original/dafea983f4905b4b.png\" width=\"60%\" heigh=\"60%\" alt=\"Semantic Search\"></div>\n\nHave a look at the notebook [Semantic Search](https://www.kaggle.com/code/aisuko/semantic-search) to get more detail.","metadata":{}},{"cell_type":"markdown","source":"## Re-Rank: Cross-Encoder\n\nThe retriever has to be efficient for large document collections with millions of entries. However, it might return irrelevant candidates.\n\nA re-ranker based on a Cross-Encoder can substantially improve the final results for the user. The query and a possible document is passed simultaneously to transformer network, which when outputs a single score between 0 and 1 indicating how relevant the document is for the given query.\n\n<div style=\"text-align: center\"><img src=\"https://hostux.social/system/media_attachments/files/111/898/502/437/707/103/original/cd1bb78c46020897.png\" width=\"60%\" heigh=\"60%\" alt=\"Re-Rank:Cross-Encoder\"></div>\n\nThe advantage of Cross-Encoders is the higher performance, as they perform attention across the query and the document.\n\nScoring thousands or millions of (query, document)-pairs would be rather slow. Hence, we use the retriever to create a set of e.g. 100 possible candidates which are then re-ranked by the Cross-Encoder.","metadata":{}},{"cell_type":"markdown","source":"# Implementation of Retrieve Rerank\n\nLet's use the smaller Simple English Wikipedia(it is fits better in RAM) as document collection to provide answers to user questions/ search queries. First, we split all Wikipedia articles into paragraphs and encode them with a Bi-encoder(semantic models). If a new qeury/question is entered, it is encoded by the same bi-encoder and the paragraphs with the highest cosine-similarity are retrieved([Semantic Search](https://www.kaggle.com/code/aisuko/semantic-search)). Next, the retrieved candidates are scored by a Cross-Encoder re-ranker and the 5 passages with the highest score from the Cross-encoder are presented to the user.\n\n**Note: WE also compare the result to lexical search(keyword search). We use the BM25 algorithm which is implemented in the rank_bm25 package. Removing the code of it will be totally fine.**","metadata":{}},{"cell_type":"markdown","source":"## Retiever: bi-encoder\n\nFor semantic research, we use model `multi-qa-MiniLM-L6-cos-v1` and retrieve 32 potentially passages that answer the input query.","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers==2.3.1\n!pip install rank-bm25==0.2.2\n!pip install datasets==2.15.0","metadata":{"execution":{"iopub.status.busy":"2024-02-09T08:54:47.078715Z","iopub.execute_input":"2024-02-09T08:54:47.079485Z","iopub.status.idle":"2024-02-09T08:55:26.877228Z","shell.execute_reply.started":"2024-02-09T08:54:47.079451Z","shell.execute_reply":"2024-02-09T08:55:26.876076Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers==2.3.1\n  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (4.37.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (1.24.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (0.20.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.3.1) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.3.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.3.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==2.3.1) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.3.1) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.3.1) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.3.1) (0.4.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.3.1) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.3.1) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.3.1) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==2.3.1) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers==2.3.1) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers==2.3.1) (1.3.0)\nDownloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.3.1\nCollecting rank-bm25==0.2.2\n  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rank-bm25==0.2.2) (1.24.4)\nInstalling collected packages: rank-bm25\nSuccessfully installed rank-bm25-0.2.2\nCollecting datasets==2.15.0\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (1.24.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (11.0.0)\nCollecting pyarrow-hotfix (from datasets==2.15.0)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.15)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.15.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.15.0 fsspec-2023.10.0 pyarrow-hotfix-0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ['DATASET_NAME']='simplewiki-2020-11-01.jsonl.gz'\nos.environ['DATASET_URL']='http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz'\n\nos.environ['MODEL_NAME']='multi-qa-MiniLM-L6-cos-v1'\nos.environ['CROSS_CODE_NAME']='cross-encoder/ms-marco-MiniLM-L-6-v2'","metadata":{"execution":{"iopub.status.busy":"2024-02-09T08:55:26.879489Z","iopub.execute_input":"2024-02-09T08:55:26.879845Z","iopub.status.idle":"2024-02-09T08:55:26.886131Z","shell.execute_reply.started":"2024-02-09T08:55:26.879810Z","shell.execute_reply":"2024-02-09T08:55:26.884712Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nimport gzip\n\nfrom sentence_transformers.util import http_get\n\nhttp_get(os.getenv('DATASET_URL'), os.getenv('DATASET_NAME'))\n\npassages=[]\nwith gzip.open(os.getenv('DATASET_NAME'), 'rt', encoding='utf8') as fIn:\n    for line in fIn:\n        data=json.loads(line.strip())\n        # add all paragraphs\n#         passages.extend(data['paragraphs'])\n\n        # only add the first paragraph\n        passages.append(data['paragraphs'][0])\n\n#         for paragraph in data['paragraphs']:\n#             # We encode the passages as [title, text]\n#             passages.append([data['title'], paragraph])\n\nlen(passages)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T08:55:45.231372Z","iopub.execute_input":"2024-02-09T08:55:45.232341Z","iopub.status.idle":"2024-02-09T08:55:49.073228Z","shell.execute_reply.started":"2024-02-09T08:55:45.232306Z","shell.execute_reply":"2024-02-09T08:55:49.072187Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/50.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b00e120713a40129f3d86ab149d0d9d"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"169597"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading embeddings\n\nWe load the pre-processed embeddings of the simple_english_wikipedia. See [simple_english_wikipedia_p0](https://huggingface.co/datasets/aisuko/simple_english_wikipedia_p0).","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# The dataset is 1.48 GB, maybe it takes little bit longer than other resources.\nembedding_dataset=load_dataset('aisuko/simple_english_wikipedia_p0')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ncorpus_embeddings=torch.from_numpy(embedding_dataset['train'].to_pandas().to_numpy()).to(torch.float)\nlen(corpus_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Retiever: bi-encoder\n\nFor semantic research, we use model `multi-qa-MiniLM-L6-cos-v1` and retrieve 32 potentially passages that answer the input query.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nbi_encoder=SentenceTransformer(os.getenv('MODEL_NAME'))\nbi_encoder.max_seq_length=256\nbi_encoder.to('cuda')\nbi_encoder","metadata":{"execution":{"iopub.status.busy":"2024-02-09T08:59:40.980852Z","iopub.execute_input":"2024-02-09T08:59:40.981228Z","iopub.status.idle":"2024-02-09T09:00:02.430485Z","shell.execute_reply.started":"2024-02-09T08:59:40.981198Z","shell.execute_reply":"2024-02-09T09:00:02.429454Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97d6df42bce42619151db3ebc3c83b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5708329fe3be46148f7afd82c5d41e41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d9f43ece0e84522b79355cb848a0d59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6218a2e887e144ddbcf243736638f2d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/540 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd15a4f74c743b1ac933b0f3767a5ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f1744795bb458eac8e523baac9e727"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"627737a64e8f486dbc729d1dbbb30f79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d142f1d83bd54f11bba305d45d11d212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ba72f5ba0a4d978d6617af6b17d228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c234359d08bf4f70b8c1d0b6a1e41708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c409de7ef119477f94f9b7e10a8c0b65"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Re-Rank:Cross-Encoder\n\nWe use a more powerful CrossEncoder `cross-encoder/ms-macro-MiniLM-L-6-v2` that scores the query and all retrieved passages for their relevancy. The cross-encoder further boost the performance, especially when we search over a corpus for which the bi-encoder was not trained for.\n\nHere we use the re-rnk the results list to improve the quality.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\n\ncross_encoder=CrossEncoder(os.getenv('CROSS_CODE_NAME'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also compare the results to lexical seach (keyword search). Here, we use the BM25 algorithm which is implemented in the rank_bm25 package.","metadata":{}}]}