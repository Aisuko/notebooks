{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn the notebooks [Semantic Textual Similarity](https://www.kaggle.com/code/aisuko/semantic-textual-similarity), [Semantic Search](https://www.kaggle.com/code/aisuko/semantic-search), [Similar Questions Retrieval](https://www.kaggle.com/code/aisuko/similar-questions-retrieval), [Semantic Search in Publications](https://www.kaggle.com/code/aisuko/semantic-search-in-publications), [Wikipedia Q&A Retrieval Semantic Search](https://www.kaggle.com/code/aisuko/wikipedia-q-a-retrieval-semantic-search). We have shown how to compute the embeddings of queries, sentences, paragraphs, articles, and how to use semantic search function.\n\nIn this notebook, we are going to show a complex search tasks, for example, for question answering retrieval, the search can significantly be improved by using **Retrieve & Re_Rank**.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Retrieve & Re-Rank Pipeline\n\nA pipeline for information retrieval / question answering retrieval that works well is the following. All components are provided and explained in here:\n\n<div style=\"text-align: center\"><img src=\"https://hostux.social/system/media_attachments/files/111/898/410/559/698/272/original/c090c2ca83c51a40.png\" width=\"100%\" heigh=\"100%\" alt=\"Retrieve&Re-Rank pipeline\"></div>\n\n\nFrom the picture above. For a query, we first use a **retrieval system** that retrieves a large list of e.g. 100 possible hits which are potentially relevant for the query. For the retrieval, we can use either lexical search, e.g, with Elasticsearch, or we can use dense retrieval with a semantic model.\n\nHowever, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, we use a **re-ranker** based on a **cross-encoder** that scores that relevancy of all candidates for the given search query.\n\nThe output will be a ranked list of hits we can present to the user.","metadata":{}},{"cell_type":"markdown","source":"## Retrieval System: Bi-Encoder\n\nFor the retrieval of the candidate set, we can either use lexical search (e.g. Elasticsearch), or we can use a semantic model. However, lexical search looks for literal matches of the query words in our document collection. It will not recognize synonyms, acronyms or spelling variations. In contrast, semantic search (or dense retrieval) encodes the search query into vector space and retrieves the document embeddings that are close in vector space.\n\n<div style=\"text-align: center\"><img src=\"https://hostux.social/system/media_attachments/files/111/888/433/770/763/125/original/dafea983f4905b4b.png\" width=\"60%\" heigh=\"60%\" alt=\"Semantic Search\"></div>\n\nHave a look at the notebook [Semantic Search](https://www.kaggle.com/code/aisuko/semantic-search) to get more detail.","metadata":{}},{"cell_type":"markdown","source":"## Re-Rank: Cross-Encoder\n\nThe retriever has to be efficient for large document collections with millions of entries. However, it might return irrelevant candidates.\n\nA re-ranker based on a Cross-Encoder can substantially improve the final results for the user. The query and a possible document is passed simultaneously to transformer network, which when outputs a single score between 0 and 1 indicating how relevant the document is for the given query.\n\n<div style=\"text-align: center\"><img src=\"https://hostux.social/system/media_attachments/files/111/898/502/437/707/103/original/cd1bb78c46020897.png\" width=\"60%\" heigh=\"60%\" alt=\"Re-Rank:Cross-Encoder\"></div>\n\nThe advantage of Cross-Encoders is the higher performance, as they perform attention across the query and the document.\n\nScoring thousands or millions of (query, document)-pairs would be rather slow. Hence, we use the retriever to create a set of e.g. 100 possible candidates which are then re-ranked by the Cross-Encoder.","metadata":{}},{"cell_type":"markdown","source":"# Implementation of Retrieve Rerank\n\nLet's use the smaller Simple English Wikipedia(it is fits better in RAM) as document collection to provide answers to user questions/ search queries. First, we split all Wikipedia articles into paragraphs and encode them with a Bi-encoder(semantic models). If a new qeury/question is entered, it is encoded by the same bi-encoder and the paragraphs with the highest cosine-similarity are retrieved([Semantic Search](https://www.kaggle.com/code/aisuko/semantic-search)). Next, the retrieved candidates are scored by a Cross-Encoder re-ranker and the 5 passages with the highest score from the Cross-encoder are presented to the user.\n\n**Note: WE also compare the result to lexical search(keyword search). We use the BM25 algorithm which is implemented in the rank_bm25 package. Removing the code of it will be totally fine.**","metadata":{}},{"cell_type":"markdown","source":"## Retiever: bi-encoder\n\nFor semantic research, we use model `multi-qa-MiniLM-L6-cos-v1` and retrieve 32 potentially passages that answer the input query.","metadata":{}},{"cell_type":"code","source":"!pip install sentence-transformers==2.3.1\n!pip install rank-bm25==0.2.2","metadata":{"execution":{"iopub.status.busy":"2024-02-09T00:39:08.362359Z","iopub.execute_input":"2024-02-09T00:39:08.363019Z","iopub.status.idle":"2024-02-09T00:39:21.896343Z","shell.execute_reply.started":"2024-02-09T00:39:08.362991Z","shell.execute_reply":"2024-02-09T00:39:21.895342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['DATASET_NAME']='simplewiki-2020-11-01.jsonl.gz'\nos.environ['DATASET_URL']='http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz'\n\nos.environ['MODEL_NAME']='multi-qa-MiniLM-L6-cos-v1'\nos.environ['CROSS_CODE_NAME']='cross-encoder/ms-marco-MiniLM-L-6-v2'","metadata":{"execution":{"iopub.status.busy":"2024-02-09T00:40:08.583255Z","iopub.execute_input":"2024-02-09T00:40:08.583917Z","iopub.status.idle":"2024-02-09T00:40:08.588574Z","shell.execute_reply.started":"2024-02-09T00:40:08.583886Z","shell.execute_reply":"2024-02-09T00:40:08.587643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Re-Rank:Cross-Encoder\n\nWe use a more powerful CrossEncoder `cross-encoder/ms-macro-MiniLM-L-6-v2` that scores the query and all retrieved passages for their relevancy. The cross-encoder further boost the performance, especially when we search over a corpus for which the bi-encoder was not trained for.","metadata":{}},{"cell_type":"code","source":"import json\nimport gzip\n\nfrom sentence_transformers.util import http_get\n\nhttp_get(os.getenv('DATASET_URL'), os.getenv('DATASET_NAME'))\n\npassages=[]\nwith gzip.open(os.getenv('DATASET_NAME'), 'rt', encoding='utf8') as fIn:\n    for line in fIn:\n        data=json.loads(line.strip())\n        # add all paragraphs\n#         passages.extend(data['paragraphs'])\n\n        # only add the first paragraph\n        passages.append(data['paragraph'][0])\n\n#         for paragraph in data['paragraphs']:\n#             # We encode the passages as [title, text]\n#             passages.append([data['title'], paragraph])\n\nlen(passages)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T00:40:14.660187Z","iopub.execute_input":"2024-02-09T00:40:14.661123Z","iopub.status.idle":"2024-02-09T00:40:30.402098Z","shell.execute_reply.started":"2024-02-09T00:40:14.661064Z","shell.execute_reply":"2024-02-09T00:40:30.401100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the model(bi_encoder)","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import normalize_embeddings\n\nbi_encoder=SentenceTransformer('nq-distilbert-base-v1')\nbi_encoder.max_seq_length=256\nbi_encoder.to('cuda')\nbi_encoder","metadata":{"execution":{"iopub.status.busy":"2024-02-09T00:40:44.237338Z","iopub.execute_input":"2024-02-09T00:40:44.238097Z","iopub.status.idle":"2024-02-09T00:40:50.096654Z","shell.execute_reply.started":"2024-02-09T00:40:44.238053Z","shell.execute_reply":"2024-02-09T00:40:50.095783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_embeddings=bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)","metadata":{},"execution_count":null,"outputs":[]}]}