{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/som-conduct-rag-s-semantic-search?scriptVersionId=168241424\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we are going to use SOM to conduct RAG's semantic search to agument the context for question and answer using LLM. More detail of SOM concept see [Using SOM algorithm to Bolster RAG](https://open.substack.com/pub/aisuko/p/using-som-algorithm-to-bolster-rag?r=fbe14&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\n\n# Implement SOM","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Original address see the acknowledge section\n# Adapter: Aisuko\n\nimport torch\nimport torch.nn as nn\nfrom tqdm.autonotebook import tqdm\nfrom typing import List\n\nimport warnings\n\nwarnings.simplefilter(\"ignore\")\n\nclass KohonenSOM():\n    def __init__(self, input_dimensions: int, som_lattice_heigh: int=20, som_lattice_width: int=20, learning_rate:float=0.3, neighborhood_radius: float=None, device:str='cuda'):\n        self.input_dimensions=int(input_dimensions)\n        self.som_lattice_height=int(som_lattice_height)\n        self.som_lattice_width=int(som_lattice_width)\n        \n        if learning_rate==None:\n            self.learning_rate=0.3\n        else:\n            self.learning_rate=float(learning_rate)\n        \n        if neighborhood_radius==None:\n            self.neighborhood_radius=max(self.som_lattice_height, self.som_lattice_width)/2.0\n        else:\n            self.neighborhood_radius=float(neighborhood_radius)\n            \n        self.device=torch.device('cuda')\n        \n        def dist_eval(data_points, weights):\n            distances=torch.cdist(data_points, weights, p=2)\n            return distances\n        \n        self.dist_evaluator=dist_eval\n        self.total_lattice_nodes=self.som_lattice_height*self.some_lattice_width\n        self.lattice_node_weights=torch.randn(self.total_lattice_nodes, self.input_dimensions, device=self.device)\n        lattice_coordinates=torch.tensor([ [[i,j] for j in range(self.som_lattice_width)] for i in range(self.som_lattice_height)], dtype=torch.int)\n        self.lattice_coordinates=lattice_coordinates.view(self.total_lattice_nodes,2)\n        self.trained=False\n    \n    def train(self, data_points: torch.Tensor, train_epochs: int=100):\n        pass\n    \n    def find_best_matching_unit(self, data_points: torch.Tensor) -> List[List[int]]:\n        pass\n    \n    def find_topk_best_matching_units(self, data_points: torch:Tensor, topk:int=1)->List[List[int]]:\n        pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c\n* https://github.com/kbmurali/som-driven-qa-rag/blob/main/kohonen_som.py\n* https://open.substack.com/pub/aisuko/p/using-som-algorithm-to-bolster-rag?r=fbe14&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true","metadata":{}}]}