{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decfcbb8",
   "metadata": {
    "papermill": {
     "duration": 0.003037,
     "end_time": "2024-02-15T01:10:10.498410",
     "exception": false,
     "start_time": "2024-02-15T01:10:10.495373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, let's familiar some (lower-level)tools are mentioned in the previous nobooks. This notebook can be a appendix for the previous notebooks which with higher level techniques. We will follow the timeline of evolution of embeddings. And all the images are from the credit section at the bottom.\n",
    "\n",
    "\n",
    "# Bag of Words\n",
    "\n",
    "Note: This approach is quite basic, and it doesn't take into account the semantic meaning of the words.\n",
    "\n",
    "It is a basic approach to converting texts into vectors. The first step to get a bag of words vector is to split the text into tokens and then reduce words to their base forms. For example, \"running\" will transform into \"run\". This process is called `stremming`. We use `NLTK` for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6ac612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T01:10:10.504646Z",
     "iopub.status.busy": "2024-02-15T01:10:10.503880Z",
     "iopub.status.idle": "2024-02-15T01:10:11.985716Z",
     "shell.execute_reply": "2024-02-15T01:10:11.984821Z"
    },
    "papermill": {
     "duration": 1.486616,
     "end_time": "2024-02-15T01:10:11.987291",
     "exception": false,
     "start_time": "2024-02-15T01:10:10.500675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Enjoy', 'a', 'beautiful', 'mostly', 'sunny', 'day', 'in', 'Melbourne', '!', 'It', \"'s\", 'currently', 'a', 'pleasant', '18°C', 'with', 'a', 'light', 'southerly', 'breeze', ',', 'but', 'make', 'sure', 'to', 'slip', 'on', 'some', 'sunscreen', 'as', 'the', 'UV', 'index', 'climbs', 'to', 'a', 'very', 'high', '10', 'later', 'today', '.', 'Expect', 'a', 'high', 'of', '21°C', ',', 'perfect', 'for', 'outdoor', 'activities', ',', 'but', 'bundle', 'up', 'a', 'bit', 'for', 'the', '13°C', 'low', 'tonight', '.', 'With', 'only', 'a', 'very', 'slight', 'chance', 'of', 'rain', ',', 'it', \"'s\", 'a', 'fantastic', 'day', 'to', 'get', 'out', 'and', 'explore', 'the', 'city', '!']\n",
      "====================================================================================================\n",
      "['enjoy', 'a', 'beauti', 'most', 'sunni', 'day', 'in', 'melbourn', '!', 'it', \"'s\", 'current', 'a', 'pleasant', '18°c', 'with', 'a', 'light', 'souther', 'breez', ',', 'but', 'make', 'sure', 'to', 'slip', 'on', 'some', 'sunscreen', 'as', 'the', 'uv', 'index', 'climb', 'to', 'a', 'veri', 'high', '10', 'later', 'today', '.', 'expect', 'a', 'high', 'of', '21°c', ',', 'perfect', 'for', 'outdoor', 'activ', ',', 'but', 'bundl', 'up', 'a', 'bit', 'for', 'the', '13°c', 'low', 'tonight', '.', 'with', 'onli', 'a', 'veri', 'slight', 'chanc', 'of', 'rain', ',', 'it', \"'s\", 'a', 'fantast', 'day', 'to', 'get', 'out', 'and', 'explor', 'the', 'citi', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "weather_of_melbourne=\"Enjoy a beautiful mostly sunny day in Melbourne! It's currently a pleasant 18°C with a light southerly breeze, but make sure to slip on some sunscreen as the UV index climbs to a very high 10 later today. Expect a high of 21°C, perfect for outdoor activities, but bundle up a bit for the 13°C low tonight. With only a very slight chance of rain, it's a fantastic day to get out and explore the city!\"\n",
    "\n",
    "# tokenization - splitting text into words\n",
    "words=word_tokenize(weather_of_melbourne)\n",
    "print(words)\n",
    "\n",
    "print(100*'=')\n",
    "#steamming\n",
    "stemmer=SnowballStemmer(language='english')\n",
    "stemmed_words=list(map(lambda x: stemmer.stem(x), words))\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448b07c",
   "metadata": {
    "papermill": {
     "duration": 0.001963,
     "end_time": "2024-02-15T01:10:11.991556",
     "exception": false,
     "start_time": "2024-02-15T01:10:11.989593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Calculating the frequencies of words\n",
    "\n",
    "> Note: In the real word case, we need to have a vocabulary to cover the whole words to create a vector\n",
    "\n",
    "We calculate their frequences to create a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91338a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T01:10:11.997123Z",
     "iopub.status.busy": "2024-02-15T01:10:11.996666Z",
     "iopub.status.idle": "2024-02-15T01:10:12.000392Z",
     "shell.execute_reply": "2024-02-15T01:10:11.999666Z"
    },
    "papermill": {
     "duration": 0.008149,
     "end_time": "2024-02-15T01:10:12.001784",
     "exception": false,
     "start_time": "2024-02-15T01:10:11.993635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 8, ',': 4, 'to': 3, 'the': 3, 'day': 2, '!': 2, 'it': 2, \"'s\": 2, 'with': 2, 'but': 2, 'veri': 2, 'high': 2, '.': 2, 'of': 2, 'for': 2, 'enjoy': 1, 'beauti': 1, 'most': 1, 'sunni': 1, 'in': 1, 'melbourn': 1, 'current': 1, 'pleasant': 1, '18°c': 1, 'light': 1, 'souther': 1, 'breez': 1, 'make': 1, 'sure': 1, 'slip': 1, 'on': 1, 'some': 1, 'sunscreen': 1, 'as': 1, 'uv': 1, 'index': 1, 'climb': 1, '10': 1, 'later': 1, 'today': 1, 'expect': 1, '21°c': 1, 'perfect': 1, 'outdoor': 1, 'activ': 1, 'bundl': 1, 'up': 1, 'bit': 1, '13°c': 1, 'low': 1, 'tonight': 1, 'onli': 1, 'slight': 1, 'chanc': 1, 'rain': 1, 'fantast': 1, 'get': 1, 'out': 1, 'and': 1, 'explor': 1, 'citi': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "bag_of_words=collections.Counter(stemmed_words)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8652547",
   "metadata": {
    "papermill": {
     "duration": 0.002076,
     "end_time": "2024-02-15T01:10:12.006052",
     "exception": false,
     "start_time": "2024-02-15T01:10:12.003976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "It is a slightly improved version of the bag of the words approach. It stands for **Term Frequency-Inverse Document Frequency**. It's the multiplication of two metrics.\n",
    "\n",
    "$$TF-IDF(t,d,D)=TF(t,d)*IDF(t,D)$$\n",
    "\n",
    "**Term Frequency** shows the frequency of the word in the document. The most common way to calculate it is to divide the raw count of the term($n_t$) in this document(like in the bag of words) by the total number of terms(words)($d$) in the document. However, there are many other approches like just raw count, boolean \"frequencies\", and different approaches to normalisation. See more on [Wikipedia](https://en.wikipedia.org/wiki/Tf–idf)\n",
    "\n",
    "$$TF(t,d)=\\frac{n_t}{d}$$\n",
    "\n",
    "\n",
    "**Inverse Document Frequency** denotes how much information the word procides. For example, the words 'a' or 'that' don't give you any additional information about the document's topic. In contrast, words like `ChatGPT` or `bioinfomatics` can help you define the domain (but not for this sentence). It's calculated as the logarithm of the ratio of the total number of documents to those containing the word. The closer IDF is to 0 - the more common the word is and the less information it provides. \n",
    "\n",
    "* D-> total number of documents in corpus D\n",
    "* -> number of documents containing term t\n",
    "\n",
    "$$IDF(t,D)=log(\\frac{D}{t})$$\n",
    "\n",
    "\n",
    "As we can tell that the common words will have low weights, while rare words that occur in the document multiple times will have higher weights. This strategy will give a bit better results, but it sill can't capture semantic meaning. Moreover, it produces pretty sparse vectors. The length of vectos is equal to the corpus size. There are about 470k unique words in English. so we will have huge vectors. Since the sentence won't have more than 50 unique words, 99.89% of the values in vectors will be 0, not encoding any info. Looking at this, scientists started to think about dense vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ed9cf",
   "metadata": {
    "papermill": {
     "duration": 0.001968,
     "end_time": "2024-02-15T01:10:12.010138",
     "exception": false,
     "start_time": "2024-02-15T01:10:12.008170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "It is the famous approaches to dense representation. There are two different word2vec approaches mentioned in the paper:\n",
    "* Continuous Bag of Words- prediction the word based on the surrounding words\n",
    "* Skip-gram - the opposite task - when we predict context based on the word\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/932/699/788/269/102/small/cf5b60614ad0d60b.webp)\n",
    "\n",
    "\n",
    "The high-level idea of sense vector representration is to train two models: encoder and decoder. For example, in the case of skip-gram, we might pass the word \"chrismas\" to the encoder. Then, the encoder will produce a vector that we pass to the decoder expecting to get the words \"merry\",\"to\" and \"you\". This model started to take into account the meaning of the words since it's trained on the context of the words. However, it ignores morphology(information we can get from the word parts, for example, that \"less\" means the lack of something). This drawback was addressed later by looking at subword skip-grams in GLove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52536b",
   "metadata": {
    "papermill": {
     "duration": 0.001994,
     "end_time": "2024-02-15T01:10:12.014240",
     "exception": false,
     "start_time": "2024-02-15T01:10:12.012246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transfromers and Sentence Embeddings\n",
    "\n",
    "See the detail from the previosuly notebooks.\n",
    "\n",
    "## Transformers\n",
    "\n",
    "* [Encoder of Transformers](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture)\n",
    "* [Decoder of Transformers](https://www.kaggle.com/code/aisuko/decoder-in-transformers-architecture)\n",
    "\n",
    "\n",
    "## Sentence Embeddings\n",
    "\n",
    "* [Computing sentence embeddings with multiple GPUs](https://www.kaggle.com/code/aisuko/computing-embeddings-with-multi-gpus)\n",
    "* [Computing sentence embeddings with streaming](https://www.kaggle.com/code/aisuko/computing-embeddings-streaming)\n",
    "* [Computing sentence embeddings with Transformers](https://www.kaggle.com/code/aisuko/sentence-embeddings-with-transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41e72e",
   "metadata": {
    "papermill": {
     "duration": 0.00197,
     "end_time": "2024-02-15T01:10:12.018361",
     "exception": false,
     "start_time": "2024-02-15T01:10:12.016391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Calculating embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18283c93",
   "metadata": {
    "papermill": {
     "duration": 0.002041,
     "end_time": "2024-02-15T01:10:12.022537",
     "exception": false,
     "start_time": "2024-02-15T01:10:12.020496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credit\n",
    "\n",
    "* https://medium.com/towards-data-science/text-embeddings-comprehensive-guide-afd97fce8fb5\n",
    "* https://arxiv.org/pdf/1301.3781.pdf"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.866345,
   "end_time": "2024-02-15T01:10:12.341808",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-15T01:10:08.475463",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
