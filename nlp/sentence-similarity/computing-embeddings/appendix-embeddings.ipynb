{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f890a41",
   "metadata": {
    "papermill": {
     "duration": 0.006844,
     "end_time": "2024-02-15T07:30:58.315184",
     "exception": false,
     "start_time": "2024-02-15T07:30:58.308340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, let's familiar some (lower-level)tools are mentioned in the previous nobooks. This notebook can be a appendix for the previous notebooks which with higher level techniques. We will follow the timeline of evolution of embeddings. And all the images are from the credit section at the bottom.\n",
    "\n",
    "\n",
    "# Bag of Words\n",
    "\n",
    "Note: This approach is quite basic, and it doesn't take into account the semantic meaning of the words.\n",
    "\n",
    "It is a basic approach to converting texts into vectors. The first step to get a bag of words vector is to split the text into tokens and then reduce words to their base forms. For example, \"running\" will transform into \"run\". This process is called `stremming`. We use `NLTK` for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f09b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:30:58.329963Z",
     "iopub.status.busy": "2024-02-15T07:30:58.329519Z",
     "iopub.status.idle": "2024-02-15T07:31:00.747672Z",
     "shell.execute_reply": "2024-02-15T07:31:00.746115Z"
    },
    "papermill": {
     "duration": 2.429078,
     "end_time": "2024-02-15T07:31:00.750920",
     "exception": false,
     "start_time": "2024-02-15T07:30:58.321842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Enjoy', 'a', 'beautiful', 'mostly', 'sunny', 'day', 'in', 'Melbourne', '!', 'It', \"'s\", 'currently', 'a', 'pleasant', '18°C', 'with', 'a', 'light', 'southerly', 'breeze', ',', 'but', 'make', 'sure', 'to', 'slip', 'on', 'some', 'sunscreen', 'as', 'the', 'UV', 'index', 'climbs', 'to', 'a', 'very', 'high', '10', 'later', 'today', '.', 'Expect', 'a', 'high', 'of', '21°C', ',', 'perfect', 'for', 'outdoor', 'activities', ',', 'but', 'bundle', 'up', 'a', 'bit', 'for', 'the', '13°C', 'low', 'tonight', '.', 'With', 'only', 'a', 'very', 'slight', 'chance', 'of', 'rain', ',', 'it', \"'s\", 'a', 'fantastic', 'day', 'to', 'get', 'out', 'and', 'explore', 'the', 'city', '!']\n",
      "====================================================================================================\n",
      "['enjoy', 'a', 'beauti', 'most', 'sunni', 'day', 'in', 'melbourn', '!', 'it', \"'s\", 'current', 'a', 'pleasant', '18°c', 'with', 'a', 'light', 'souther', 'breez', ',', 'but', 'make', 'sure', 'to', 'slip', 'on', 'some', 'sunscreen', 'as', 'the', 'uv', 'index', 'climb', 'to', 'a', 'veri', 'high', '10', 'later', 'today', '.', 'expect', 'a', 'high', 'of', '21°c', ',', 'perfect', 'for', 'outdoor', 'activ', ',', 'but', 'bundl', 'up', 'a', 'bit', 'for', 'the', '13°c', 'low', 'tonight', '.', 'with', 'onli', 'a', 'veri', 'slight', 'chanc', 'of', 'rain', ',', 'it', \"'s\", 'a', 'fantast', 'day', 'to', 'get', 'out', 'and', 'explor', 'the', 'citi', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "weather_of_melbourne=\"Enjoy a beautiful mostly sunny day in Melbourne! It's currently a pleasant 18°C with a light southerly breeze, but make sure to slip on some sunscreen as the UV index climbs to a very high 10 later today. Expect a high of 21°C, perfect for outdoor activities, but bundle up a bit for the 13°C low tonight. With only a very slight chance of rain, it's a fantastic day to get out and explore the city!\"\n",
    "\n",
    "# tokenization - splitting text into words\n",
    "words=word_tokenize(weather_of_melbourne)\n",
    "print(words)\n",
    "\n",
    "print(100*'=')\n",
    "#steamming\n",
    "stemmer=SnowballStemmer(language='english')\n",
    "stemmed_words=list(map(lambda x: stemmer.stem(x), words))\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fb637",
   "metadata": {
    "papermill": {
     "duration": 0.009228,
     "end_time": "2024-02-15T07:31:00.769569",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.760341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Calculating the frequencies of words\n",
    "\n",
    "> Note: In the real word case, we need to have a vocabulary to cover the whole words to create a vector\n",
    "\n",
    "We calculate their frequences to create a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac75ef45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:31:00.785170Z",
     "iopub.status.busy": "2024-02-15T07:31:00.784287Z",
     "iopub.status.idle": "2024-02-15T07:31:00.791046Z",
     "shell.execute_reply": "2024-02-15T07:31:00.789832Z"
    },
    "papermill": {
     "duration": 0.017998,
     "end_time": "2024-02-15T07:31:00.794333",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.776335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 8, ',': 4, 'to': 3, 'the': 3, 'day': 2, '!': 2, 'it': 2, \"'s\": 2, 'with': 2, 'but': 2, 'veri': 2, 'high': 2, '.': 2, 'of': 2, 'for': 2, 'enjoy': 1, 'beauti': 1, 'most': 1, 'sunni': 1, 'in': 1, 'melbourn': 1, 'current': 1, 'pleasant': 1, '18°c': 1, 'light': 1, 'souther': 1, 'breez': 1, 'make': 1, 'sure': 1, 'slip': 1, 'on': 1, 'some': 1, 'sunscreen': 1, 'as': 1, 'uv': 1, 'index': 1, 'climb': 1, '10': 1, 'later': 1, 'today': 1, 'expect': 1, '21°c': 1, 'perfect': 1, 'outdoor': 1, 'activ': 1, 'bundl': 1, 'up': 1, 'bit': 1, '13°c': 1, 'low': 1, 'tonight': 1, 'onli': 1, 'slight': 1, 'chanc': 1, 'rain': 1, 'fantast': 1, 'get': 1, 'out': 1, 'and': 1, 'explor': 1, 'citi': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "bag_of_words=collections.Counter(stemmed_words)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0b7a3",
   "metadata": {
    "papermill": {
     "duration": 0.006959,
     "end_time": "2024-02-15T07:31:00.808016",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.801057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "It is a slightly improved version of the bag of the words approach. It stands for **Term Frequency-Inverse Document Frequency**. It's the multiplication of two metrics.\n",
    "\n",
    "$$TF-IDF(t,d,D)=TF(t,d)*IDF(t,D)$$\n",
    "\n",
    "**Term Frequency** shows the frequency of the word in the document. The most common way to calculate it is to divide the raw count of the term($n_t$) in this document(like in the bag of words) by the total number of terms(words)($d$) in the document. However, there are many other approches like just raw count, boolean \"frequencies\", and different approaches to normalisation. See more on [Wikipedia](https://en.wikipedia.org/wiki/Tf–idf)\n",
    "\n",
    "$$TF(t,d)=\\frac{n_t}{d}$$\n",
    "\n",
    "\n",
    "**Inverse Document Frequency** denotes how much information the word procides. For example, the words 'a' or 'that' don't give you any additional information about the document's topic. In contrast, words like `ChatGPT` or `bioinfomatics` can help you define the domain (but not for this sentence). It's calculated as the logarithm of the ratio of the total number of documents to those containing the word. The closer IDF is to 0 - the more common the word is and the less information it provides. \n",
    "\n",
    "* D-> total number of documents in corpus D\n",
    "* -> number of documents containing term t\n",
    "\n",
    "$$IDF(t,D)=log(\\frac{D}{t})$$\n",
    "\n",
    "\n",
    "As we can tell that the common words will have low weights, while rare words that occur in the document multiple times will have higher weights. This strategy will give a bit better results, but it sill can't capture semantic meaning. Moreover, it produces pretty sparse vectors. The length of vectos is equal to the corpus size. There are about 470k unique words in English. so we will have huge vectors. Since the sentence won't have more than 50 unique words, 99.89% of the values in vectors will be 0, not encoding any info. Looking at this, scientists started to think about dense vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ee23c",
   "metadata": {
    "papermill": {
     "duration": 0.006631,
     "end_time": "2024-02-15T07:31:00.821458",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.814827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "It is the famous approaches to dense representation. There are two different word2vec approaches mentioned in the paper:\n",
    "* Continuous Bag of Words- prediction the word based on the surrounding words\n",
    "* Skip-gram - the opposite task - when we predict context based on the word\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/932/699/788/269/102/small/cf5b60614ad0d60b.webp)\n",
    "\n",
    "\n",
    "The high-level idea of sense vector representration is to train two models: encoder and decoder. For example, in the case of skip-gram, we might pass the word \"chrismas\" to the encoder. Then, the encoder will produce a vector that we pass to the decoder expecting to get the words \"merry\",\"to\" and \"you\". This model started to take into account the meaning of the words since it's trained on the context of the words. However, it ignores morphology(information we can get from the word parts, for example, that \"less\" means the lack of something). This drawback was addressed later by looking at subword skip-grams in GLove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce960633",
   "metadata": {
    "papermill": {
     "duration": 0.006496,
     "end_time": "2024-02-15T07:31:00.834748",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.828252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transfromers and Sentence Embeddings\n",
    "\n",
    "See the detail from the previosuly notebooks.\n",
    "\n",
    "## Transformers\n",
    "\n",
    "* [Encoder of Transformers](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture)\n",
    "* [Decoder of Transformers](https://www.kaggle.com/code/aisuko/decoder-in-transformers-architecture)\n",
    "\n",
    "\n",
    "## Sentence Embeddings\n",
    "\n",
    "* [Computing sentence embeddings with multiple GPUs](https://www.kaggle.com/code/aisuko/computing-embeddings-with-multi-gpus)\n",
    "* [Computing sentence embeddings with streaming](https://www.kaggle.com/code/aisuko/computing-embeddings-streaming)\n",
    "* [Computing sentence embeddings with Transformers](https://www.kaggle.com/code/aisuko/sentence-embeddings-with-transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992a547",
   "metadata": {
    "papermill": {
     "duration": 0.006297,
     "end_time": "2024-02-15T07:31:00.847661",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.841364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Distance between vectors\n",
    "\n",
    "Embeddings are actually vectors. So, if we want to unerstand how close two sentences are to each other, we can calcuate the distance between vectors. A smaller distance would be equivalent to a closer semantic meaning.\n",
    "\n",
    "Different metrics can be used to measure the distance between two vectors:\n",
    "\n",
    "* Euclidean distance (L2)\n",
    "* Manhattant distance (L1)\n",
    "* Dot product\n",
    "* Cosine distance\n",
    "\n",
    "Let's discuss them. As a simple exapmle, we will be using two 2D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484770b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:31:00.862747Z",
     "iopub.status.busy": "2024-02-15T07:31:00.862316Z",
     "iopub.status.idle": "2024-02-15T07:31:00.867570Z",
     "shell.execute_reply": "2024-02-15T07:31:00.866321Z"
    },
    "papermill": {
     "duration": 0.016177,
     "end_time": "2024-02-15T07:31:00.870388",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.854211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector1=[1,4]\n",
    "vector2=[2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac3da8",
   "metadata": {
    "papermill": {
     "duration": 0.007453,
     "end_time": "2024-02-15T07:31:00.887550",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.880097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Euclidean distance(L2)\n",
    "\n",
    "The most standard way to define distance between two points(or vectors) is Euclidean distance or L2 norm. This metric is the most commonly used in day-to-day life, for example, when we are talking about the distance between 2 towns. Here is a visual representation and formula for L2 distance.\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/933/848/504/549/053/original/1045050091437874.webp)\n",
    "\n",
    "We can calculate this metric using vanilla Python or leveraging the numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aad621a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:31:00.903988Z",
     "iopub.status.busy": "2024-02-15T07:31:00.902716Z",
     "iopub.status.idle": "2024-02-15T07:31:00.913176Z",
     "shell.execute_reply": "2024-02-15T07:31:00.911995Z"
    },
    "papermill": {
     "duration": 0.021112,
     "end_time": "2024-02-15T07:31:00.915391",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.894279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n",
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(sum(list(map(lambda x,y:(x-y)**2, vector1, vector2)))**0.5)\n",
    "\n",
    "print(np.linalg.norm((np.array(vector1)-np.array(vector2)), ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec2c13",
   "metadata": {
    "papermill": {
     "duration": 0.006516,
     "end_time": "2024-02-15T07:31:00.928858",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.922342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manhattant distance(L1)\n",
    "\n",
    "The other commonly used distance is the L1 norm or Manhattan distance. This distance was called after the island of Manhattan(New York). This island has a grid layout of streets, and the shortest routes between two points in Manhattan will be L1 distance since you need to follow the grid.\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/933/886/084/041/180/original/504b3c9fcda74345.webp)\n",
    "\n",
    "We can also implement it from scratch or use the numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a873578a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:31:00.945463Z",
     "iopub.status.busy": "2024-02-15T07:31:00.944026Z",
     "iopub.status.idle": "2024-02-15T07:31:00.952545Z",
     "shell.execute_reply": "2024-02-15T07:31:00.951418Z"
    },
    "papermill": {
     "duration": 0.019233,
     "end_time": "2024-02-15T07:31:00.954852",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.935619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(list(map(lambda x,y:abs(x-y), vector1, vector2))))\n",
    "      \n",
    "print(np.linalg.norm((np.array(vector1)-np.array(vector2)), ord=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b8c4e",
   "metadata": {
    "papermill": {
     "duration": 0.006726,
     "end_time": "2024-02-15T07:31:00.968692",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.961966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dot product\n",
    "\n",
    "> Note: This metric is a bot tricky to interpret. On the one hand, it shows you whether vectors are pointing in one direction. On the other hand, the results highly depend on the magnitudes of the vectors.\n",
    "\n",
    "Another way to look at the distance between vectors is to calculate a dot or scalar product. Here's formula and we can easily implement it.\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/933/930/157/590/756/original/3208368343fcbecb.webp)\n",
    "\n",
    "As we mentioned above, the results highly depends on the magnitudes of the vectors. For example, if we calculate the dot products between two pairs of vectors:\n",
    "\n",
    "* (1,1) vs (1,1)\n",
    "* (1,1) vs (10,10)\n",
    "\n",
    "In both cases, vectors are **collinear**, but the dot product is ten times bigger in the second case: 2 vs 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200f57a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:31:00.985223Z",
     "iopub.status.busy": "2024-02-15T07:31:00.984237Z",
     "iopub.status.idle": "2024-02-15T07:31:00.992364Z",
     "shell.execute_reply": "2024-02-15T07:31:00.991358Z"
    },
    "papermill": {
     "duration": 0.020137,
     "end_time": "2024-02-15T07:31:00.995804",
     "exception": false,
     "start_time": "2024-02-15T07:31:00.975667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(sum(list(map(lambda x,y:x*y, vector1, vector2))))\n",
    "\n",
    "print(np.dot(vector1, vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b04b9",
   "metadata": {
    "papermill": {
     "duration": 0.007976,
     "end_time": "2024-02-15T07:31:01.010986",
     "exception": false,
     "start_time": "2024-02-15T07:31:01.003010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Quite often, cosine similarity is used. Cosine similarity is a dot product normalised by vectors' magnitudes(or normes).\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/933/983/599/203/267/original/a4aa2175b2a0a759.webp)\n",
    "\n",
    "We can either calculate everything ourselves(as previously) or use the function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c643b585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:31:01.027867Z",
     "iopub.status.busy": "2024-02-15T07:31:01.026987Z",
     "iopub.status.idle": "2024-02-15T07:31:01.043494Z",
     "shell.execute_reply": "2024-02-15T07:31:01.042651Z"
    },
    "papermill": {
     "duration": 0.02808,
     "end_time": "2024-02-15T07:31:01.046192",
     "exception": false,
     "start_time": "2024-02-15T07:31:01.018112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8574929257125442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8574929257125441"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dot_product=sum(list(map(lambda x,y:x*y, vector1, vector2)))\n",
    "norm_vector1=sum(list(map(lambda x:x**2, vector1)))**0.5\n",
    "norm_vector2=sum(list(map(lambda x:x**2, vector2)))**0.5\n",
    "\n",
    "print(dot_product/norm_vector1/norm_vector2)\n",
    "\n",
    "cosine_similarity(\n",
    "    np.array(vector1).reshape(1,-1),\n",
    "    np.array(vector2).reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae5b04",
   "metadata": {
    "papermill": {
     "duration": 0.010134,
     "end_time": "2024-02-15T07:31:01.066853",
     "exception": false,
     "start_time": "2024-02-15T07:31:01.056719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The function `cosine_similarity` expects 2D arrays. That's why we need to reshape the numpy arrays. Cosine similarity is equal to the cosine between two vectors. The closer the vectors are, the higher the metric value.\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/934/012/642/244/598/original/40ec654df0850059.webp)\n",
    "\n",
    "We can even calculate the exact angle between our vectors in degrees. We get result around 30 degreees, and it looks pretty reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4ca4a7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T07:31:01.090218Z",
     "iopub.status.busy": "2024-02-15T07:31:01.089438Z",
     "iopub.status.idle": "2024-02-15T07:31:01.097585Z",
     "shell.execute_reply": "2024-02-15T07:31:01.096745Z"
    },
    "papermill": {
     "duration": 0.02295,
     "end_time": "2024-02-15T07:31:01.100292",
     "exception": false,
     "start_time": "2024-02-15T07:31:01.077342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.962968709327708"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.degrees(math.acos(0.8575))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8439cf",
   "metadata": {
    "papermill": {
     "duration": 0.010334,
     "end_time": "2024-02-15T07:31:01.121373",
     "exception": false,
     "start_time": "2024-02-15T07:31:01.111039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What metric to use?\n",
    "\n",
    "We have discussed different ways to calcualte the distance between two vectors. And we can use any distance to compare the embeddings we have. However, for NLP tasks, the best practice is sually to use cosine similarity. And some reasons behind it:\n",
    "\n",
    "* Cosine similarity is between -1 and 1, while L1 and L2 are unbounded, so it's easier to interpret.\n",
    "* From the practical perspective, it's more effective to calculate dot products than square roots for Euclidean distance.\n",
    "* Cosine similarity is less affected by the curse of dimensionality\n",
    "\n",
    "Note: There are many of functions that support normalization the embeddings like `sentenceTransformer.encode()`. This can help us to avoid `The curse of dimensionality`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564bc074",
   "metadata": {
    "papermill": {
     "duration": 0.00809,
     "end_time": "2024-02-15T07:31:01.140119",
     "exception": false,
     "start_time": "2024-02-15T07:31:01.132029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credit\n",
    "\n",
    "* https://medium.com/towards-data-science/text-embeddings-comprehensive-guide-afd97fce8fb5\n",
    "* https://arxiv.org/pdf/1301.3781.pdf"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.515729,
   "end_time": "2024-02-15T07:31:01.668383",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-15T07:30:55.152654",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
