{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/computing-embeddings-streaming?scriptVersionId=162452512\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn the notebook [Computing Embeddings with Multi-GPUs](https://www.kaggle.com/code/aisuko/computing-embeddings-with-multi-gpus), we did the computing of embeddings by using multiple GPUs. In this notebook, we want to add streaming data. Let's see the distribution computing+streaming data which is helpful in limit memory. And with streaming data, we don't need to wait for an extremely large dataset to download. See more detail about the streaming data on dataset at https://huggingface.co/docs/datasets/stream","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install sentence-transformers==2.3.1\n!pip install datasets==2.15.0\n!pip install tqdm==4.66.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\nlogging.basicConfig(\n    format='%(asctime)s-%(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S',\n    level=logging.INFO,\n    handlers=[LoggingHandler()]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementation\n\n**NOTE:** We need to shield our code with if `__name__`. Otherwise, CUDA runs into issues when spawning new processes. ","metadata":{}},{"cell_type":"code","source":"if __name__ =='__main__':\n    # size of the data that is loaded into memroy at once\n    data_stream_size=16384\n    # size of the chunks that are sent to each process\n    chunk_size=1024\n    # batch size of the model\n    encode_batch_size=128\n    \n    # Load a large dataset in streaming mode\n    ds=load_dataset('yahoo_answers_topics', split='train', streaming=True)\n    dataloader=DataLoader(ds.with_format('torch'), batch_size=data_stream_size)\n    \n    # define the model\n    model=SentenceTransformer('all-MiniLM-L6-v2')\n    \n    \n    # start the multi-process pool on all available CUDA devices\n    pool=model.start_multi_process_pool()\n    \n    for i, batch in enumerate(tqdm(dataloader)):\n        # compute the embeddings using the multi-process pool\n        sentences=batch['best_answer']\n        batch_emb=model.encode_multi_process(sentences, pool, chunk_size=chunk_size, batch_size=encode_batch_size)\n        print('Embeddings computed for 1 batch. Shape:', batch_emb.shape)\n    # optional: stop the processes in the pool\n    model.stop_multi_process_pool(pool)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}