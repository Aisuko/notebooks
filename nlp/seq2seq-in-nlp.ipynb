{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nI do not have too much experience of hands on tensorflow. So, let's see how to use it pre-process data, create model and do the training. Before the coding part, let's see the common framework and mechanism in modern LLMs.\n\n## Encoder-Decoder architecture\n\nSequence-to-sequence models have revolutionized the way we approach language tasks in NLP. The core idea is to map a sequence of inputs (like words in a sentence) to a sequence of outputs (like translated words in another language). This mapping is achieved through two main components: the encoder and the decoder, often implemented using Long Short-Term Memory (LSTM) networks or Gated Recurrent Units (GRUs).\n\n## Attention Mechanism\n\nWhile the encoder-decoder architecture provides a robust framework for sequence mapping, it’s not without limitations. One key issue is the reliance on a fixed-length context vector to encode the entire input sequence, which can be problematic for long sequences.\n\nAttention Mechanism computes a weight distribution (or attention scores) that determines the importance of each input element for each output.\n\n* Attention Scores\n* Context Vector\n* Decoder’s Input","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\ndef data_preprocessor(source_sentences, target_sentences):\n    \n    source_tokenizer = Tokenizer()\n    source_tokenizer.fit_on_texts(source_sentences)\n    source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n    source_padded = pad_sequences(source_sequences, padding='post')\n      \n    target_tokenizer = Tokenizer()\n    target_tokenizer.fit_on_texts(target_sentences)\n    target_sequences = target_tokenizer.texts_to_sequences(target_sentences)\n    target_padded = pad_sequences(target_sequences, padding='post')\n    \n    return source_padded, target_padded, source_tokenizer, target_tokenizer\n\nenglish_sentences = ['hello', 'world', 'how are you', 'I am fine', 'have a good day']\nspanish_sentences = ['hola', 'mundo', 'cómo estás', 'estoy bien', 'ten un buen día']\ninput_texts, target_texts, source_tokenizer, target_tokenizer = data_preprocessor(english_sentences, spanish_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T00:14:40.119887Z","iopub.execute_input":"2024-04-22T00:14:40.120395Z","iopub.status.idle":"2024-04-22T00:14:40.132145Z","shell.execute_reply.started":"2024-04-22T00:14:40.120362Z","shell.execute_reply":"2024-04-22T00:14:40.130709Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate\nfrom tensorflow.keras.layers import AdditiveAttention as Attention\nfrom tensorflow.keras.models import Model\n\n# Model parameters\nembedding_dim = 256\nlatent_dim = 512\nnum_encoder_tokens = len(source_tokenizer.word_index) + 1\nnum_decoder_tokens = len(target_tokenizer.word_index) + 1\n\n# Encoder\nencoder_inputs = Input(shape=(None,))\nencoder_embedding = Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\nencoder_lstm = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\nencoder_states = [state_h, state_c]\n\n# Decoder\ndecoder_inputs = Input(shape=(None,))\ndecoder_embedding = Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n\n# Attention Layer\nattention = Attention()\nattention_output = attention([decoder_outputs, encoder_outputs])\n\n# Concatenating attention output and decoder LSTM output \ndecoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n\n# Dense layer\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_concat_input)\n\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T00:14:58.562798Z","iopub.execute_input":"2024-04-22T00:14:58.563240Z","iopub.status.idle":"2024-04-22T00:14:59.051448Z","shell.execute_reply.started":"2024-04-22T00:14:58.563206Z","shell.execute_reply":"2024-04-22T00:14:59.050264Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ndecoder_target_data = to_categorical(target_texts, num_decoder_tokens)\nmodel.fit([input_texts, target_texts], decoder_target_data, batch_size=64, epochs=50, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T00:15:19.826751Z","iopub.execute_input":"2024-04-22T00:15:19.827205Z","iopub.status.idle":"2024-04-22T00:15:30.960384Z","shell.execute_reply.started":"2024-04-22T00:15:19.827168Z","shell.execute_reply":"2024-04-22T00:15:30.959408Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.0000e+00 - loss: 2.3969 - val_accuracy: 0.0000e+00 - val_loss: 2.4097\nEpoch 2/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6250 - loss: 2.2366 - val_accuracy: 0.0000e+00 - val_loss: 2.4232\nEpoch 3/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.6250 - loss: 2.0541 - val_accuracy: 0.0000e+00 - val_loss: 2.4432\nEpoch 4/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.6250 - loss: 1.7782 - val_accuracy: 0.0000e+00 - val_loss: 2.4780\nEpoch 5/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6250 - loss: 1.4115 - val_accuracy: 0.0000e+00 - val_loss: 2.5372\nEpoch 6/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.6250 - loss: 1.1964 - val_accuracy: 0.0000e+00 - val_loss: 2.5743\nEpoch 7/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.6250 - loss: 1.1376 - val_accuracy: 0.0000e+00 - val_loss: 2.6060\nEpoch 8/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6250 - loss: 1.0893 - val_accuracy: 0.0000e+00 - val_loss: 2.6369\nEpoch 9/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.6250 - loss: 1.0472 - val_accuracy: 0.0000e+00 - val_loss: 2.6668\nEpoch 10/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.6250 - loss: 1.0091 - val_accuracy: 0.0000e+00 - val_loss: 2.6972\nEpoch 11/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.6250 - loss: 0.9735 - val_accuracy: 0.0000e+00 - val_loss: 2.7283\nEpoch 12/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.6250 - loss: 0.9394 - val_accuracy: 0.0000e+00 - val_loss: 2.7607\nEpoch 13/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6250 - loss: 0.9063 - val_accuracy: 0.0000e+00 - val_loss: 2.7949\nEpoch 14/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6250 - loss: 0.8735 - val_accuracy: 0.0000e+00 - val_loss: 2.8311\nEpoch 15/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.6250 - loss: 0.8409 - val_accuracy: 0.0000e+00 - val_loss: 2.8696\nEpoch 16/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.6250 - loss: 0.8083 - val_accuracy: 0.0000e+00 - val_loss: 2.9107\nEpoch 17/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.6250 - loss: 0.7754 - val_accuracy: 0.0000e+00 - val_loss: 2.9544\nEpoch 18/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.6875 - loss: 0.7421 - val_accuracy: 0.0000e+00 - val_loss: 3.0012\nEpoch 19/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7500 - loss: 0.7082 - val_accuracy: 0.0000e+00 - val_loss: 3.0516\nEpoch 20/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8750 - loss: 0.6736 - val_accuracy: 0.0000e+00 - val_loss: 3.1065\nEpoch 21/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8750 - loss: 0.6382 - val_accuracy: 0.0000e+00 - val_loss: 3.1669\nEpoch 22/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8750 - loss: 0.6022 - val_accuracy: 0.0000e+00 - val_loss: 3.2344\nEpoch 23/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8750 - loss: 0.5659 - val_accuracy: 0.0000e+00 - val_loss: 3.3109\nEpoch 24/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9375 - loss: 0.5302 - val_accuracy: 0.0000e+00 - val_loss: 3.3985\nEpoch 25/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.4958 - val_accuracy: 0.0000e+00 - val_loss: 3.4987\nEpoch 26/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.4637 - val_accuracy: 0.0000e+00 - val_loss: 3.6117\nEpoch 27/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.4342 - val_accuracy: 0.0000e+00 - val_loss: 3.7360\nEpoch 28/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 0.4072 - val_accuracy: 0.0000e+00 - val_loss: 3.8692\nEpoch 29/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 0.3823 - val_accuracy: 0.0000e+00 - val_loss: 4.0088\nEpoch 30/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.3590 - val_accuracy: 0.0000e+00 - val_loss: 4.1532\nEpoch 31/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.3369 - val_accuracy: 0.0000e+00 - val_loss: 4.3009\nEpoch 32/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.3158 - val_accuracy: 0.0000e+00 - val_loss: 4.4511\nEpoch 33/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 0.2956 - val_accuracy: 0.0000e+00 - val_loss: 4.6027\nEpoch 34/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.2762 - val_accuracy: 0.0000e+00 - val_loss: 4.7563\nEpoch 35/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 0.2578 - val_accuracy: 0.0000e+00 - val_loss: 4.9085\nEpoch 36/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9375 - loss: 0.2428 - val_accuracy: 0.0000e+00 - val_loss: 5.0621\nEpoch 37/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9375 - loss: 0.2497 - val_accuracy: 0.0000e+00 - val_loss: 5.1619\nEpoch 38/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9375 - loss: 0.2408 - val_accuracy: 0.0000e+00 - val_loss: 5.2844\nEpoch 39/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9375 - loss: 0.2170 - val_accuracy: 0.0000e+00 - val_loss: 5.3635\nEpoch 40/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9375 - loss: 0.1996 - val_accuracy: 0.0000e+00 - val_loss: 5.6067\nEpoch 41/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9375 - loss: 0.1883 - val_accuracy: 0.0000e+00 - val_loss: 5.4744\nEpoch 42/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9375 - loss: 0.1793 - val_accuracy: 0.0000e+00 - val_loss: 6.1132\nEpoch 43/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9375 - loss: 0.1763 - val_accuracy: 0.0000e+00 - val_loss: 5.3881\nEpoch 44/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9375 - loss: 0.1762 - val_accuracy: 0.0000e+00 - val_loss: 6.7143\nEpoch 45/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9375 - loss: 0.1883 - val_accuracy: 0.0000e+00 - val_loss: 5.3282\nEpoch 46/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9375 - loss: 0.1846 - val_accuracy: 0.0000e+00 - val_loss: 7.0696\nEpoch 47/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9375 - loss: 0.1936 - val_accuracy: 0.0000e+00 - val_loss: 5.5261\nEpoch 48/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.1504 - val_accuracy: 0.0000e+00 - val_loss: 6.6531\nEpoch 49/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.1253 - val_accuracy: 0.0000e+00 - val_loss: 6.3134\nEpoch 50/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.1115 - val_accuracy: 0.0000e+00 - val_loss: 6.4737\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7f5c6e7930d0>"},"metadata":{}}]},{"cell_type":"code","source":"# Encoder Inference Model\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# Decoder Inference Model\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T00:15:37.034948Z","iopub.execute_input":"2024-04-22T00:15:37.037786Z","iopub.status.idle":"2024-04-22T00:15:37.146677Z","shell.execute_reply.started":"2024-04-22T00:15:37.037735Z","shell.execute_reply":"2024-04-22T00:15:37.142978Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m decoder_outputs, state_h, state_c \u001b[38;5;241m=\u001b[39m decoder_lstm(decoder_embedding, initial_state\u001b[38;5;241m=\u001b[39mdecoder_states_inputs)\n\u001b[1;32m      9\u001b[0m decoder_states \u001b[38;5;241m=\u001b[39m [state_h, state_c]\n\u001b[0;32m---> 10\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m decoder_model \u001b[38;5;241m=\u001b[39m Model([decoder_inputs] \u001b[38;5;241m+\u001b[39m decoder_states_inputs, [decoder_outputs] \u001b[38;5;241m+\u001b[39m decoder_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"dense_1\" is incompatible with the layer: expected axis -1 of input shape to have value 1024, but received input with shape (None, None, 512)"],"ename":"ValueError","evalue":"Input 0 of layer \"dense_1\" is incompatible with the layer: expected axis -1 of input shape to have value 1024, but received input with shape (None, None, 512)","output_type":"error"}]},{"cell_type":"code","source":"def translate(input_text):\n    # Tokenize and pad the input sequence\n    input_seq = source_tokenizer.texts_to_sequences([input_text])\n    input_seq = pad_sequences(input_seq, maxlen=input_texts.shape[1], padding='post')\n\n    # Get the encoder states\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate an empty target sequence of length 1\n    target_seq = np.zeros((1, 1))\n\n    # Populate the first character of the target sequence with the start character\n    target_seq[0, 0] = target_tokenizer.word_index['start']  # Assuming 'start' is the start token\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = target_tokenizer.index_word[sampled_token_index]\n        decoded_sentence += ' ' + sampled_char\n\n        # Exit condition: either hit max length or find stop token.\n        if (sampled_char == 'end' or len(decoded_sentence) > 50):  # Assuming 'end' is the end token\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\n# Example usage\ntranslated_sentence = translate(\"hello\")\nprint(translated_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T00:11:34.758275Z","iopub.status.idle":"2024-04-22T00:11:34.758745Z","shell.execute_reply.started":"2024-04-22T00:11:34.758512Z","shell.execute_reply":"2024-04-22T00:11:34.758530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledge\n* https://medium.com/@mervebdurna/exploring-seq2seq-encoder-decoder-and-attention-mechanisms-in-nlp-theory-and-practice-9b1022cf50b4\n* https://github.com/mervebdurna/10-days-NLP-blog-series/blob/main/Day%207%20-%20seq2seq%2C%20encoder-decoder/LanguageTranslation.ipynb","metadata":{}}]}