{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/fine-tuning-microsoft-phi2?scriptVersionId=161744144\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nMicrosoft-Phi2 with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered website. According to the model card, it showcased a nearly state-of-the-art performance among models with less than 13 billion parameters. This means it has a remarkable performance.\n\nLet's fine-tune it on Kaggle environment.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers==4.36.2\n!pip install datasets==2.15.0\n!pip install peft==0.7.1\n!pip install bitsandbytes==0.41.3\n!pip install accelerate==0.25.0\n!pip install trl==0.7.7\n!pip install tqdm==4.66.1\n# Although flash-attn is not supported in Kaggle env.However, we prepare the notebook for future usage.\n# !pip install flash-attn==2.4.2","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:38:33.4511Z","iopub.execute_input":"2024-02-05T02:38:33.451379Z","iopub.status.idle":"2024-02-05T02:40:10.1422Z","shell.execute_reply.started":"2024-02-05T02:38:33.451353Z","shell.execute_reply":"2024-02-05T02:40:10.140994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tune-models\"\nos.environ[\"WANDB_NOTES\"] = \"Fine-tuning casual language models\"\nos.environ[\"WANDB_NAME\"] = \"fine-tuning-Phi2-with-webglm-qa-with-lora\"\nos.environ[\"MODEL_NAME\"] = \"microsoft/phi-2\"\nos.environ[\"DATASET_NAME\"]=\"THUDM/webglm-qa\"","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:10.144391Z","iopub.execute_input":"2024-02-05T02:40:10.144698Z","iopub.status.idle":"2024-02-05T02:40:11.108946Z","shell.execute_reply.started":"2024-02-05T02:40:10.144667Z","shell.execute_reply":"2024-02-05T02:40:11.107984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:11.11028Z","iopub.execute_input":"2024-02-05T02:40:11.110648Z","iopub.status.idle":"2024-02-05T02:40:21.390557Z","shell.execute_reply.started":"2024-02-05T02:40:11.110613Z","shell.execute_reply":"2024-02-05T02:40:21.389419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvdia-smi","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:21.393481Z","iopub.execute_input":"2024-02-05T02:40:21.394864Z","iopub.status.idle":"2024-02-05T02:40:22.407539Z","shell.execute_reply.started":"2024-02-05T02:40:21.394812Z","shell.execute_reply":"2024-02-05T02:40:22.406464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the dataset\n\nHere are the several steps:\n* load the dataset\n* tokenize the train/test datasets for fine-tuning purposes\n\nHere we are merging validate and test datasets, which amount to 1400 rows.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_dataset=load_dataset(os.getenv(\"DATASET_NAME\"), split=\"train[5000:6000]\")\n\n# merge validation/test datasets\ntest_dataset=load_dataset(os.getenv(\"DATASET_NAME\"), split=\"validation+test\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:22.409325Z","iopub.execute_input":"2024-02-05T02:40:22.409759Z","iopub.status.idle":"2024-02-05T02:40:39.130749Z","shell.execute_reply.started":"2024-02-05T02:40:22.409721Z","shell.execute_reply":"2024-02-05T02:40:39.130001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the processing function","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Setting up the tokenizer for Phi-2\ntokenizer=AutoTokenizer.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    add_eos_token=True, \n    trust_remote_code=True\n)\n\ntokenizer.pad_token=tokenizer.eos_token\ntokenizer.truncation_side=\"left\"","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:39.132117Z","iopub.execute_input":"2024-02-05T02:40:39.133176Z","iopub.status.idle":"2024-02-05T02:40:42.667932Z","shell.execute_reply.started":"2024-02-05T02:40:39.133138Z","shell.execute_reply":"2024-02-05T02:40:42.666868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_and_tokenize(examples):\n    question=examples[\"question\"][0].replace('\"',r'\\\"')\n    answer=examples[\"answer\"][0].replace('\"',r'\\\"')\n    references='\\n'.join([f\"[{index+1}] {string}\" for index, string in enumerate(examples[\"references\"][0])])\n    \n    # Merging into one prompt for tokenization and training\n    prompt=f\"\"\"###System:\nRead the reference provided and answer the corresponding question.\n###References:\n{references}\n###Question:\n{question}\n###Answer:\n{answer}\"\"\"\n    \n    # Tokenize the prompt\n    encoded =tokenizer(\n        prompt,\n        return_tensors=\"np\",\n        padding=\"max_length\",\n        truncation=True,\n        max_length=None,\n    )\n    \n    encoded[\"labels\"]=encoded[\"input_ids\"]\n    return encoded","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:42.669281Z","iopub.execute_input":"2024-02-05T02:40:42.670177Z","iopub.status.idle":"2024-02-05T02:40:42.677558Z","shell.execute_reply.started":"2024-02-05T02:40:42.670138Z","shell.execute_reply":"2024-02-05T02:40:42.676489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will just keep the input_ids and labels that we add in function above.\ncolumns_to_remove=[\"question\",\"answer\",\"references\"]\n\n#tokenize the training and test datasets\ntokenized_dataset_train=train_dataset.map(\n    collate_and_tokenize,\n    batched=True,\n    batch_size=1,\n    remove_columns=columns_to_remove\n)\n\ntokenized_dataset_test=test_dataset.map(\n    collate_and_tokenize,\n    batched=True,\n    batch_size=1,\n    remove_columns=columns_to_remove\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:42.678804Z","iopub.execute_input":"2024-02-05T02:40:42.679383Z","iopub.status.idle":"2024-02-05T02:40:50.571311Z","shell.execute_reply.started":"2024-02-05T02:40:42.679356Z","shell.execute_reply":"2024-02-05T02:40:50.570322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the model\n\n\nWe are going to use quantization technique.\n\n32-bit floating points will cause 4 bytes of memory for each weight. 16-bit requires 2 bytes, an 8-bit requires 1 byte. 4-bit requires 0.5 bytes.\n\nFor Phi-2, with 2.7 billion parameters, the memory requirement for loading the model is approximately $2.7*4=10.8$ GB. It's important to note that this is solely for loading the model; during training, the memory usage expands ofeten doubling the initial requirement. And with Adam optimizer, it will quadruple it.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import BitsAndBytesConfig\n\nbnb_config=BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_enable_fp32_cpu_offload=True\n)\n\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    device_map='auto',\n    quantization_config=bnb_config,\n#     attn_implementation=\"flash_attention_2\"\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16\n)\n\ndef print_trainable_parameters(model):\n    trainable_params=0\n    all_params=0\n    for _, param in model.named_parameters():\n        all_params+=param.numel()\n        if param.requires_grad:\n            trainable_params+=param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params/all_params:.2f}\")\n\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:40:50.572804Z","iopub.execute_input":"2024-02-05T02:40:50.573286Z","iopub.status.idle":"2024-02-05T02:41:27.35394Z","shell.execute_reply.started":"2024-02-05T02:40:50.573247Z","shell.execute_reply":"2024-02-05T02:41:27.352814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_memory_footprint()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:27.355327Z","iopub.execute_input":"2024-02-05T02:41:27.355641Z","iopub.status.idle":"2024-02-05T02:41:27.366947Z","shell.execute_reply.started":"2024-02-05T02:41:27.355614Z","shell.execute_reply":"2024-02-05T02:41:27.365995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.quantization_config","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:27.370401Z","iopub.execute_input":"2024-02-05T02:41:27.370685Z","iopub.status.idle":"2024-02-05T02:41:27.979661Z","shell.execute_reply.started":"2024-02-05T02:41:27.37066Z","shell.execute_reply":"2024-02-05T02:41:27.978023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training with QLoRA","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\n#gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\nmodel.get_memory_footprint()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:27.98276Z","iopub.execute_input":"2024-02-05T02:41:27.983297Z","iopub.status.idle":"2024-02-05T02:41:28.132616Z","shell.execute_reply.started":"2024-02-05T02:41:27.98325Z","shell.execute_reply":"2024-02-05T02:41:28.131362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#freeze base model layers and casr layernorm in fp32\nprepared_model=prepare_model_for_kbit_training(\n    model, use_gradient_checkpointing=True\n)\nprepared_model.get_memory_footprint()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:28.134404Z","iopub.execute_input":"2024-02-05T02:41:28.134833Z","iopub.status.idle":"2024-02-05T02:41:28.182848Z","shell.execute_reply.started":"2024-02-05T02:41:28.134771Z","shell.execute_reply":"2024-02-05T02:41:28.181802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we print the model, we can see that the target modules it uses. We are going to use these target_modules in our LoRA adapter below.","metadata":{}},{"cell_type":"code","source":"print(prepared_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:28.184365Z","iopub.execute_input":"2024-02-05T02:41:28.185311Z","iopub.status.idle":"2024-02-05T02:41:28.216976Z","shell.execute_reply.started":"2024-02-05T02:41:28.18527Z","shell.execute_reply":"2024-02-05T02:41:28.215641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ValueError: FSDP requires PyTorch >= 2.1.0\n\n# from accelerate import FullyShardedDataParallelPlugin, Accelerator\n# from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n\n# fsdp_plugin=FullyShardedDataParallelPlugin(\n#     state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n#     optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False)\n# )\n\n# accelerator=Accelerator(fsdp_plugin=fsdp_plugin)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:28.218747Z","iopub.execute_input":"2024-02-05T02:41:28.21948Z","iopub.status.idle":"2024-02-05T02:41:28.312273Z","shell.execute_reply.started":"2024-02-05T02:41:28.21944Z","shell.execute_reply":"2024-02-05T02:41:28.310846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\npeft_config=LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense',\n        'fc1',\n        'fc2',\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=TaskType.CAUSAL_LM\n)\n\nlora_model=get_peft_model(prepared_model, peft_config)\nlora_model.get_memory_footprint()\n\n# lora_model=accelerator.prepare_model(lora_model)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:28.314033Z","iopub.execute_input":"2024-02-05T02:41:28.314709Z","iopub.status.idle":"2024-02-05T02:41:29.174787Z","shell.execute_reply.started":"2024-02-05T02:41:28.314665Z","shell.execute_reply":"2024-02-05T02:41:29.173815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction of the parameters\n\n* **per_device_train_batch_size** and **gradient_accumulation_steps**\n\n    Both these params together would form the overall batch size. As we have these set to \"2\" and \"5\", our training batch size is 10. That means the our total steps would be $(1000/10)*1=100$. Where 1000 is the training dataset size, 10 is the batch size and 1 is the number of epochs.\n    \n* **max_steps** and **num_train_epochs**\n\n    These two parameters are mutually exclusive. One epoch is one full cycle through the training data, whereas steps is calculated as (datasetsize/batch_size)*(num_epcohs)\n    \n* **optim**\n\n    Optimizers are primarily responsible for minimizing the error of loss of the model by adjusting the model's parameters or weights. Their ultimate goal is to find the \"optimal\" set of parameters that enables the model to make close-to-accurate predictions on new, previosuly unseen data.\n    Regular optimizers like Adam can consume a substantially large amount of GPU memory. That's why we are using an 8-bit paged optimizer, employing lower precision to store the state and enabling paging, which reduce the load on the GPU.\n    ","metadata":{}},{"cell_type":"code","source":"import time\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    overwrite_output_dir=True,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=5,\n    gradient_checkpointing=True,  # Enable gradient checkpointing\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    warmup_steps=20,\n    max_steps=100, # Total number of training steps\n    num_train_epochs=1, # Number of training epochs\n    learning_rate=5e-5, # Learning rate\n    weight_decay=0.01, # Weight decay\n    optim=\"paged_adamw_8bit\", # Keep the optimizer state and quantize it\n#     bf16=True, # Do not supported in Kaggle environment, require Ampere....\n    fp16=True, # use fp16 16bit(mixed) precision training instead of 32-bit training.\n    logging_dir='./logs',\n    logging_strategy=\"steps\",\n    logging_steps=20,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2, # Limit the total number of checkpoints\n    evaluation_strategy=\"steps\",\n    eval_steps=20,\n    load_best_model_at_end=True, # Load the best model at the end of training,\n    report_to=\"wandb\",\n    run_name=os.getenv(\"WANDB_NAME\")\n)\n\nlora_model.config.use_cache=False\n\ntrainer=Trainer(\n    model=lora_model,\n    train_dataset=tokenized_dataset_train,\n    eval_dataset=tokenized_dataset_test,\n    args=training_args,\n)\n\n\nstart_time=time.time()\ntrainer.train()\nend_time=time.time()\n\ntraining_time=end_time-start_time\n\nprint(f\"Training completed in {training_time} seconds.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T02:41:29.176158Z","iopub.execute_input":"2024-02-05T02:41:29.176847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"#Setup a prompt that we can use for testing\n\nnew_prompt = \"\"\"###System:\nRead the references provided and answer the corresponding question.\n###References:\n[1] For most people, the act of reading is a reward in itself. However, studies show that reading books also has benefits that range from a longer life to career success. If you’re looking for reasons to pick up a book, read on for seven science-backed reasons why reading is good for your health, relationships and happiness.\n[2] As per a study, one of the prime benefits of reading books is slowing down mental disorders such as Alzheimer’s and Dementia  It happens since reading stimulates the brain and keeps it active, which allows it to retain its power and capacity.\n[3] Another one of the benefits of reading books is that they can improve our ability to empathize with others. And empathy has many benefits – it can reduce stress, improve our relationships, and inform our moral compasses.\n[4] Here are 10 benefits of reading that illustrate the importance of reading books. When you read every day you:\n[5] Why is reading good for you? Reading is good for you because it improves your focus, memory, empathy, and communication skills. It can reduce stress, improve your mental health, and help you live longer. Reading also allows you to learn new things to help you succeed in your work and relationships.\n###Question:\nWhy is reading books widely considered to be beneficial?\n###Answer:\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del lora_model, trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs=tokenizer(\n    new_prompt, \n    return_tensors=\"pt\", \n    return_attention_mask=False, \n    padding=True, \n    truncation=True)\n\ninputs.to('cuda')\nprepared_model.config.use_cache=True\n\noutputs=prepared_model.generate(**inputs, repetition_penalty=1.0, max_length=1000)\nresult=tokenizer.batch_decode(outputs, skip_special_tokens=True)\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\n\nmodel_name=\"aisuko/\"+os.getenv(\"WANDB_NAME\")\npeft_model=PeftModel.from_pretrained(prepared_model, model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs=peft_model.generate(**inputs, max_length=1000)\ntext=tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Credit\n\n* https://medium.com/@yernenip/optimizing-phi-2-a-deep-dive-into-fine-tuning-small-language-models-9d545ac90a99","metadata":{}}]}