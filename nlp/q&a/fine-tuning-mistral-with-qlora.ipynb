{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.36.2\n!pip install accelerate==0.25.0\n!pip install datasets==2.15.0\n!pip install peft==0.7.1\n!pip install bitsandbytes==0.41.3","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:10:32.203934Z","iopub.execute_input":"2024-03-04T11:10:32.204794Z","iopub.status.idle":"2024-03-04T11:11:50.076074Z","shell.execute_reply.started":"2024-03-04T11:10:32.204750Z","shell.execute_reply":"2024-03-04T11:11:50.074946Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Mistral-7B-01\"\nos.environ[\"WANDB_NAME\"] = \"ft-mistral-7b-v01\"\nos.environ[\"MODEL_NAME\"] = \"mistralai/Mistral-7B-v0.1\"\nos.environ[\"DATASET\"] = \"OpenAssistant/oasst_top1_2023-08-25\"\n\ntorch.backends.cudnn.deterministic=True","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:11:50.078051Z","iopub.execute_input":"2024-03-04T11:11:50.078350Z","iopub.status.idle":"2024-03-04T11:11:54.211933Z","shell.execute_reply.started":"2024-03-04T11:11:50.078323Z","shell.execute_reply":"2024-03-04T11:11:54.211071Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:11:54.213044Z","iopub.execute_input":"2024-03-04T11:11:54.213465Z","iopub.status.idle":"2024-03-04T11:12:04.411301Z","shell.execute_reply.started":"2024-03-04T11:11:54.213440Z","shell.execute_reply":"2024-03-04T11:12:04.410284Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading pretrained config for `mistralai/Mistral-7B-v0.1` from `transformers`...\nconfig.json: 100%|█████████████████████████████| 571/571 [00:00<00:00, 3.08MB/s]\n┌────────────────────────────────────────────────────────┐\n│  Memory Usage for loading `mistralai/Mistral-7B-v0.1`  │\n├───────┬─────────────┬──────────┬───────────────────────┤\n│ dtype │Largest Layer│Total Size│  Training using Adam  │\n├───────┼─────────────┼──────────┼───────────────────────┤\n│float32│  864.03 MB  │ 27.49 GB │       109.96 GB       │\n│float16│  432.02 MB  │ 13.74 GB │        54.98 GB       │\n│  int8 │  216.01 MB  │ 6.87 GB  │        27.49 GB       │\n│  int4 │   108.0 MB  │ 3.44 GB  │        13.74 GB       │\n└───────┴─────────────┴──────────┴───────────────────────┘\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Datasets","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset=load_dataset(os.getenv(\"DATASET\"), split=\"train[:500]\")\ndataset=dataset.train_test_split(test_size=0.1)\nprint(dataset[\"train\"][0][\"text\"])\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-04T11:12:04.412875Z","iopub.execute_input":"2024-03-04T11:12:04.413202Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/512 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7187da3094d4b948599005cb854ac54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"132f18a392cd407c95ee729f79841161"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/31.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4693926cba54de187ea2e95b8f08cdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb7c5dbf7b5645a49bf7a2e3732efc86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f772fefd07d4bdf8ce450a5cc0b087f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ae96b7036a44e1e8632b00efeb9db67"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading the Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# fast tokenizer sometimes ignores added tokens\ntokenizer=AutoTokenizer.from_pretrained(os.getenv('MODEL_NAME'), use_fast=False)\n# add tokens <|im_start|> and <|im_end|>, latter is special eos token\ntokenizer.pad_token=\"</s>\"\ntokenizer.add_tokens([\"<|im_start|>\"])\ntokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\nprint(len(tokenizer))\ntokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"def preprocess_func(example):\n    return tokenizer(example[\"text\"], truncation=True, max_length=2048, add_special_tokens=False)\n\ndataset_tokenized=dataset.map(preprocess_func, batched=True, num_proc=os.cpu_count(), remove_columns=[\"text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spliting Batch","metadata":{}},{"cell_type":"code","source":"def collate(example):\n    \"\"\"\n    Transform list of dictionaties [{input_ids:[123,...]}, {...}] \n    to single batch dictionary { input_ids: [...], labels: [...], attention_mask: [...]}\n    \"\"\"\n    tokenlist=[e[\"input_ids\"] for e in elements]\n    tokens_maxlen=max([len(t) for t in tokenlist])\n    \n    input_ids, labels, attention_masks=[],[],[]\n    for tokens in tokenlist:\n        pad_len=tokens_maxlen-len(tokens)\n        # pad input_ids with pad_token, label with ignore_index (-100) and set attention_mask 1 where content, otherwise 0\n        input_ids.append(tokens+[tokenizer.pad_token_id]*pad_len)\n        labels.append(tokens+[-100]*pad_len)\n        attention_masks.append([1]*len(tokens)+[0]*pad_len)\n    batch={\n        \"input_ids\":torch.tensor(input_ids),\n        \"labels\":torch.tensor(labels),\n        \"attention_mask\": torch.tensor(attention_masks)\n    }\n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config=BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_enable_fp32_cpu_offload=True,\n)\n\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n)\n\ndef print_trainable_parameters(model):\n    trainable_params=0\n    all_params=0\n    for _, param in model.named_parameters():\n        all_params+=param.numel()\n        if param.requires_grad:\n            trainable_params+=param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params/all_params:.2f}\")\n\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.eos_token_id=tokenizer.eos_token_id\nmodel.gradient_checkpointing_enable()\n\nprint_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Freeze Weights and add LoRA","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nprepared_model=prepare_model_for_kbit_training(\n    model, use_gradient_checkpointing=True\n)\n\nprint_trainable_parameters(prepared_model)\nprint(prepared_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\n\nlora_config=LoraConfig(\n    r=64,\n    lora_alpha=16,\n    target_modules=['q_proj', 'k_proj', 'down_proj', 'v_proj','gate_proj', 'o_proj', 'up_proj'],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_Save=[\"lm_head\",\"embed_tokens\"], # we added new tokens to tokenizer, this is necesarry\n    task_type=TaskType.CAUSAL_LM\n)\n\nlora_model=get_peft_model(prepared_model, lora_config)\nlora_model.config.use_cache=False\nprint_trainable_parameters(lora_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nbs=8\nga_steps=4\nepochs=5\n\nsteps_per_epoch=len(dataset_tokenized[\"train\"])//(bs*ga_steps)\n\nargs=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    per_device_train_batch_size=bs,\n    per_device_eval_batch_size=bs,\n    evaluation_strategy=\"steps\",\n    logging_steps=1,\n    eval_steps=steps_per_epoch,\n    save_steps=steps_per_epoch,\n    # increases effective batch size without consuming additional VRAM but makes training slower.\n    # the effective batch size is batch_size* gradient_accumulation_steps\n    gradient_accumulation_steps=ga_steps,\n    num_train_epochs=epochs,\n    lr_scheduler_type=\"constant\",\n    optim=\"paged_adamw_32bit\",\n    # using default lr suggested by QLoRA. 0.0002 for &b/13B model. For more parameters, lower lr are suggested.\n    # for example, 0.0001 for models with 33B and 65B parameters.\n    learning_rate=0.0002,\n    group_by_length=True,\n    fp16=True,\n    ddp_find_unused_parameters=False # needed for training with accelerate\n)\n\ntrainer=Trainer(\n    model=lora_model,\n    tokenizer=tokenizer,\n    data_collator=collate,\n    train_dataset=dataset_tokenized[\"train\"],\n    eval_dataset=dataset_tokenized[\"test\"],\n    args=args\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}