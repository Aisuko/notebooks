{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7811545,"sourceType":"datasetVersion","datasetId":4575347}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nBefore we got our own fine-tuned LLMs. How we can make our basline LLM in a higher accuracy? One of the answers might be **Retrieval Augmented Generation(RAG)**. RAG enables LLMs reach beyond their static knowledge and grab real-time information quite easily. RAG taps into external sources, like internal company documents, to understand the context of our queries a bit better.\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/064/408/832/155/132/small/3f7a8bd57563298f.webp)\n\nIn this notebook, we are going to use RAG model. Retrieval-agumented generation(\"RAG\") models combine the powers of pretrained dense retrieval(DPR) and Seq2Seq2 models. RAG models retrieve docs, pass them to a seq2seq2 model, then marginalize to generate outputs. The retriever and seq2seq modules are initlized from pretrained models, and fine-tuned jointly, allowing both retreival and generation to adapt to downstream tasks. Here are some classes inside transformers that we need to know:\n\n\n# RagConfig Class\n\nIt stores the configuration of a RagModel which is used to control thr model outputs.\n\n\n# Retriever Class\n\nIt used to get documents from vector queries. It retrives the documents embeddings as well as the documents contents, and it formats them to be used with a RagModel.\n\n\n# RAGModel Class\n\nRAG is a seq2seq model which encapsulates two core components: a question encoder and a generator. During a forward pass, we encode the input with the question encoder and pass it to the retriever to extract releveant context documents. The documents are then prepended to the input. Such contextualized inputs is passed to the generator.\n\nThe question encoder can be any autoencoding model, preferably **DPPQuestionEncoder** and the generator can be any **seq2seq**, preferably **BartForConditionalGeneration**. So, we can initialzed with the RAG model with any autoencoding mdoel as the question_encoder and any seq2seq model with language model heads as the generator. It inherits from **PreTrainedModel**, and it is also a PyTorch **torch.nn.Module** subclass. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!conda install -c pytorch faiss-cpu=1.8.0 -y","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:07:19.435901Z","iopub.execute_input":"2024-03-11T04:07:19.436211Z","iopub.status.idle":"2024-03-11T04:08:40.834453Z","shell.execute_reply.started":"2024-03-11T04:07:19.436184Z","shell.execute_reply":"2024-03-11T04:08:40.833510Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - pytorch\n - rapidsai\n - nvidia\n - conda-forge\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n    current version: 23.11.0\n    latest version: 24.1.2\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - faiss-cpu=1.8.0\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    faiss-cpu-1.8.0            |py3.10_h9d89a2e_0_cpu         4.4 MB  pytorch\n    libfaiss-1.8.0             |   hf65b397_0_cpu         5.2 MB  pytorch\n    mkl-2023.2.0               |   h84fe81f_50496       156.8 MB  conda-forge\n    ------------------------------------------------------------\n                                           Total:       166.3 MB\n\nThe following NEW packages will be INSTALLED:\n\n  faiss-cpu          pytorch/linux-64::faiss-cpu-1.8.0-py3.10_h9d89a2e_0_cpu \n  libfaiss           pytorch/linux-64::libfaiss-1.8.0-hf65b397_0_cpu \n\nThe following packages will be DOWNGRADED:\n\n  mkl                               2024.0.0-ha957f24_49657 --> 2023.2.0-h84fe81f_50496 \n\n\n\nDownloading and Extracting Packages:\nmkl-2023.2.0         | 156.8 MB  |                                       |   0% \nlibfaiss-1.8.0       | 5.2 MB    |                                       |   0% \u001b[A\n\nmkl-2023.2.0         | 156.8 MB  | 1                                     |   0% \u001b[A\u001b[A\nlibfaiss-1.8.0       | 5.2 MB    | ###5                                  |  10% \u001b[A\n\nmkl-2023.2.0         | 156.8 MB  | ####################################3 |  98% \u001b[A\u001b[A\n\nfaiss-cpu-1.8.0      | 4.4 MB    | ##################################### | 100% \u001b[A\u001b[A\n\nfaiss-cpu-1.8.0      | 4.4 MB    | ##################################### | 100% \u001b[A\u001b[A\nlibfaiss-1.8.0       | 5.2 MB    | ##################################### | 100% \u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}]},{"cell_type":"code","source":"# %%capture\n# !conda install -c pytorch -c nvidia faiss-gpu=1.8.0 -y","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:08:40.836059Z","iopub.execute_input":"2024-03-11T04:08:40.836380Z","iopub.status.idle":"2024-03-11T04:08:40.841166Z","shell.execute_reply.started":"2024-03-11T04:08:40.836353Z","shell.execute_reply":"2024-03-11T04:08:40.840190Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.38.2\n# !pip install accelerate==0.27.2\n!pip install datasets==2.18.0 # Fix issue numpy attributes error of RagRetriever\n# !pip install peft==0.9.0\n# !pip install bitsandbytes==0.42.0","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:08:40.842431Z","iopub.execute_input":"2024-03-11T04:08:40.842735Z","iopub.status.idle":"2024-03-11T04:09:16.585777Z","shell.execute_reply.started":"2024-03-11T04:08:40.842708Z","shell.execute_reply":"2024-03-11T04:09:16.584732Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!transformers-cli env","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:09:16.588367Z","iopub.execute_input":"2024-03-11T04:09:16.588826Z","iopub.status.idle":"2024-03-11T04:09:36.111207Z","shell.execute_reply.started":"2024-03-11T04:09:16.588785Z","shell.execute_reply":"2024-03-11T04:09:36.110250Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2024-03-11 04:09:22.640527: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-11 04:09:22.640747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-11 04:09:22.776410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n\n- `transformers` version: 4.38.2\n- Platform: Linux-5.15.133+-x86_64-with-glibc2.31\n- Python version: 3.10.13\n- Huggingface_hub version: 0.20.3\n- Safetensors version: 0.4.2\n- Accelerate version: 0.27.2\n- Accelerate config: \tnot found\n- PyTorch version (GPU?): 2.1.2+cpu (False)\n- Tensorflow version (GPU?): 2.15.0 (False)\n- Flax version (CPU?/GPU?/TPU?): 0.8.1 (cpu)\n- Jax version: 0.4.24\n- JaxLib version: 0.4.24\n- Using GPU in script?: <fill in>\n- Using distributed or parallel set-up in script?: <fill in>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport warnings\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\n\nos.environ[\"MODEL_NAME\"] = \"facebook/rag-token-base\"\nos.environ[\"DATASET\"] = \"\"\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic=True\n    # https://github.com/huggingface/transformers/issues/28731\n    torch.backends.cuda.enable_mem_efficient_sdp(False)\n    device='cuda'\nelse:\n    device='cpu'\n    \nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:11:43.497400Z","iopub.execute_input":"2024-03-11T04:11:43.497739Z","iopub.status.idle":"2024-03-11T04:11:45.673899Z","shell.execute_reply.started":"2024-03-11T04:11:43.497712Z","shell.execute_reply":"2024-03-11T04:11:45.673012Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv(\"MODEL_NAME\"))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:11:45.675446Z","iopub.execute_input":"2024-03-11T04:11:45.676526Z","iopub.status.idle":"2024-03-11T04:11:49.235622Z","shell.execute_reply.started":"2024-03-11T04:11:45.676483Z","shell.execute_reply":"2024-03-11T04:11:49.234213Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d5a2d531a14b0097c76386eef77276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)_encoder_tokenizer/tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ac48a0279a422eadf8d5c0ad02a48f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"question_encoder_tokenizer/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6950ae4baafb409b9cb657af3c284fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ncoder_tokenizer/special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e711c17b54294e9aad8ccd9f6a60a7de"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'DPRQuestionEncoderTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"(…)enerator_tokenizer/tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a6ac249b1147dab2ec47da1b75e05b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generator_tokenizer/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d15893722f6743ff83487840dcc9ac2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generator_tokenizer/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b93354597f441aab1de195ddfb5647"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)erator_tokenizer/special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad703351d6f42f9bbe4d20096a0ab86"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'BartTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'BartTokenizerFast'.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RagRetriever\n\nretriever=RagRetriever.from_pretrained(\n    os.getenv(\"MODEL_NAME\"), \n    index_name=\"exact\", \n    use_dummy_dataset=True,\n    trust_remote_code=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:11:49.236945Z","iopub.execute_input":"2024-03-11T04:11:49.237279Z","iopub.status.idle":"2024-03-11T04:15:25.832154Z","shell.execute_reply.started":"2024-03-11T04:11:49.237258Z","shell.execute_reply":"2024-03-11T04:15:25.831487Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'DPRQuestionEncoderTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'BartTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RagTokenizer'. \nThe class this function is called from is 'BartTokenizerFast'.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e1daaf1b590495aa61c6d58986e0e13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/15.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e7f44ba5b34ef38f461635756f7901"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.69G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00471d3e238e410e8abc33ac4b21924d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.32G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ba8bd7eee04a1eb40337920f81ab43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17798b05ed2a4d339416e3622451f3a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10d1aa84db754f10b2896349a97a6d71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957ba5e6dd4f4fc1b534b23d7050cc50"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import RagSequenceForGeneration\n\nmodel=RagSequenceForGeneration.from_pretrained(os.getenv(\"MODEL_NAME\"), retriever=retriever, device=device)\ntype(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:15:25.833897Z","iopub.execute_input":"2024-03-11T04:15:25.834523Z","iopub.status.idle":"2024-03-11T04:17:05.254051Z","shell.execute_reply.started":"2024-03-11T04:15:25.834500Z","shell.execute_reply":"2024-03-11T04:17:05.253035Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab89a41245454291a5a5a0fdf0ac4f10"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"transformers.models.rag.modeling_rag.RagSequenceForGeneration"},"metadata":{}}]},{"cell_type":"code","source":"input_ids=tokenizer(\"How many people live in Melbourne?\", return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:17:05.255201Z","iopub.execute_input":"2024-03-11T04:17:05.255646Z","iopub.status.idle":"2024-03-11T04:17:05.264901Z","shell.execute_reply.started":"2024-03-11T04:17:05.255617Z","shell.execute_reply":"2024-03-11T04:17:05.263808Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"generated=model.generate(input_ids[\"input_ids\"], max_length=50)\ngenerated_str=tokenizer.decode(generated[0], skip_special_tokens=True)\ngenerated_str","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:17:05.266144Z","iopub.execute_input":"2024-03-11T04:17:05.266429Z","iopub.status.idle":"2024-03-11T04:17:39.673710Z","shell.execute_reply.started":"2024-03-11T04:17:05.266407Z","shell.execute_reply":"2024-03-11T04:17:39.673046Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'The!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Reference List\n\n* https://www.promptingguide.ai/research/rag\n* https://blog.gopenai.com/retrieval-augmented-generation-rag-585aa903d6bd\n* https://github.com/ray-project/ray\n* https://github.com/facebookresearch/faiss/blob/main/INSTALL.md\n* https://lightning.ai/docs/pytorch/stable/","metadata":{}}]}