{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will use building a RAG system that suggests short and easy to read ML paper titles from original ML paper titles. Our use case is that the paper tiles can be too technical for a general audience so using RAG to generate short titles based on previously created short titles can make research paper titles more accessible and used for science communication such as in the form of newsletters or blogs.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.38.2\n!pip install accelerate==0.27.2\n# !pip install datasets==2.18.0\n!pip install peft==0.9.0\n!pip install bitsandbytes==0.42.0\n!pip install sentence-transformers==2.5.1\n!pip install chromadb==0.4.24","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:10:13.046682Z","iopub.execute_input":"2024-03-15T10:10:13.047550Z","iopub.status.idle":"2024-03-15T10:12:06.212696Z","shell.execute_reply.started":"2024-03-15T10:10:13.047519Z","shell.execute_reply":"2024-03-15T10:12:06.211165Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\n\nos.environ[\"MODEL_NAME\"] = \"mistralai/Mistral-7B-Instruct-v0.2\"\nos.environ[\"DATASET\"]=\"/kaggle/input/weekly-top-trending-ml-papers/ml-potw-10232023.csv\"\n\n\ntorch.backends.cudnn.deterministic=True\n# https://github.com/huggingface/transformers/issues/28731\ntorch.backends.cuda.enable_mem_efficient_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:12:06.215430Z","iopub.execute_input":"2024-03-15T10:12:06.215778Z","iopub.status.idle":"2024-03-15T10:12:10.638120Z","shell.execute_reply.started":"2024-03-15T10:12:06.215736Z","shell.execute_reply":"2024-03-15T10:12:10.637261Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:12:10.639351Z","iopub.execute_input":"2024-03-15T10:12:10.639799Z","iopub.status.idle":"2024-03-15T10:12:21.662994Z","shell.execute_reply.started":"2024-03-15T10:12:10.639754Z","shell.execute_reply":"2024-03-15T10:12:21.662019Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading pretrained config for `mistralai/Mistral-7B-Instruct-v0.2` from `transformers`...\nconfig.json: 100%|█████████████████████████████| 596/596 [00:00<00:00, 2.82MB/s]\n┌──────────────────────────────────────────────────────────────────┐\n│  Memory Usage for loading `mistralai/Mistral-7B-Instruct-v0.2`   │\n├───────┬─────────────┬──────────┬─────────────────────────────────┤\n│ dtype │Largest Layer│Total Size│       Training using Adam       │\n├───────┼─────────────┼──────────┼─────────────────────────────────┤\n│float32│  864.03 MB  │ 27.49 GB │            109.96 GB            │\n│float16│  432.02 MB  │ 13.74 GB │             54.98 GB            │\n│  int8 │  216.01 MB  │ 6.87 GB  │             27.49 GB            │\n│  int4 │   108.0 MB  │ 3.44 GB  │             13.74 GB            │\n└───────┴─────────────┴──────────┴─────────────────────────────────┘\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(os.getenv(\"MODEL_NAME\"))\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:12:21.665984Z","iopub.execute_input":"2024-03-15T10:12:21.666400Z","iopub.status.idle":"2024-03-15T10:12:23.767517Z","shell.execute_reply.started":"2024-03-15T10:12:21.666363Z","shell.execute_reply":"2024-03-15T10:12:23.766575Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4a5fe5e1079495abfcbcc8c574a4f3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9ee4ac7ae64a6dad3eda6d8bd988ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71223f2d21ec47acaf68ebb57002306f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3483b3afb52247539bf8b837425b78bd"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_enable_fp32_cpu_offload=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\nmodel.config.eos_token_id=tokenizer.eos_token_id\nmodel.gradient_checkpointing_enable() # reducing memory usage\nprint(model.model.embed_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:12:23.768808Z","iopub.execute_input":"2024-03-15T10:12:23.769310Z","iopub.status.idle":"2024-03-15T10:13:44.535504Z","shell.execute_reply.started":"2024-03-15T10:12:23.769278Z","shell.execute_reply":"2024-03-15T10:13:44.534548Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce39b8b2ef114e878cc72cf5be6e9638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bcbde13ef17475990090483e295fdb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef078ab2928041e1ba0f276f0cfc17ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74eb1279e61b4c1f939a02641026a687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157c4b7ff8354e93b035a868db83c8dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a95d40cf14634d6d9fb7166abd97daf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec039feeea984859b388b9a8c79117ac"}},"metadata":{}},{"name":"stdout","text":"Embedding(32000, 4096)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Checking the Model\n\nTips: Please know that we do not fine-tune the model for fitting the specific tasks. So, the answer may not good. However, it is enough for us to illustrate our RAG's solution.","metadata":{}},{"cell_type":"code","source":"input_text = \"The weather in Melbourne is \"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_length=20, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:14:37.207888Z","iopub.execute_input":"2024-03-15T10:14:37.208777Z","iopub.status.idle":"2024-03-15T10:14:38.698989Z","shell.execute_reply.started":"2024-03-15T10:14:37.208717Z","shell.execute_reply":"2024-03-15T10:14:38.698034Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> The weather in Melbourne is 17C, sunny but with a slight breeze.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prompt with Chat Template","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"[INST]\nGiven the following wedding guest data, write a very short 3-sentences thank you letter:\n\n{\n  \"name\": \"John Doe\",\n  \"relationship\": \"Bride's cousin\",\n  \"hometown\": \"New York, NY\",\n  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n  \"attending_with\": \"Sophia Smith\",\n  \"bride_groom_name\": \"Tom and Mary\"\n}\n\nUse only the data provided in the JSON object above.\n\nThe senders of the letter is the bride and groom, Tom and Mary.\n[/INST]\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_length=300, do_sample=True)\n\ndecoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:18:30.093316Z","iopub.execute_input":"2024-03-15T10:18:30.094039Z","iopub.status.idle":"2024-03-15T10:18:36.045679Z","shell.execute_reply.started":"2024-03-15T10:18:30.094007Z","shell.execute_reply":"2024-03-15T10:18:36.044586Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST]\nGiven the following wedding guest data, write a very short 3-sentences thank you letter:\n\n{\n  \"name\": \"John Doe\",\n  \"relationship\": \"Bride's cousin\",\n  \"hometown\": \"New York, NY\",\n  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n  \"attending_with\": \"Sophia Smith\",\n  \"bride_groom_name\": \"Tom and Mary\"\n}\n\nUse only the data provided in the JSON object above.\n\nThe senders of the letter is the bride and groom, Tom and Mary.\n[/INST] Dear John and Sophia,\n\nWe were thrilled to have you both at our wedding. Your presence made the day even more special.\n\nThank you for sharing your fun fact about climbing Mount Everest – that's truly impressive!\n\nBest wishes,\nTom and Mary (Bride and Groom)\n","output_type":"stream"}]},{"cell_type":"code","source":"messages = [{\"role\": \"user\",\"content\": prompt}]\nencoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\noutputs = model.generate(encoded_input, max_length=300, do_sample=True)\ndecoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:19:01.909903Z","iopub.execute_input":"2024-03-15T10:19:01.910646Z","iopub.status.idle":"2024-03-15T10:19:08.808561Z","shell.execute_reply.started":"2024-03-15T10:19:01.910618Z","shell.execute_reply":"2024-03-15T10:19:08.807596Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST] [INST]\nGiven the following wedding guest data, write a very short 3-sentences thank you letter:\n\n{\n  \"name\": \"John Doe\",\n  \"relationship\": \"Bride's cousin\",\n  \"hometown\": \"New York, NY\",\n  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n  \"attending_with\": \"Sophia Smith\",\n  \"bride_groom_name\": \"Tom and Mary\"\n}\n\nUse only the data provided in the JSON object above.\n\nThe senders of the letter is the bride and groom, Tom and Mary.\n[/INST] [/INST] Dear John,\nThank you for joining us on our special day in New York. Your presence, as the bride's courageous cousin, added to the joy and happiness of the occasion. We were thrilled to hear about your amazing achievement of climbing Mount Everest in 2020.\nWarm regards,\nTom and Mary, the Bride and Groom.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nml_papers=pd.read_csv(os.getenv(\"DATASET\"), header=0)\nml_papers.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:20:32.032092Z","iopub.execute_input":"2024-03-15T10:20:32.032793Z","iopub.status.idle":"2024-03-15T10:20:32.060600Z","shell.execute_reply.started":"2024-03-15T10:20:32.032752Z","shell.execute_reply":"2024-03-15T10:20:32.059688Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 420 entries, 0 to 419\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   Title        420 non-null    object\n 1   Description  415 non-null    object\n 2   PaperURL     420 non-null    object\n 3   TweetURL     416 non-null    object\n 4   Abstract     414 non-null    object\ndtypes: object(5)\nmemory usage: 16.5+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"def check_df(df):\n    print(\"############# Shape #############\")\n    print(df.shape)\n    print(\"############# Types #############\")\n    print(df.dtypes)\n    print(\"############# NA #############\")\n    print(df.isnull().sum())\n    print(\"############# Quantiles #############\")\n    numeric_columns=df.select_dtypes(include=['number']).columns\n    # return values at the given quantile over requested axis\n    print(df[numeric_columns].quantile([0,0.05,0.50,0.95,0.99], 1).T)\n    \n\ncheck_df(ml_papers)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:20:42.025686Z","iopub.execute_input":"2024-03-15T10:20:42.026569Z","iopub.status.idle":"2024-03-15T10:20:42.048024Z","shell.execute_reply.started":"2024-03-15T10:20:42.026535Z","shell.execute_reply":"2024-03-15T10:20:42.047110Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"############# Shape #############\n(420, 5)\n############# Types #############\nTitle          object\nDescription    object\nPaperURL       object\nTweetURL       object\nAbstract       object\ndtype: object\n############# NA #############\nTitle          0\nDescription    5\nPaperURL       0\nTweetURL       4\nAbstract       6\ndtype: int64\n############# Quantiles #############\n     0.00  0.05  0.50  0.95  0.99\n0     NaN   NaN   NaN   NaN   NaN\n1     NaN   NaN   NaN   NaN   NaN\n2     NaN   NaN   NaN   NaN   NaN\n3     NaN   NaN   NaN   NaN   NaN\n4     NaN   NaN   NaN   NaN   NaN\n..    ...   ...   ...   ...   ...\n415   NaN   NaN   NaN   NaN   NaN\n416   NaN   NaN   NaN   NaN   NaN\n417   NaN   NaN   NaN   NaN   NaN\n418   NaN   NaN   NaN   NaN   NaN\n419   NaN   NaN   NaN   NaN   NaN\n\n[420 rows x 5 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# remove rows with empty titles to descriptions\nml_papers=ml_papers.dropna(subset=[\"Title\",\"Description\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:20:49.135852Z","iopub.execute_input":"2024-03-15T10:20:49.136676Z","iopub.status.idle":"2024-03-15T10:20:49.145984Z","shell.execute_reply.started":"2024-03-15T10:20:49.136646Z","shell.execute_reply":"2024-03-15T10:20:49.144802Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"ml_papers.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:20:52.891313Z","iopub.execute_input":"2024-03-15T10:20:52.891723Z","iopub.status.idle":"2024-03-15T10:20:52.905621Z","shell.execute_reply.started":"2024-03-15T10:20:52.891692Z","shell.execute_reply":"2024-03-15T10:20:52.904748Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                               Title  \\\n0                                             Llemma   \n1                      LLMs for Software Engineering   \n2                                           Self-RAG   \n3  Retrieval-Augmentation for Long-form Question ...   \n4                                           GenBench   \n\n                                         Description  \\\n0  an LLM for mathematics which is based on conti...   \n1  a comprehensive survey of LLMs for software en...   \n2  presents a new retrieval-augmented framework t...   \n3  explores retrieval-augmented language models o...   \n4  presents a framework for characterizing and un...   \n\n                                            PaperURL  \\\n0                   https://arxiv.org/abs/2310.10631   \n1                   https://arxiv.org/abs/2310.03533   \n2                   https://arxiv.org/abs/2310.11511   \n3                   https://arxiv.org/abs/2310.12150   \n4  https://www.nature.com/articles/s42256-023-007...   \n\n                                            TweetURL  \\\n0  https://x.com/zhangir_azerbay/status/171409802...   \n1  https://x.com/omarsar0/status/1713940983199506...   \n2  https://x.com/AkariAsai/status/171511027707796...   \n3  https://x.com/omarsar0/status/1714986431859282...   \n4  https://x.com/AIatMeta/status/1715041427283902...   \n\n                                            Abstract  \n0  We present Llemma, a large language model for ...  \n1  This paper provides a survey of the emerging a...  \n2  Despite their remarkable capabilities, large l...  \n3  We present a study of retrieval-augmented lang...  \n4                                                NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Description</th>\n      <th>PaperURL</th>\n      <th>TweetURL</th>\n      <th>Abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Llemma</td>\n      <td>an LLM for mathematics which is based on conti...</td>\n      <td>https://arxiv.org/abs/2310.10631</td>\n      <td>https://x.com/zhangir_azerbay/status/171409802...</td>\n      <td>We present Llemma, a large language model for ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LLMs for Software Engineering</td>\n      <td>a comprehensive survey of LLMs for software en...</td>\n      <td>https://arxiv.org/abs/2310.03533</td>\n      <td>https://x.com/omarsar0/status/1713940983199506...</td>\n      <td>This paper provides a survey of the emerging a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Self-RAG</td>\n      <td>presents a new retrieval-augmented framework t...</td>\n      <td>https://arxiv.org/abs/2310.11511</td>\n      <td>https://x.com/AkariAsai/status/171511027707796...</td>\n      <td>Despite their remarkable capabilities, large l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Retrieval-Augmentation for Long-form Question ...</td>\n      <td>explores retrieval-augmented language models o...</td>\n      <td>https://arxiv.org/abs/2310.12150</td>\n      <td>https://x.com/omarsar0/status/1714986431859282...</td>\n      <td>We present a study of retrieval-augmented lang...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GenBench</td>\n      <td>presents a framework for characterizing and un...</td>\n      <td>https://www.nature.com/articles/s42256-023-007...</td>\n      <td>https://x.com/AIatMeta/status/1715041427283902...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pre-processing Data\n\nWe convert dataframe to list of dicts with Title and Description columns only. Furthermore, we will use SentenceTransformer to generate embeddings for the data and store it to the vector dataset.","metadata":{}},{"cell_type":"code","source":"ml_papers_dict=ml_papers.to_dict(orient=\"records\")\nml_papers_dict[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:21:04.608805Z","iopub.execute_input":"2024-03-15T10:21:04.609423Z","iopub.status.idle":"2024-03-15T10:21:04.622727Z","shell.execute_reply.started":"2024-03-15T10:21:04.609393Z","shell.execute_reply":"2024-03-15T10:21:04.621720Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'Title': 'Llemma',\n 'Description': 'an LLM for mathematics which is based on continued pretraining from Code Llama on the Proof-Pile-2 dataset; the dataset involves scientific paper, web data containing mathematics, and mathematical code; Llemma outperforms open base models and the unreleased Minerva on the MATH benchmark; the model is released, including dataset and code to replicate experiments.',\n 'PaperURL': 'https://arxiv.org/abs/2310.10631',\n 'TweetURL': 'https://x.com/zhangir_azerbay/status/1714098025956864031?s=20',\n 'Abstract': 'We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading SentenceTransformer","metadata":{}},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom sentence_transformers import SentenceTransformer\n\nencoder=SentenceTransformer(\"all-MiniLM-L12-v2\")\nencoder.max_seq_length=256\n\nclass Embed(EmbeddingFunction):\n    def __call__(self, input: Documents)-> Embeddings:\n        batch_embeddings=encoder.encode(input, show_progress_bar=True, device=\"cuda\")\n        return batch_embeddings.tolist()\n\nembed=Embed()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:23:46.051746Z","iopub.execute_input":"2024-03-15T10:23:46.052628Z","iopub.status.idle":"2024-03-15T10:23:47.164168Z","shell.execute_reply.started":"2024-03-15T10:23:46.052598Z","shell.execute_reply":"2024-03-15T10:23:47.163165Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Initialize Vector DB ","metadata":{}},{"cell_type":"code","source":"import chromadb\n\nname=f\"ml-papers-nov-2023\"\n# initialize the chromadb directory, and client\nclient=chromadb.PersistentClient(path=\"./chromadb\")\n\ncollection=client.get_or_create_collection(name=name)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:21:36.879385Z","iopub.execute_input":"2024-03-15T10:21:36.880138Z","iopub.status.idle":"2024-03-15T10:21:37.355307Z","shell.execute_reply.started":"2024-03-15T10:21:36.880102Z","shell.execute_reply":"2024-03-15T10:21:37.354514Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Generate Embeddings\n\nWe generate embeddings and index titles in batches.","metadata":{}},{"cell_type":"code","source":"import random\nfrom tqdm import tqdm\n\nbatch_size=50\n\n# loop through batches and generated +store embeddings\nfor i in tqdm(range(0, len(ml_papers_dict), batch_size)):\n    i_end=min(i+batch_size, len(ml_papers_dict))\n    batch=ml_papers_dict[i:i+batch_size]\n    \n    # replace title with \"No Title\" if empty string\n    batch_titles=[str(paper[\"Title\"]) if str(paper[\"Title\"]) != \"\" else \"No Title\" for paper in batch]\n    batch_ids=[str(sum(ord(c)+random.randint(1, 10000) for c in paper[\"Title\"])) for paper in batch]\n    batch_metadata=[dict(url=paper[\"PaperURL\"], abstract=paper['Abstract']) for paper in batch]\n    \n    # generate embeddings\n    batch_embeddings=encoder.encode(batch_titles)\n    \n    collection.upsert(\n        ids=batch_ids,\n        metadatas=batch_metadata,\n        documents=batch_titles,\n        embeddings=batch_embeddings.tolist()\n    )  ","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:22:55.655626Z","iopub.execute_input":"2024-03-15T10:22:55.656000Z","iopub.status.idle":"2024-03-15T10:22:56.766387Z","shell.execute_reply.started":"2024-03-15T10:22:55.655971Z","shell.execute_reply":"2024-03-15T10:22:56.765373Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"  0%|          | 0/9 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56697f3e9ab84b549b9e60ead25c3212"}},"metadata":{}},{"name":"stderr","text":" 11%|█         | 1/9 [00:00<00:01,  7.88it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd62be35ce814e989d93acd375056a22"}},"metadata":{}},{"name":"stderr","text":" 22%|██▏       | 2/9 [00:00<00:00,  8.04it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d67f2a41c0472c8c61df2fc230fbac"}},"metadata":{}},{"name":"stderr","text":" 33%|███▎      | 3/9 [00:00<00:00,  8.31it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee0dc169345a435bb41994c04416d62f"}},"metadata":{}},{"name":"stderr","text":" 44%|████▍     | 4/9 [00:00<00:00,  8.28it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c585af1dc16439293d97e984f705166"}},"metadata":{}},{"name":"stderr","text":" 56%|█████▌    | 5/9 [00:00<00:00,  7.85it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c604af191d4d0c8db5e05e7fd655ea"}},"metadata":{}},{"name":"stderr","text":" 67%|██████▋   | 6/9 [00:00<00:00,  7.62it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b61804db04e148c590dc3af007843c0a"}},"metadata":{}},{"name":"stderr","text":" 78%|███████▊  | 7/9 [00:00<00:00,  7.62it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026cd533ee434bc1b85c981cf5359dd5"}},"metadata":{}},{"name":"stderr","text":" 89%|████████▉ | 8/9 [00:01<00:00,  7.50it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a803d0fc43ac45a6a89940e22eb4ff50"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 9/9 [00:01<00:00,  8.18it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing the Retriever","metadata":{}},{"cell_type":"code","source":"collection=client.get_or_create_collection(name=name, embedding_function=embed)\n\nretriever_results=collection.query(query_texts=[\"Software Engineering\"], n_results=2)\n\nprint(retriever_results[\"documents\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:24:01.501333Z","iopub.execute_input":"2024-03-15T10:24:01.502020Z","iopub.status.idle":"2024-03-15T10:24:01.543227Z","shell.execute_reply.started":"2024-03-15T10:24:01.501985Z","shell.execute_reply":"2024-03-15T10:24:01.542356Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a5af86e78d49c79fd0de6139068e0b"}},"metadata":{}},{"name":"stdout","text":"[['LLMs for Software Engineering', 'Communicative Agents for Software Development']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"query = \"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models\"\n\n# query for user query\nresults=collection.query(query_texts=[prompt], n_results=10)\n\n# concatenate titles into a single string\nshort_titles='\\n'.join(results['documents'][0])\n\n\n\nprompt = f'''[INST]\n\nYour main task is to generate 5 SUGGESTED_TITLES based for the PAPER_TITLE\n\nYou should mimic a similar style and length as SHORT_TITLES but PLEASE DO NOT include titles from SHORT_TITLES in the SUGGESTED_TITLES, only generate versions of the PAPER_TILE.\n\nPAPER_TITLE: {query}\n\nSHORT_TITLES: {short_titles}\n\nSUGGESTED_TITLES:\n\n[/INST]\n'''\n\nencoded_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**encoded_input, max_length=2000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\ndecoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T10:28:15.709954Z","iopub.execute_input":"2024-03-15T10:28:15.710566Z","iopub.status.idle":"2024-03-15T10:28:24.329414Z","shell.execute_reply.started":"2024-03-15T10:28:15.710531Z","shell.execute_reply":"2024-03-15T10:28:24.328472Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2832a8a44d1468a990df91bbba7652d"}},"metadata":{}},{"name":"stdout","text":"[INST]\n\nYour main task is to generate 5 SUGGESTED_TITLES based for the PAPER_TITLE\n\nYou should mimic a similar style and length as SHORT_TITLES but PLEASE DO NOT include titles from SHORT_TITLES in the SUGGESTED_TITLES, only generate versions of the PAPER_TILE.\n\nPAPER_TITLE: S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models\n\nSHORT_TITLES: ChemCrow: Augmenting large-language models with chemistry tools\nEmergent autonomous scientific research capabilities of large language models\nMusicLM: Generating Music From Text\nA Survey of Large Language Models\nPythia: A Suite for Analyzing Large Language Models Across Training and Scaling\nEight Things to Know about Large Language Models\nA Watermark for Large Language Models\nAugmented Language Models: a Survey\nREPLUG: Retrieval-Augmented Black-Box Language Models\nCrowd Workers Widely Use Large Language Models for Text Production Tasks\n\nSUGGESTED_TITLES:\n\n[/INST]\n1. ScaleScore: Evaluating Large Language Models with Synthetic Data\n2. S3Model benchmarks: Systematic Assessment of Scaled Language Models\n3. SyntheticScores: A Framework for Measuring Large Language Model Performance\n4. Evaluating Large Language Models through Synthetic Systems\n5. S3Validation: Assessing the Scalability and Reliability of Large Language Models\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-rag.ipynb\n* https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main\n* https://www.sbert.net/docs/quickstart.html\n* https://www.kaggle.com/code/aisuko/semantic-search-in-publications\n* https://www.kaggle.com/code/aisuko/titanic-question-with-tf-decision-forests\n* https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/SentenceTransformer.py\n* https://www.kaggle.com/code/aisuko/llm-prompt-recovery-with-gemma\n","metadata":{}}]}