{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will use building a RAG system that suggests short and easy to read ML paper titles from original ML paper titles. Our use case is that the paper tiles can be too technical for a general audience so using RAG to generate short titles based on previously created short titles can make research paper titles more accessible and used for science communication such as in the form of newsletters or blogs.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.38.2\n!pip install accelerate==0.27.2\n# !pip install datasets==2.18.0\n!pip install peft==0.9.0\n!pip install bitsandbytes==0.42.0\n!pip install sentence-transformers==2.5.1\n!pip install chromadb==0.4.24","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:10:38.893776Z","iopub.execute_input":"2024-03-15T07:10:38.894096Z","iopub.status.idle":"2024-03-15T07:12:32.350815Z","shell.execute_reply.started":"2024-03-15T07:10:38.894071Z","shell.execute_reply":"2024-03-15T07:12:32.349647Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\n\nos.environ[\"MODEL_NAME\"] = \"google/gemma-2b-it\"\nos.environ[\"DATASET\"]=\"/kaggle/input/weekly-top-trending-ml-papers/ml-potw-10232023.csv\"\n\n\ntorch.backends.cudnn.deterministic=True\n# https://github.com/huggingface/transformers/issues/28731\ntorch.backends.cuda.enable_mem_efficient_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:12:32.353213Z","iopub.execute_input":"2024-03-15T07:12:32.353618Z","iopub.status.idle":"2024-03-15T07:12:35.148555Z","shell.execute_reply.started":"2024-03-15T07:12:32.353585Z","shell.execute_reply":"2024-03-15T07:12:35.147658Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:12:35.149638Z","iopub.execute_input":"2024-03-15T07:12:35.149966Z","iopub.status.idle":"2024-03-15T07:12:44.944909Z","shell.execute_reply.started":"2024-03-15T07:12:35.149944Z","shell.execute_reply":"2024-03-15T07:12:44.943726Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading pretrained config for `google/gemma-2b-it` from `transformers`...\nconfig.json: 100%|█████████████████████████████| 627/627 [00:00<00:00, 3.23MB/s]\n┌────────────────────────────────────────────────────┐\n│   Memory Usage for loading `google/gemma-2b-it`    │\n├───────┬─────────────┬──────────┬───────────────────┤\n│ dtype │Largest Layer│Total Size│Training using Adam│\n├───────┼─────────────┼──────────┼───────────────────┤\n│float32│   1.95 GB   │ 9.34 GB  │      37.38 GB     │\n│float16│  1000.0 MB  │ 4.67 GB  │      18.69 GB     │\n│  int8 │   500.0 MB  │ 2.34 GB  │      9.34 GB      │\n│  int4 │   250.0 MB  │ 1.17 GB  │      4.67 GB      │\n└───────┴─────────────┴──────────┴───────────────────┘\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(os.getenv(\"MODEL_NAME\"))\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:12:44.947331Z","iopub.execute_input":"2024-03-15T07:12:44.947662Z","iopub.status.idle":"2024-03-15T07:12:48.202894Z","shell.execute_reply.started":"2024-03-15T07:12:44.947636Z","shell.execute_reply":"2024-03-15T07:12:48.201914Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ea2802cff9c4c41b9cbf7936330c844"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48185d2fd56848ee953885199de1f44e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a8f06e619248f099d83e644b5ec04e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9663f36cf0f24ff9b0d417aad9a93b1f"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"GemmaTokenizerFast(name_or_path='google/gemma-2b-it', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_enable_fp32_cpu_offload=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\nmodel.config.eos_token_id=tokenizer.eos_token_id\nmodel.gradient_checkpointing_enable() # reducing memory usage\nprint(model.model.embed_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:12:48.203936Z","iopub.execute_input":"2024-03-15T07:12:48.204309Z","iopub.status.idle":"2024-03-15T07:13:08.330380Z","shell.execute_reply.started":"2024-03-15T07:12:48.204284Z","shell.execute_reply":"2024-03-15T07:13:08.329458Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45fadce1577a43ea87b4e2df0eb1b375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770b22214370408dba668b0ef85ef6b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acc7fdc9f65048058c7fb250732b8883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c90ed08376f4b6283a0fd02043972ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea201bcb11f64b05bae8bfd39f98337a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb286fe118041a7aa4935e51fc46313"}},"metadata":{}},{"name":"stdout","text":"Embedding(256000, 2048, padding_idx=0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Checking the Model\n\nTips: Please know that we do not fine-tune the Gemma-2b to fit the specific tasks. So, the answer may not good. However, it is enough for us to illustrate our RAG's solution.\n\nWe give Gemma two \"Text2text\" tasks. The second case we use a chat template for formating the prompt. ","metadata":{}},{"cell_type":"code","source":"input_text = \"The Weather of Melbourne\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_length=20, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:16:38.877965Z","iopub.execute_input":"2024-03-15T07:16:38.878841Z","iopub.status.idle":"2024-03-15T07:16:39.823199Z","shell.execute_reply.started":"2024-03-15T07:16:38.878808Z","shell.execute_reply":"2024-03-15T07:16:39.822277Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"<bos>The Weather of Melbourne is an Australian public station dedicated to providing 24/7 weather,\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"\"\"\nGiven the following wedding guest data, write a very short 3-sentences thank you letter:\n\n{\n  \"name\": \"John Doe\",\n  \"relationship\": \"Bride's cousin\",\n  \"hometown\": \"New York, NY\",\n  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n  \"attending_with\": \"Sophia Smith\",\n  \"bride_groom_name\": \"Tom and Mary\"\n}\n\nUse only the data provided in the JSON object above.\n\nThe senders of the letter is the bride and groom, Tom and Mary.\n\"\"\"\n\nmessages = [{\"role\": \"user\",\"content\": prompt}]\n\nencoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\noutputs = model.generate(encoded_input, max_length=300, do_sample=True)\ndecoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T07:15:54.758382Z","iopub.execute_input":"2024-03-15T07:15:54.759248Z","iopub.status.idle":"2024-03-15T07:15:57.066877Z","shell.execute_reply.started":"2024-03-15T07:15:54.759212Z","shell.execute_reply":"2024-03-15T07:15:57.065916Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"user\nGiven the following wedding guest data, write a very short 3-sentences thank you letter:\n\n{\n  \"name\": \"John Doe\",\n  \"relationship\": \"Bride's cousin\",\n  \"hometown\": \"New York, NY\",\n  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n  \"attending_with\": \"Sophia Smith\",\n  \"bride_groom_name\": \"Tom and Mary\"\n}\n\nUse only the data provided in the JSON object above.\n\nThe senders of the letter is the bride and groom, Tom and Mary.\nmodel\nDear John,\n\nThank you for your involvement in our special day. We're deeply grateful for your friendship and support, and hope you enjoyed the celebration.\n\nSincerely,\nTom and Mary\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nml_papers=pd.read_csv(os.getenv(\"DATASET\"), header=0)\nml_papers.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove rows with empty titles to descriptions\nml_papers=ml_papers.dropna(subset=[\"Title\",\"Description\"])","metadata":{},"execution_count":null,"outputs":[]}]}