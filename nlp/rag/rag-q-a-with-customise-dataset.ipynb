{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7811545,"sourceType":"datasetVersion","datasetId":4575347}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will use our own dataset as the knwoloedge of the RAG. We will load our data from CSV file. Next, loading `RagRetriever` witht the customise data. Finally, we will do some inference under the context of the customize data.\n\n\n# Installing faiss(GPU+CPU)\n\nHere we install Faceboook Faiss which is a library for efficient **similarity search** and **cluster of dense vectors**. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Some of the most useful algorithms are implemented on the GPU. Further more, the GPU implementation can accept input from either CPU or GPU memeory. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!conda install -c pytorch -c nvidia faiss-gpu=1.8.0 -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.38.2\n# !pip install accelerate==0.27.2\n!pip install datasets==2.18.0 # Fix issue numpy attributes error of RagRetriever\n# !pip install peft==0.9.0\n# !pip install bitsandbytes==0.42.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport faiss # for checking faiss-gpu\nimport warnings\n\nos.environ['CSV_PATH']='/kaggle/input/knowledge-dataset/own_knwoledge dataset.csv'\nos.environ['EM_MODEL']='facebook/dpr-ctx_encoder-multiset-base'\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic=True\n    # https://github.com/huggingface/transformers/issues/28731\n    torch.backends.cuda.enable_mem_efficient_sdp(False)\n    device='cuda'\nelse:\n    device='cpu'\n    \nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the Basic Information of Data\n\nThe dataset needed for RAG must have two columns:\n- title(string): title of the document\n- text(string): text of a passage of the document\n\nWe visualization the data and make sure it's format was corrected.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.read_csv(csv_path)\nprint(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Data from CSV","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, load_dataset\nfrom typing import List, Optional\n\n\ndef split_text(text: str, n=100, character=\" \") -> List[str]:\n    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n    text = text.split(character)\n    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n\n\ndef split_documents(documents: dict) -> dict:\n    \"\"\"Split documents into passages\"\"\"\n    titles, texts = [], []\n    for title, text in zip(documents[\"title\"], documents[\"text\"]):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else \"\")\n                texts.append(passage)\n    return {\"title\": titles, \"text\": texts}\n\n\n# Using pandas dataframe without set columnes names will cause issue\n# dataset=Dataset.from_pandas(df, split=\"train\")\n# dataset=dataset.map(split_documents, batched=True, num_proc=4) # 4 vCPUs in Kaggle\n\n\ndataset = load_dataset(\"csv\", data_files=[os.getenv('CSV_PATH')], split=\"train\", delimiter=\"\\t\", column_names=[\"title\", \"text\"])\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding and Computing the Embeddings","metadata":{}},{"cell_type":"code","source":"from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast\n\nctx_encoder=DPRContextEncoder.from_pretrained(os.getenv(\"EM_MODEL\")).to(device)\nctx_tokenizer=DPRContextEncoderTokenizerFast.from_pretrained(os.getenv(\"EM_MODEL\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nfrom datasets import Features, Sequence, Value\n\n\ndef embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    \"\"\"Compute the DPR embeddings of document passages\"\"\"\n    input_ids = ctx_tokenizer(documents[\"title\"], documents[\"text\"], truncation=True, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {\"embeddings\": embeddings.detach().cpu().numpy()}\n    \nnew_features = Features({\n    \"text\": Value(\"string\"), \n    \"title\": Value(\"string\"), \n    \"embeddings\": Sequence(Value(\"float32\"))})  # optional, save as float32 instead of float64 to save space\n\ndataset = dataset.map(\n    partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer),\n    batched=True,\n    batch_size=16,\n    features=new_features,\n)\n\npath_passages=os.path.join('/kaggle/input/',\"my_knowledge_dataset\")\ndataset.save_to_disk(path_passages)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also load the dataset from the local disk.\n\n```python\nfrom datasets import load_from_disk\n\ndataset=load_from_disk(path_passages)\n```","metadata":{}},{"cell_type":"markdown","source":"# Implementation Fast Approximate Nearest Neighbor Search\n\nWe are going to use the Faiss implementation of HNSW for fast approcimate nearest neighbor search.","metadata":{}},{"cell_type":"code","source":"index=faiss.","metadata":{},"execution_count":null,"outputs":[]}]}