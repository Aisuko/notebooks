{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7820769,"sourceType":"datasetVersion","datasetId":4575347}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/rag-q-a-with-customise-dataset?scriptVersionId=166899821\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will use our own dataset as the knwoloedge of the RAG. We will load our data from CSV file. Next, loading `RagRetriever` witht the customise data. Finally, we will do some inference under the context of the customize data.\n\n\n# Installing faiss(GPU+CPU)\n\nHere we install Faceboook Faiss which is a library for efficient **similarity search** and **cluster of dense vectors**. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Some of the most useful algorithms are implemented on the GPU. Further more, the GPU implementation can accept input from either CPU or GPU memeory. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!conda install -c pytorch -c nvidia faiss-gpu=1.8.0 -y","metadata":{"execution":{"iopub.status.busy":"2024-03-12T05:13:22.593902Z","iopub.execute_input":"2024-03-12T05:13:22.594743Z","iopub.status.idle":"2024-03-12T05:16:53.155108Z","shell.execute_reply.started":"2024-03-12T05:13:22.594672Z","shell.execute_reply":"2024-03-12T05:16:53.153769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.38.2\n# !pip install accelerate==0.27.2\n!pip install datasets==2.18.0 # Fix issue numpy attributes error of RagRetriever\n# !pip install peft==0.9.0\n# !pip install bitsandbytes==0.42.0","metadata":{"execution":{"iopub.status.busy":"2024-03-12T05:16:53.157339Z","iopub.execute_input":"2024-03-12T05:16:53.157656Z","iopub.status.idle":"2024-03-12T05:17:37.679939Z","shell.execute_reply.started":"2024-03-12T05:16:53.157628Z","shell.execute_reply":"2024-03-12T05:17:37.67855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport faiss # for checking faiss-gpu\nimport warnings\n\nos.environ['CSV_PATH']='/kaggle/input/knowledge-dataset/own_knwoledge dataset.csv'\nos.environ['CSV_DEMO']='/kaggle/input/knowledge-dataset/my_knowledge_dataset.csv'\nos.environ['EM_MODEL']='facebook/dpr-ctx_encoder-multiset-base'\nos.environ['RAG_MODEL']='facebook/rag-sequence-nq'\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic=True\n    # https://github.com/huggingface/transformers/issues/28731\n    torch.backends.cuda.enable_mem_efficient_sdp(False)\n    device='cuda'\nelse:\n    device='cpu'\n    \nwarnings.filterwarnings('ignore')\n\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T05:18:15.200757Z","iopub.execute_input":"2024-03-12T05:18:15.201771Z","iopub.status.idle":"2024-03-12T05:18:15.208503Z","shell.execute_reply.started":"2024-03-12T05:18:15.201707Z","shell.execute_reply":"2024-03-12T05:18:15.207535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the Basic Information of Data\n\nThe dataset needed for RAG must have two columns:\n- title(string): title of the document\n- text(string): text of a passage of the document\n\nWe visualization the data and make sure it's format was corrected.","metadata":{}},{"cell_type":"code","source":"# Bad case\nimport pandas as pd\n\ndf=pd.read_csv(os.getenv('CSV_PATH'))\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-12T05:18:21.735037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf1=pd.read_csv(os.getenv(\"CSV_DEMO\"))\nprint(df1.shape)\ndf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Data from CSV\n\nWe load the CSV file and split the data into passages of 100 words","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, load_dataset\nfrom typing import List, Optional\n\n\ndef split_text(text: str, n=100, character=\" \") -> List[str]:\n    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n    text = text.split(character)\n    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n\n\ndef split_documents(documents: dict) -> dict:\n    \"\"\"Split documents into passages\"\"\"\n    titles, texts = [], []\n    for title, text in zip(documents[\"title\"], documents[\"text\"]):\n        if text is not None:\n            for passage in split_text(text):\n                titles.append(title if title is not None else \"\")\n                texts.append(passage)\n    return {\"title\": titles, \"text\": texts}\n\n\n# Using pandas dataframe without set columnes names will cause issue\n# dataset=Dataset.from_pandas(df, split=\"train\")\n# dataset=dataset.map(split_documents, batched=True, num_proc=4) # 4 vCPUs in Kaggle\n\n# Spliting the documents into passages of 100 words\ndataset = load_dataset(\"csv\", data_files=[os.getenv('CSV_DEMO')], split=\"train\", delimiter=\"\\t\", column_names=[\"title\", \"text\"])\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Dataset from CSV\n\n","metadata":{}},{"cell_type":"code","source":"from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast\n\nctx_encoder=DPRContextEncoder.from_pretrained(os.getenv(\"EM_MODEL\")).to(device)\nctx_tokenizer=DPRContextEncoderTokenizerFast.from_pretrained(os.getenv(\"EM_MODEL\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nfrom datasets import Features, Sequence, Value\n\n\ndef embed(documents: dict, ctx_encoder: DPRContextEncoder, ctx_tokenizer: DPRContextEncoderTokenizerFast) -> dict:\n    \"\"\"Compute the DPR embeddings of document passages\"\"\"\n    print(documents[\"title\"])\n    print(documents[\"text\"])\n    input_ids = ctx_tokenizer(documents[\"title\"], documents[\"text\"], truncation=True, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n    embeddings = ctx_encoder(input_ids.to(device=device), return_dict=True).pooler_output\n    return {\"embeddings\": embeddings.detach().cpu().numpy()}\n    \nnew_features = Features({\n    \"text\": Value(\"string\"), \n    \"title\": Value(\"string\"), \n    \"embeddings\": Sequence(Value(\"float32\"))})  # optional, save as float32 instead of float64 to save space\n\n\ndataset = dataset.map(\n    partial(embed, ctx_encoder=ctx_encoder, ctx_tokenizer=ctx_tokenizer),\n    batched=True,\n    batch_size=16,\n    features=new_features,\n)\n\ndataset.save_to_disk('my_knowledge_dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also load the dataset from the local disk.\n\n```python\nfrom datasets import load_from_disk\n\ndataset=load_from_disk(path_passages)\n```","metadata":{}},{"cell_type":"markdown","source":"# Index the Dataset\n\nWe are going to use the Faiss implementation of HNSW for fast approcimate nearest neighbor search.","metadata":{}},{"cell_type":"code","source":"dimension=768 # The dimension of the embeddings to pass to the HNSW Faiss index.\nm=128 # The number of bi-directional links created for every new element during the HNSW index construction.\n\nindex=faiss.IndexHNSWFlat(dimension, m, faiss.METRIC_INNER_PRODUCT)\ntype(index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.add_faiss_index(\"embeddings\", custom_index=index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the Index","metadata":{}},{"cell_type":"code","source":"dataset.get_index(\"embeddings\").save(\"my_knowledge_hnsw_index.faiss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading RAG\n\nWe load RagRetriever and RagSequenceForGeneration seperately.","metadata":{}},{"cell_type":"code","source":"from transformers import RagRetriever, RagSequenceForGeneration, RagTokenizer\n\nretriever=RagRetriever.from_pretrained(os.getenv(\"RAG_MODEL\"), index=\"custom\", indexed_dataset=dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=RagSequenceForGeneration.from_pretrained(os.getenv(\"RAG_MODEL\"), retriever=retriever).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=RagTokenizer.from_pretrained(os.getenv(\"RAG_MODEL\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infernece","metadata":{}},{"cell_type":"code","source":"question=\"What does Moses' rod turn into?\"\n\ndef inference(question:str):\n    input_ids=tokenizer.question_encoder(question, return_tensors=\"pt\").to(device)\n    generated=model.generate(input_idsp['input_ids'])\n    generated_str=tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n    return generated_str\n\ninference(question)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question2=\"How many people live in Melbourne?\"\ninference(question2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question3=\"What is the relationship between Nintendo and Pokémon?\"\ninference(question3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question4=\"What does Moses' rod turn into?\"\ninference(question4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question5=\"Where is Pokémon company?\"\ninference(question5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nAs we can see if we want to use our own dataset. We need to make sure our private data can cover same basic knowledge around the world. Otherwise, The answer might be worse than we assumed.","metadata":{}}]}