{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nWe are going to fine-tune Microsoft Phi2 using QLoRA in this notebook.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.36.2\n!pip install accelerate==0.25.0\n!pip install datasets==2.15.0\n!pip install peft==0.7.1\n!pip install bitsandbytes==0.41.3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Microsoft-phi-2\"\nos.environ[\"WANDB_NAME\"] = \"ft-microsoft-phi-2\"\nos.environ[\"MODEL_NAME\"] = \"microsoft/phi-2\"\nos.environ[\"DATASET\"] = \"g-ronimo/riddles_evolved\"\n\ntorch.backends.cudnn.deterministic=True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset=load_dataset(os.getenv(\"DATASET\"), split=\"train[:500]\")\ndataset=dataset.train_test_split(test_size=0.1)\nprint(dataset[\"train\"][0])\ndataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Tokenizer\n\n* Loading tokenizer\n* Adding chatML tokens to tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenier\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv(\"MODEL_NAME\"), use_fast=False)\nprint(len(tokenizer))\n# add chatML tokens to tokenizer\ntokenizer.add_tokens([\"<|im_start|>\",\"<PAD>\"])\ntokenizer.pad_token=\"<PAD>\"\ntokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\nprint(len(tokenizer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Data\n\nWe are going to apply ChatML format and tokenize to the data.","metadata":{}},{"cell_type":"code","source":"from functools import partial\n\ntemplates=[\n    '<|im_start|>assistant\\n{msg}<|im_end|>',\n    '<|im_start|>user\\n{msg}<|im_end|>'\n]\n\nIGNORE_INDEX=-100\n\ndef preprocess_func(input, max_length):\n    input_ids, attention_mask, labels=[],[],[]\n    for i, msg in enumerate(input[\"messages\"]):\n        isHuman=i%2==0\n        msg_chatml=templates[isHuman].format(msg=msg)\n        msg_tokenized=tokenizer(msg_chatml,truncation=False, add_special_tokens=False)\n        \n        input_ids+=msg_tokenized[\"input_ids\"]\n        atention_mask+=msg_tokenized[\"attention_mask\"]\n        labels+=[IGNORE_INDEX]*len(msg_tokenized[\"input_ids\"]) if isHuman else msg_tokenized[\"input_ids\"]\n\n    return {\n        \"input_ids\": input_ids[:max_length],\n        \"attention_mask\": attention_mask[:max_length],\n        \"labels\": labels[:max_length]\n    }\n\ndataset_tokenized=dataset.map(\n    partial(preprocess_func, max_length=1024), # max sample length 1024 tokens, enough for the dataset\n    batched=False,\n    num_proc=os.cpu_count(),\n    remove_columns=dataset[\"train\"].column_names # do not need this anymore, we have tokens from here on\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndata=[len(tok) for tok in (dataset_tokenized[\"train\"][\"input_ids\"]+dataset_tokenized[\"test\"][\"input_ids\"])]\nprint(f\"Longest sample: {max(data)} tokens\")\n\nplt.hist(data, bins=10)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Batch the Data","metadata":{}},{"cell_type":"code","source":"def collate(example):\n    tokens=[e[\"input_ids\"] for e in example]\n    tokens_maxlen=max([len(t) for t in tokens])\n    \n    for i, sample in enumerate(elements):\n        input_ids=sample[\"input_ids\"]\n        labels=sample[\"labels\"]\n        attention_mask=sample[\"attention_mask\"]\n        \n        pad_len=tokens_maxlen-len(input_ids)\n        \n        input_ids.extend(pad_len*[tokenizer.pad_token_id])\n        labels.extend(pad_len*[IGNORE_INDEX])\n        attention_mask.extend(pad_len*[0])\n        \n    batch={\n        \"input_ids\":torch.tensor([e[\"input_ids\"] for e in example]),\n        \"labels\":torch.tensor([e[\"labels\"] for e in example]),\n        \"attention_mask\": torch.tensor([e[\"attention_mask\"] for e in example])\n    }\n    \n    return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Model\n\n* Quantization\n* Free the original weights\n* LoRA\n\nThere is no need to resize the token embeeddings, phi-2 already has embeddings sized for additional tokens. The model's vocabulary size is 51200, this means you can add ~700 tokens to the tokenizer without having to resize the embeddings.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config=BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_enable_fp32_cpu_offload=True,\n)\n\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True, # for phi-2\n)\n\nmodel.config.eos_token_id=tokenizer.eos_token_id\nmodel.gradient_checkpointing_enable() # reducing memory usage\nprint(model.model.embed_tokens)\n\ndef print_trainable_parameters(model):\n    trainable_params=0\n    all_params=0\n    for _, param in model.named_parameters():\n        all_params+=param.numel()\n        if param.requires_grad:\n            trainable_params+=param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params/all_params:.2f}\")\n\nprint_trainable_parameters(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Freeze weigths and apply LoRA","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nprepared_model=prepare_model_for_kbit_training(\n    model, use_gradient_checkpointing=True\n)\n\nprint_trainable_parameters(prepared_model)\nprint(prepared_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\n\nlora_config=LoraConfig(\n    r=32,\n    lora_alpha=32,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'dense'],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"lm_head\",\"embed_tokens\"], # we added new tokens to tokenizer, this is necesarry\n    task_type=TaskType.CAUSAL_LM\n)\n\nlora_model=get_peft_model(prepared_model, lora_config)\nlora_model.config.use_cache=False\nprint_trainable_parameters(lora_model)\nprint(lora_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import set_seed, TrainingArguments, Trainer\n\nset_seed(2024)\n\nbs=4\nbs_eval=16\nga_steps=16\nlr=0.00002\nepochs=3\n\nsteps_per_epoch=len(dataset_tokenized[\"train\"]//(bs*ga_steps))\n\nargs=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    per_device_train_batch_size=bs,\n    per_device_eval_batch_size=bs_eval,\n    evaluation_strategy=\"steps\",\n    logging_steps=1,\n    eval_steps=steps_per_epoch//2,\n    save_steps=steps_per_epoch,\n    gradient_accumulation_steps=ga_steps,\n    num_train_epochs=epochs,\n    lr_scheduler_type=\"constant\",\n    optim=\"paged_adamw_32bit\", # val_loss will go nan with paged_adamw_8bit\n    leanring_rate=lr,\n    group_by_length=False,\n    fp16=True,\n    ddp_find_unused_parameters=False\n)\n\ntrainer=Trainer(\n    model=lora_model,\n    tokenizer=tokenizer,\n    args=args,\n    data_collator=collate,\n    train_dataset=dataset_tokenized[\"train\"],\n    eval_dataset=dataset_tokenized[\"test\"]\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs={\n    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n    'finetuned_from': os.getenv('MODEL_NAME'),\n#     'tasks': '',\n#     'dataset_tags':'',\n    'dataset': os.getenv(\"DATASET\")\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import gc\n\ndel tokenizer, lora_model, prepared_model, model\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained(os.getenv(\"MODEL_NAME\"), use_fast=False)\ntokenizer.add_tokens([\"<|im_start|>\",\"<PAD>\"])\ntokenizer.pad_token=\"<PAD>\"\ntokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n\ntokenizer.chat_template=\"{% if not add_generation_prompt is defined%}{% set add_generation_prompt=false%}{%endif%}{%for message in messages%}{{'<|im_start|>'+message['role']+message['content']+'<|im_end|>'}}{%endfor%}{%if add_generation_prompt%}{{'<|im_start|>assistant'}}{%endif%}\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=AutoModelForCausalLM.from_pretrained(os.getenv(\"MODEL_NAME\"), torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.config.eos_token_id=tokenizer.eos_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel\n\ngeneration_config=GenerationConfig(\n    max_new_tokens=100,\n    temperature=0.7,\n    top_p=0.1,\n    top_k=40,\n    repetition_penalty=1.18,\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_pad_id\n)\n\nmodel=PeftModel.from_pretrained(model, os.getenv(\"WANDB_NAME\"))\nmodel=model.merge_and_unload()\n\nmodel.save_pretrained(os.getenv(\"WANDB_NAME\")+\"-merged\", safe_serialization=True, max_shared_size=\"4GB\")\ntokenizer.save_pretrained(os.getenv(\"WANDB_NAME\")+\"-merged\")\ngeneration_config.save_pretrined(os.getenv(\"WANDB_NAME\")+\"-merged\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Credit\n\n* https://medium.com/@geronimo7/phinetuning-2-0-28a2be6de110\n* https://github.com/geronimi73/phi2-finetune/blob/main/nb_qlora.ipynb","metadata":{}}]}