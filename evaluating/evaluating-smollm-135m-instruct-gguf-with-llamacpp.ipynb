{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332fa1fe",
   "metadata": {
    "papermill": {
     "duration": 0.004402,
     "end_time": "2024-09-26T08:14:18.618442",
     "exception": false,
     "start_time": "2024-09-26T08:14:18.614040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "This note evaluate smolLM-135M instruct model with llama.cpp\n",
    "\n",
    "# Env and tools prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068f5457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:14:18.627195Z",
     "iopub.status.busy": "2024-09-26T08:14:18.626902Z",
     "iopub.status.idle": "2024-09-26T08:36:08.967343Z",
     "shell.execute_reply": "2024-09-26T08:36:08.966014Z"
    },
    "papermill": {
     "duration": 1310.34772,
     "end_time": "2024-09-26T08:36:08.970067",
     "exception": false,
     "start_time": "2024-09-26T08:14:18.622347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\r\n",
      "remote: Enumerating objects: 34776, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (6485/6485), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (354/354), done.\u001b[K\r\n",
      "remote: Total 34776 (delta 6297), reused 6200 (delta 6130), pack-reused 28291 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (34776/34776), 56.54 MiB | 14.92 MiB/s, done.\r\n",
      "Resolving deltas: 100% (25233/25233), done.\r\n",
      "/kaggle/working/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "%cd llama.cpp\n",
    "# !export GGML_CUDA=1 \n",
    "!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ \n",
    "!make GGML_CUDA=1 CUDA_PATH=/usr/local/nvidia  > make.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05af251e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:36:08.993071Z",
     "iopub.status.busy": "2024-09-26T08:36:08.992727Z",
     "iopub.status.idle": "2024-09-26T08:38:42.178053Z",
     "shell.execute_reply": "2024-09-26T08:38:42.176914Z"
    },
    "papermill": {
     "duration": 153.199319,
     "end_time": "2024-09-26T08:38:42.180371",
     "exception": false,
     "start_time": "2024-09-26T08:36:08.981052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\r\n",
      "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  ggml/src/llamafile/sgemm.o ggml/src/ggml-cuda.o ggml/src/ggml-cuda/acc.o ggml/src/ggml-cuda/arange.o ggml/src/ggml-cuda/argsort.o ggml/src/ggml-cuda/binbcast.o ggml/src/ggml-cuda/clamp.o ggml/src/ggml-cuda/concat.o ggml/src/ggml-cuda/conv-transpose-1d.o ggml/src/ggml-cuda/convert.o ggml/src/ggml-cuda/cpy.o ggml/src/ggml-cuda/cross-entropy-loss.o ggml/src/ggml-cuda/diagmask.o ggml/src/ggml-cuda/dmmv.o ggml/src/ggml-cuda/fattn-tile-f16.o ggml/src/ggml-cuda/fattn-tile-f32.o ggml/src/ggml-cuda/fattn.o ggml/src/ggml-cuda/getrows.o ggml/src/ggml-cuda/im2col.o ggml/src/ggml-cuda/mmq.o ggml/src/ggml-cuda/mmvq.o ggml/src/ggml-cuda/norm.o ggml/src/ggml-cuda/opt-step-adamw.o ggml/src/ggml-cuda/out-prod.o ggml/src/ggml-cuda/pad.o ggml/src/ggml-cuda/pool2d.o ggml/src/ggml-cuda/quantize.o ggml/src/ggml-cuda/rope.o ggml/src/ggml-cuda/rwkv-wkv.o ggml/src/ggml-cuda/scale.o ggml/src/ggml-cuda/softmax.o ggml/src/ggml-cuda/sum.o ggml/src/ggml-cuda/sumrows.o ggml/src/ggml-cuda/tsembd.o ggml/src/ggml-cuda/unary.o ggml/src/ggml-cuda/upscale.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \r\n",
      "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/gen-docs/gen-docs.cpp -o examples/gen-docs/gen-docs.o\r\n",
      "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  ggml/src/llamafile/sgemm.o ggml/src/ggml-cuda.o ggml/src/ggml-cuda/acc.o ggml/src/ggml-cuda/arange.o ggml/src/ggml-cuda/argsort.o ggml/src/ggml-cuda/binbcast.o ggml/src/ggml-cuda/clamp.o ggml/src/ggml-cuda/concat.o ggml/src/ggml-cuda/conv-transpose-1d.o ggml/src/ggml-cuda/convert.o ggml/src/ggml-cuda/cpy.o ggml/src/ggml-cuda/cross-entropy-loss.o ggml/src/ggml-cuda/diagmask.o ggml/src/ggml-cuda/dmmv.o ggml/src/ggml-cuda/fattn-tile-f16.o ggml/src/ggml-cuda/fattn-tile-f32.o ggml/src/ggml-cuda/fattn.o ggml/src/ggml-cuda/getrows.o ggml/src/ggml-cuda/im2col.o ggml/src/ggml-cuda/mmq.o ggml/src/ggml-cuda/mmvq.o ggml/src/ggml-cuda/norm.o ggml/src/ggml-cuda/opt-step-adamw.o ggml/src/ggml-cuda/out-prod.o ggml/src/ggml-cuda/pad.o ggml/src/ggml-cuda/pool2d.o ggml/src/ggml-cuda/quantize.o ggml/src/ggml-cuda/rope.o ggml/src/ggml-cuda/rwkv-wkv.o ggml/src/ggml-cuda/scale.o ggml/src/ggml-cuda/softmax.o ggml/src/ggml-cuda/sum.o ggml/src/ggml-cuda/sumrows.o ggml/src/ggml-cuda/tsembd.o ggml/src/ggml-cuda/unary.o ggml/src/ggml-cuda/upscale.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/gen-docs/gen-docs.o -o llama-gen-docs -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \r\n",
      "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\r\n",
      "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/deprecation-warning/deprecation-warning.cpp -o examples/deprecation-warning/deprecation-warning.o\r\n",
      "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  examples/deprecation-warning/deprecation-warning.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \r\n",
      "NOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.\r\n",
      "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  examples/deprecation-warning/deprecation-warning.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \r\n",
      "NOTICE: The 'server' binary is deprecated. Please use 'llama-server' instead.\r\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\r\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "rapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\r\n",
      "rmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\r\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.9.0 which is incompatible.\r\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.9.0 gguf-0.10.0 huggingface-hub-0.25.1 idna-3.10 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 packaging-24.1 protobuf-4.25.3 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 sympy-1.13.3 tokenizers-0.20.0 torch-2.2.2+cpu tqdm-4.66.5 transformers-4.45.0 typing-extensions-4.12.2 urllib3-2.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!tail make.log\n",
    "!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n",
    "!tail pip_install.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10d187",
   "metadata": {
    "papermill": {
     "duration": 0.010635,
     "end_time": "2024-09-26T08:38:42.203494",
     "exception": false,
     "start_time": "2024-09-26T08:38:42.192859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dowload wiki text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca0f53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:38:42.226827Z",
     "iopub.status.busy": "2024-09-26T08:38:42.225997Z",
     "iopub.status.idle": "2024-09-26T08:39:53.669585Z",
     "shell.execute_reply": "2024-09-26T08:39:53.668686Z"
    },
    "papermill": {
     "duration": 71.457651,
     "end_time": "2024-09-26T08:39:53.671757",
     "exception": false,
     "start_time": "2024-09-26T08:38:42.214106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-26 08:38:43--  https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\r\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.19, 86.50.254.18\r\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.19|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 532958396 (508M) [application/gzip]\r\n",
      "Saving to: 'en.txt.gz'\r\n",
      "\r\n",
      "en.txt.gz           100%[===================>] 508.27M  9.78MB/s    in 52s     \r\n",
      "\r\n",
      "2024-09-26 08:39:36 (9.80 MB/s) - 'en.txt.gz' saved [532958396/532958396]\r\n",
      "\r\n",
      "--2024-09-26 08:39:52--  https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip\r\n",
      "Resolving huggingface.co (huggingface.co)... 108.158.20.65, 108.158.20.31, 108.158.20.78, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|108.158.20.65|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727599192&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzU5OTE5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=uDmN5J4aqqcH7f9IYyWfjAAC8syejDjsidnGcSLrdHGrTQm7920KD0E6I4Ca-qZGoUrYk5vp8S6ESXqxFXXjMSBW099NiDHUMIihVoj2KSooTqby10twm5fyG9KBMvdPGDhCNgpk1Tg7FX-tOrAdT6mfL0yOpPW8sUh50eAwEq6xG3H4hN6FDuBHMixLED9pYtM3b5DisvEpWZkI1MJTnRt35XKPCabhfX3RbhOrr2cKHYqTvkbeKEMuQJq87GXT3vqdY9IYWouxUgQdg2mXfQxSKNFqCodKPqnoEZxzyCqAsTZ230M0MJ9uQSMmiMXV7iS-XDxAYEQgeUINqAAtIA__&Key-Pair-Id=K24J24Z295AEI9 [following]\r\n",
      "--2024-09-26 08:39:52--  https://cdn-lfs-us-1.hf.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8''wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727599192&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzU5OTE5Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=uDmN5J4aqqcH7f9IYyWfjAAC8syejDjsidnGcSLrdHGrTQm7920KD0E6I4Ca-qZGoUrYk5vp8S6ESXqxFXXjMSBW099NiDHUMIihVoj2KSooTqby10twm5fyG9KBMvdPGDhCNgpk1Tg7FX-tOrAdT6mfL0yOpPW8sUh50eAwEq6xG3H4hN6FDuBHMixLED9pYtM3b5DisvEpWZkI1MJTnRt35XKPCabhfX3RbhOrr2cKHYqTvkbeKEMuQJq87GXT3vqdY9IYWouxUgQdg2mXfQxSKNFqCodKPqnoEZxzyCqAsTZ230M0MJ9uQSMmiMXV7iS-XDxAYEQgeUINqAAtIA__&Key-Pair-Id=K24J24Z295AEI9\r\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 108.158.20.76, 108.158.20.89, 108.158.20.87, ...\r\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|108.158.20.76|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 4721645 (4.5M) [application/zip]\r\n",
      "Saving to: 'wikitext-2-raw-v1.zip'\r\n",
      "\r\n",
      "wikitext-2-raw-v1.z 100%[===================>]   4.50M  10.5MB/s    in 0.4s    \r\n",
      "\r\n",
      "2024-09-26 08:39:53 (10.5 MB/s) - 'wikitext-2-raw-v1.zip' saved [4721645/4721645]\r\n",
      "\r\n",
      "Archive:  wikitext-2-raw-v1.zip\r\n",
      "   creating: wikitext-2-raw/\r\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \r\n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \r\n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \r\n",
      "Usage:\r\n",
      "\r\n",
      "  ./llama-perplexity -m model.gguf -f wikitext-2-raw/wiki.test.raw [other params]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\n",
    "!gunzip en.txt.gz\n",
    "!head -n 10000 en.txt > en-h10000.txt\n",
    "!sh /kaggle/working/llama.cpp/scripts/get-wikitext-2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d896d38",
   "metadata": {
    "papermill": {
     "duration": 0.024053,
     "end_time": "2024-09-26T08:39:53.720247",
     "exception": false,
     "start_time": "2024-09-26T08:39:53.696194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run perplexity\n",
    "\n",
    "Perplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\n",
    "Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n",
    "\n",
    "Perplexity benchmark shows how accurate the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d5c230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:39:53.769957Z",
     "iopub.status.busy": "2024-09-26T08:39:53.769588Z",
     "iopub.status.idle": "2024-09-26T08:42:35.076418Z",
     "shell.execute_reply": "2024-09-26T08:42:35.075200Z"
    },
    "papermill": {
     "duration": 161.334728,
     "end_time": "2024-09-26T08:42:35.079033",
     "exception": false,
     "start_time": "2024-09-26T08:39:53.744305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_V2.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > pip_fp16.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8545e694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:42:35.129949Z",
     "iopub.status.busy": "2024-09-26T08:42:35.129562Z",
     "iopub.status.idle": "2024-09-26T08:42:38.059838Z",
     "shell.execute_reply": "2024-09-26T08:42:38.058759Z"
    },
    "papermill": {
     "duration": 2.958051,
     "end_time": "2024-09-26T08:42:38.062006",
     "exception": false,
     "start_time": "2024-09-26T08:42:35.103955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final estimate: PPL = 23.2651 +/- 0.28000\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    1295.16 ms\r\n",
      "llama_perf_context_print: prompt eval time =   28096.28 ms / 131072 tokens (    0.21 ms per token,  4665.10 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   47355.83 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 23.2651 +/- 0.28000\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    1590.96 ms\r\n",
      "llama_perf_context_print: prompt eval time =   28008.81 ms / 131072 tokens (    0.21 ms per token,  4679.67 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   47313.35 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 21.2879 +/- 0.25264\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    2513.20 ms\r\n",
      "llama_perf_context_print: prompt eval time =   37893.68 ms / 131072 tokens (    0.29 ms per token,  3458.94 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   57083.19 ms / 131073 tokens\r\n"
     ]
    }
   ],
   "source": [
    "!tail -6 ppl_Q4_V2.log\n",
    "!tail -6 ppl_Q4.log\n",
    "!tail -6 pip_fp16.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57664b0f",
   "metadata": {
    "papermill": {
     "duration": 0.024358,
     "end_time": "2024-09-26T08:42:38.111524",
     "exception": false,
     "start_time": "2024-09-26T08:42:38.087166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking the Inference Throughput and Memory Consumption \n",
    "\n",
    "Llama-bench shows performance and resource consumption. \n",
    "There are three types of test\n",
    "- Prompt processing (pp): processing a prompt in batches (-p)\n",
    "- Text generation (tg): generating a sequence of tokens (-n)\n",
    "- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n",
    "\n",
    "In the following test, we run text generation and prompt processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a7e157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:42:38.162080Z",
     "iopub.status.busy": "2024-09-26T08:42:38.161722Z",
     "iopub.status.idle": "2024-09-26T08:50:03.887921Z",
     "shell.execute_reply": "2024-09-26T08:50:03.886982Z"
    },
    "papermill": {
     "duration": 445.754037,
     "end_time": "2024-09-26T08:50:03.890183",
     "exception": false,
     "start_time": "2024-09-26T08:42:38.136146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\n",
      "ggml_cuda_init: found 1 CUDA devices:\r\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\r\n",
      "| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\r\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |     7880.33 ± 221.36 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         90.59 ± 1.33 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         87.86 ± 4.58 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         84.72 ± 1.88 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9498.40 ± 223.11 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        114.26 ± 1.19 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        113.61 ± 0.74 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        108.81 ± 1.63 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12302.26 ± 460.51 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        165.94 ± 1.10 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        163.13 ± 1.47 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        158.63 ± 4.66 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |    13766.18 ± 181.35 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        225.86 ± 0.60 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        222.90 ± 0.49 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        216.65 ± 0.98 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      8074.98 ± 20.11 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         90.82 ± 2.28 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         89.78 ± 0.62 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         84.89 ± 2.21 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9605.60 ± 241.28 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        113.90 ± 1.83 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        108.87 ± 3.53 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        107.57 ± 0.75 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12401.27 ± 478.31 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        159.60 ± 6.86 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        163.93 ± 1.58 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        161.82 ± 0.61 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13712.88 ± 43.35 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        226.54 ± 0.54 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        222.03 ± 3.50 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        217.64 ± 0.33 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      5980.42 ± 11.98 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         77.05 ± 1.46 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         74.80 ± 2.31 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         72.60 ± 0.98 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     7983.90 ± 216.07 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        101.22 ± 2.20 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |         98.73 ± 0.52 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |         95.91 ± 1.30 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12412.63 ± 435.75 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        152.27 ± 0.73 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        148.11 ± 8.17 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        148.97 ± 0.69 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13814.87 ± 18.78 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        242.82 ± 0.86 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        238.51 ± 1.10 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        232.35 ± 0.37 |\r\n",
      "\r\n",
      "build: 7691654c (3827)\r\n"
     ]
    }
   ],
   "source": [
    "!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db615863",
   "metadata": {
    "papermill": {
     "duration": 0.028967,
     "end_time": "2024-09-26T08:50:03.949183",
     "exception": false,
     "start_time": "2024-09-26T08:50:03.920216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Package llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff7bf42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:50:04.007611Z",
     "iopub.status.busy": "2024-09-26T08:50:04.007256Z",
     "iopub.status.idle": "2024-09-26T08:55:36.207610Z",
     "shell.execute_reply": "2024-09-26T08:55:36.206616Z"
    },
    "papermill": {
     "duration": 332.232312,
     "end_time": "2024-09-26T08:55:36.210061",
     "exception": false,
     "start_time": "2024-09-26T08:50:03.977749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp\r\n",
      "tar: Removing leading `/' from member names\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!cd /kaggle/working/\n",
    "!tar -czf /kaggle/working/llama.tar.gz /kaggle/working/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3626ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T08:55:36.269717Z",
     "iopub.status.busy": "2024-09-26T08:55:36.269344Z",
     "iopub.status.idle": "2024-09-26T08:55:37.254528Z",
     "shell.execute_reply": "2024-09-26T08:55:37.253360Z"
    },
    "papermill": {
     "duration": 1.017663,
     "end_time": "2024-09-26T08:55:37.256830",
     "exception": false,
     "start_time": "2024-09-26T08:55:36.239167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  llama.cpp  llama.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 125947,
     "modelInstanceId": 101729,
     "sourceId": 120932,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125947,
     "modelInstanceId": 101730,
     "sourceId": 120933,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2482.563913,
   "end_time": "2024-09-26T08:55:37.705740",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-26T08:14:15.141827",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
