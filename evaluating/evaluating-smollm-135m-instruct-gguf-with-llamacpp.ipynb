{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120932,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101729,"modelId":125947},{"sourceId":120933,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101730,"modelId":125947}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\nThis note evaluate smolLM-135M instruct model with llama.cpp\n\n# Env and tools prepare","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp\n%cd llama.cpp\n# !export GGML_CUDA=1 \n!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ \n!make GGML_CUDA=1 CUDA_PATH=/usr/local/nvidia  > make.log 2>&1","metadata":{"execution":{"iopub.status.busy":"2024-09-26T02:10:38.517494Z","iopub.execute_input":"2024-09-26T02:10:38.517797Z","iopub.status.idle":"2024-09-26T02:32:40.175404Z","shell.execute_reply.started":"2024-09-26T02:10:38.517763Z","shell.execute_reply":"2024-09-26T02:32:40.174085Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 34756, done.\u001b[K\nremote: Counting objects: 100% (6738/6738), done.\u001b[K\nremote: Compressing objects: 100% (356/356), done.\u001b[K\nremote: Total 34756 (delta 6554), reused 6445 (delta 6381), pack-reused 28018 (from 1)\u001b[K\nReceiving objects: 100% (34756/34756), 58.27 MiB | 25.32 MiB/s, done.\nResolving deltas: 100% (25209/25209), done.\n/kaggle/working/llama.cpp\n","output_type":"stream"}]},{"cell_type":"code","source":"!tail make.log\n!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n!tail pip_install.log","metadata":{"execution":{"iopub.status.busy":"2024-09-26T02:33:21.903315Z","iopub.execute_input":"2024-09-26T02:33:21.904154Z","iopub.status.idle":"2024-09-26T02:35:01.529693Z","shell.execute_reply.started":"2024-09-26T02:33:21.904109Z","shell.execute_reply":"2024-09-26T02:35:01.528423Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  ggml/src/llamafile/sgemm.o ggml/src/ggml-cuda.o ggml/src/ggml-cuda/acc.o ggml/src/ggml-cuda/arange.o ggml/src/ggml-cuda/argsort.o ggml/src/ggml-cuda/binbcast.o ggml/src/ggml-cuda/clamp.o ggml/src/ggml-cuda/concat.o ggml/src/ggml-cuda/conv-transpose-1d.o ggml/src/ggml-cuda/convert.o ggml/src/ggml-cuda/cpy.o ggml/src/ggml-cuda/cross-entropy-loss.o ggml/src/ggml-cuda/diagmask.o ggml/src/ggml-cuda/dmmv.o ggml/src/ggml-cuda/fattn-tile-f16.o ggml/src/ggml-cuda/fattn-tile-f32.o ggml/src/ggml-cuda/fattn.o ggml/src/ggml-cuda/getrows.o ggml/src/ggml-cuda/im2col.o ggml/src/ggml-cuda/mmq.o ggml/src/ggml-cuda/mmvq.o ggml/src/ggml-cuda/norm.o ggml/src/ggml-cuda/opt-step-adamw.o ggml/src/ggml-cuda/out-prod.o ggml/src/ggml-cuda/pad.o ggml/src/ggml-cuda/pool2d.o ggml/src/ggml-cuda/quantize.o ggml/src/ggml-cuda/rope.o ggml/src/ggml-cuda/rwkv-wkv.o ggml/src/ggml-cuda/scale.o ggml/src/ggml-cuda/softmax.o ggml/src/ggml-cuda/sum.o ggml/src/ggml-cuda/sumrows.o ggml/src/ggml-cuda/tsembd.o ggml/src/ggml-cuda/unary.o ggml/src/ggml-cuda/upscale.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/gen-docs/gen-docs.cpp -o examples/gen-docs/gen-docs.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  ggml/src/llamafile/sgemm.o ggml/src/ggml-cuda.o ggml/src/ggml-cuda/acc.o ggml/src/ggml-cuda/arange.o ggml/src/ggml-cuda/argsort.o ggml/src/ggml-cuda/binbcast.o ggml/src/ggml-cuda/clamp.o ggml/src/ggml-cuda/concat.o ggml/src/ggml-cuda/conv-transpose-1d.o ggml/src/ggml-cuda/convert.o ggml/src/ggml-cuda/cpy.o ggml/src/ggml-cuda/cross-entropy-loss.o ggml/src/ggml-cuda/diagmask.o ggml/src/ggml-cuda/dmmv.o ggml/src/ggml-cuda/fattn-tile-f16.o ggml/src/ggml-cuda/fattn-tile-f32.o ggml/src/ggml-cuda/fattn.o ggml/src/ggml-cuda/getrows.o ggml/src/ggml-cuda/im2col.o ggml/src/ggml-cuda/mmq.o ggml/src/ggml-cuda/mmvq.o ggml/src/ggml-cuda/norm.o ggml/src/ggml-cuda/opt-step-adamw.o ggml/src/ggml-cuda/out-prod.o ggml/src/ggml-cuda/pad.o ggml/src/ggml-cuda/pool2d.o ggml/src/ggml-cuda/quantize.o ggml/src/ggml-cuda/rope.o ggml/src/ggml-cuda/rwkv-wkv.o ggml/src/ggml-cuda/scale.o ggml/src/ggml-cuda/softmax.o ggml/src/ggml-cuda/sum.o ggml/src/ggml-cuda/sumrows.o ggml/src/ggml-cuda/tsembd.o ggml/src/ggml-cuda/unary.o ggml/src/ggml-cuda/upscale.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.o ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.o ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.o ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.o ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.o ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.o ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o ggml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/build-info.o common/json-schema-to-grammar.o examples/gen-docs/gen-docs.o -o llama-gen-docs -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \ncc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  -c examples/deprecation-warning/deprecation-warning.cpp -o examples/deprecation-warning/deprecation-warning.o\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  examples/deprecation-warning/deprecation-warning.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \nNOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.\nc++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_USE_GRAPHS -I/usr/local/nvidia/include -I/usr/local/nvidia/targets/x86_64-linux/include  examples/deprecation-warning/deprecation-warning.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/nvidia/lib64 -L/usr/lib64 -L/usr/local/nvidia/targets/x86_64-linux/lib -L/usr/local/nvidia/lib64/stubs -L/usr/lib/wsl/lib \nNOTICE: The 'server' binary is deprecated. Please use 'llama-server' instead.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ns3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.9.0 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.9.0 gguf-0.10.0 huggingface-hub-0.25.1 idna-3.10 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 packaging-24.1 protobuf-4.25.3 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 sympy-1.13.3 tokenizers-0.20.0 torch-2.2.2+cpu tqdm-4.66.5 transformers-4.45.0 typing-extensions-4.12.2 urllib3-2.2.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dowload wiki text\n\n","metadata":{}},{"cell_type":"code","source":"!wget https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\n!gunzip en.txt.gz\n!head -n 10000 en.txt > en-h10000.txt\n!sh /kaggle/working/llama.cpp/scripts/get-wikitext-2.sh","metadata":{"execution":{"iopub.status.busy":"2024-09-26T02:44:56.571972Z","iopub.execute_input":"2024-09-26T02:44:56.572430Z","iopub.status.idle":"2024-09-26T02:45:23.108015Z","shell.execute_reply.started":"2024-09-26T02:44:56.572386Z","shell.execute_reply":"2024-09-26T02:45:23.107010Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"--2024-09-26 02:44:57--  https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\nResolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\nConnecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 532958396 (508M) [application/gzip]\nSaving to: 'en.txt.gz'\n\nen.txt.gz           100%[===================>] 508.27M  61.9MB/s    in 8.5s    \n\n2024-09-26 02:45:06 (59.5 MB/s) - 'en.txt.gz' saved [532958396/532958396]\n\n--2024-09-26 02:45:22--  https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip\nResolving huggingface.co (huggingface.co)... 18.239.69.31, 18.239.69.50, 18.239.69.71, ...\nConnecting to huggingface.co (huggingface.co)|18.239.69.31|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.hf.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727577922&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzU3NzkyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=c1D4QPyckGK5ps5AA%7E9SZVADzlrX9nR1iVZESegFJSUf8LFCK08wYmi9Stst0Dd7TPNJnoOi8s4fIBvB5fErPR%7EGSFxt1eOUDlhwRxB%7E9B0plZ25C0BwoW5eY%7Ei3GbAraYZY6Mu0L65Ps0alHwSMdWg4xh7jg%7ETXrms7WcFp-LZj72OsE5xz4OAPUcXV1sTA5SUgDcdCAMQF68tyAf6qIElBhRQmT5EPaBn6EGe3afQtERqE8yqTTOSK08tqbWpshGF5%7E83TKZV-E0tRPQ6HtmyJxcjjP5ndh0q1vDqSMEA70nUA62DBzIQX5rtJv8G2ItHgMwuaGLxhzx-x29wxXA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n--2024-09-26 02:45:22--  https://cdn-lfs-us-1.hf.co/repos/c6/78/c67802fcd48fa6f6a86773410b21cc6db1c5c546b20683b6c30b95f327a66922/ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11?response-content-disposition=inline%3B+filename*%3DUTF-8''wikitext-2-raw-v1.zip%3B+filename%3D%22wikitext-2-raw-v1.zip%22%3B&response-content-type=application%2Fzip&Expires=1727577922&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzU3NzkyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2M2Lzc4L2M2NzgwMmZjZDQ4ZmE2ZjZhODY3NzM0MTBiMjFjYzZkYjFjNWM1NDZiMjA2ODNiNmMzMGI5NWYzMjdhNjY5MjIvZWY3ZWRiNTY2ZTNlMmIyZDMxYjI5YzFmZGIwYzg5YTRjYzY4MzU5NzQ4NGMzZGMyNTE3OTE5YzYxNTQzNWExMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=c1D4QPyckGK5ps5AA~9SZVADzlrX9nR1iVZESegFJSUf8LFCK08wYmi9Stst0Dd7TPNJnoOi8s4fIBvB5fErPR~GSFxt1eOUDlhwRxB~9B0plZ25C0BwoW5eY~i3GbAraYZY6Mu0L65Ps0alHwSMdWg4xh7jg~TXrms7WcFp-LZj72OsE5xz4OAPUcXV1sTA5SUgDcdCAMQF68tyAf6qIElBhRQmT5EPaBn6EGe3afQtERqE8yqTTOSK08tqbWpshGF5~83TKZV-E0tRPQ6HtmyJxcjjP5ndh0q1vDqSMEA70nUA62DBzIQX5rtJv8G2ItHgMwuaGLxhzx-x29wxXA__&Key-Pair-Id=K24J24Z295AEI9\nResolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.238.243.19, 18.238.243.3, 18.238.243.22, ...\nConnecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.238.243.19|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4721645 (4.5M) [application/zip]\nSaving to: 'wikitext-2-raw-v1.zip'\n\nwikitext-2-raw-v1.z 100%[===================>]   4.50M  11.5MB/s    in 0.4s    \n\n2024-09-26 02:45:22 (11.5 MB/s) - 'wikitext-2-raw-v1.zip' saved [4721645/4721645]\n\nArchive:  wikitext-2-raw-v1.zip\n   creating: wikitext-2-raw/\n  inflating: wikitext-2-raw/wiki.test.raw  \n  inflating: wikitext-2-raw/wiki.valid.raw  \n  inflating: wikitext-2-raw/wiki.train.raw  \nUsage:\n\n  ./llama-perplexity -m model.gguf -f wikitext-2-raw/wiki.test.raw [other params]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run perplexity","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_V2.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > pip_fp16.log 2>&1\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:20:23.337202Z","iopub.execute_input":"2024-09-26T03:20:23.337965Z","iopub.status.idle":"2024-09-26T03:20:24.805483Z","shell.execute_reply.started":"2024-09-26T03:20:23.337921Z","shell.execute_reply":"2024-09-26T03:20:24.804313Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"build: 3827 (7691654c) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\nllama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Cosmo2 135M Webinst Sc2\nllama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\nllama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\nllama_model_loader: - kv   5:                           general.basename str              = cosmo2\nllama_model_loader: - kv   6:                         general.size_label str              = 135M\nllama_model_loader: - kv   7:                            general.license str              = apache-2.0\nllama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\nllama_model_loader: - kv   9:                          llama.block_count u32              = 30\nllama_model_loader: - kv  10:                       llama.context_length u32              = 2048\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 576\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 1\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\nllama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\nllama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\nllama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   61 tensors\nllama_model_loader: - type  f16:  211 tensors\nllm_load_vocab: special tokens cache size = 17\nllm_load_vocab: token to piece cache size = 0.3170 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 49152\nllm_load_print_meta: n_merges         = 48900\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 2048\nllm_load_print_meta: n_embd           = 576\nllm_load_print_meta: n_layer          = 30\nllm_load_print_meta: n_head           = 9\nllm_load_print_meta: n_head_kv        = 3\nllm_load_print_meta: n_rot            = 64\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 64\nllm_load_print_meta: n_embd_head_v    = 64\nllm_load_print_meta: n_gqa            = 3\nllm_load_print_meta: n_embd_k_gqa     = 192\nllm_load_print_meta: n_embd_v_gqa     = 192\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 1536\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 2048\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = ?B\nllm_load_print_meta: model ftype      = F16\nllm_load_print_meta: model params     = 134.52 M\nllm_load_print_meta: model size       = 256.63 MiB (16.00 BPW) \nllm_load_print_meta: general.name     = Cosmo2 135M Webinst Sc2\nllm_load_print_meta: BOS token        = 1 '<|im_start|>'\nllm_load_print_meta: EOS token        = 2 '<|im_end|>'\nllm_load_print_meta: UNK token        = 0 '<|endoftext|>'\nllm_load_print_meta: PAD token        = 2 '<|im_end|>'\nllm_load_print_meta: LF token         = 143 'Ä'\nllm_load_print_meta: EOT token        = 0 '<|endoftext|>'\nllm_load_print_meta: EOG token        = 0 '<|endoftext|>'\nllm_load_print_meta: EOG token        = 2 '<|im_end|>'\nllm_load_print_meta: max token length = 162\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.13 MiB\nllm_load_tensors: offloading 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/31 layers to GPU\nllm_load_tensors:        CPU buffer size =   256.63 MiB\n....................................................................\nllama_new_context_with_model: n_ctx      = 2048\nllama_new_context_with_model: n_batch    = 2048\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:  CUDA_Host KV buffer size =    45.00 MiB\nllama_new_context_with_model: KV self size  =   45.00 MiB, K (f16):   22.50 MiB, V (f16):   22.50 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.75 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   151.12 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =     5.13 MiB\nllama_new_context_with_model: graph nodes  = 966\nllama_new_context_with_model: graph splits = 334\nllama_init_from_gpt_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 4 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nperplexity: tokenizing the input ..\nperplexity: tokenization took 0.004 ms\nperplexity: you need at least 1024 tokens to evaluate perplexity with a context of 512\nperplexity: the data file you provided tokenizes to only 0 tokens\n\nllama_perf_context_print:        load time =     356.88 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =       0.23 ms /     2 tokens\n","output_type":"stream"}]},{"cell_type":"code","source":"!tail -6 ppl_Q4_V2.log\n!tail -6 ppl_Q4.log\n!tail -6 pip_fp16.log","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:08:07.930732Z","iopub.execute_input":"2024-09-26T03:08:07.931682Z","iopub.status.idle":"2024-09-26T03:08:10.887689Z","shell.execute_reply.started":"2024-09-26T03:08:07.931638Z","shell.execute_reply":"2024-09-26T03:08:10.886730Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Final estimate: PPL = 23.2651 +/- 0.28000\n\nllama_perf_context_print:        load time =     373.09 ms\nllama_perf_context_print: prompt eval time =   28168.07 ms / 131072 tokens (    0.21 ms per token,  4653.21 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   47571.89 ms / 131073 tokens\nFinal estimate: PPL = 23.2651 +/- 0.28000\n\nllama_perf_context_print:        load time =     340.33 ms\nllama_perf_context_print: prompt eval time =   28459.90 ms / 131072 tokens (    0.22 ms per token,  4605.50 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   47960.10 ms / 131073 tokens\nFinal estimate: PPL = 21.2879 +/- 0.25264\n\nllama_perf_context_print:        load time =     339.44 ms\nllama_perf_context_print: prompt eval time =   38147.45 ms / 131072 tokens (    0.29 ms per token,  3435.93 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   57457.93 ms / 131073 tokens\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption ","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-bench -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -mg 1 ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:22:54.176507Z","iopub.execute_input":"2024-09-26T03:22:54.177663Z","iopub.status.idle":"2024-09-26T03:23:05.277022Z","shell.execute_reply.started":"2024-09-26T03:22:54.177616Z","shell.execute_reply":"2024-09-26T03:23:05.275913Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\n| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  99 |          1 |         pp512 |     13816.28 ± 21.44 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  99 |          1 |         tg128 |        223.06 ± 3.16 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  99 |          1 |         pp512 |     13680.32 ± 34.43 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  99 |          1 |         tg128 |        224.77 ± 0.29 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  99 |          1 |         pp512 |     13757.69 ± 23.58 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  99 |          1 |         tg128 |        240.23 ± 0.35 |\n\nbuild: 7691654c (3827)\n","output_type":"stream"}]}]}