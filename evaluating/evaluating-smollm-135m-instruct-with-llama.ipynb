{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":198397091,"sourceType":"kernelVersion"},{"sourceId":120932,"sourceType":"modelInstanceVersion","modelInstanceId":101729,"modelId":125947},{"sourceId":120933,"sourceType":"modelInstanceVersion","modelInstanceId":101730,"modelId":125947}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\nThis note evaluate smolLM-135M instruct model with llama.cpp\n\n# Env and tools prepare","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/llama-cpp-binary-compiled-with-gpu/llama.cpp /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2024-09-29T01:33:48.387114Z","iopub.execute_input":"2024-09-29T01:33:48.387472Z","iopub.status.idle":"2024-09-29T01:35:03.084081Z","shell.execute_reply.started":"2024-09-29T01:33:48.387437Z","shell.execute_reply":"2024-09-29T01:35:03.082847Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!chmod +x -R llama.cpp\n%cd llama.cpp\n!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ ","metadata":{"execution":{"iopub.status.busy":"2024-09-29T01:35:03.086079Z","iopub.execute_input":"2024-09-29T01:35:03.086390Z","iopub.status.idle":"2024-09-29T01:35:23.092833Z","shell.execute_reply.started":"2024-09-29T01:35:03.086358Z","shell.execute_reply":"2024-09-29T01:35:23.091618Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/llama.cpp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run perplexity\n\nPerplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\nFormally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n\nPerplexity benchmark shows how accurate the model is ","metadata":{}},{"cell_type":"code","source":"\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_V2.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > pip_fp16.log 2>&1\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T01:35:23.094610Z","iopub.execute_input":"2024-09-29T01:35:23.095038Z","iopub.status.idle":"2024-09-29T01:38:07.379546Z","shell.execute_reply.started":"2024-09-29T01:35:23.094993Z","shell.execute_reply":"2024-09-29T01:38:07.378459Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!tail -6 ppl_Q4_V2.log\n!tail -6 ppl_Q4.log\n!tail -6 pip_fp16.log","metadata":{"execution":{"iopub.status.busy":"2024-09-29T01:38:07.382380Z","iopub.execute_input":"2024-09-29T01:38:07.382841Z","iopub.status.idle":"2024-09-29T01:38:10.309177Z","shell.execute_reply.started":"2024-09-29T01:38:07.382792Z","shell.execute_reply":"2024-09-29T01:38:10.307815Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Final estimate: PPL = 23.2651 +/- 0.28000\n\nllama_perf_context_print:        load time =    1361.88 ms\nllama_perf_context_print: prompt eval time =   29115.58 ms / 131072 tokens (    0.22 ms per token,  4501.78 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   49144.05 ms / 131073 tokens\nFinal estimate: PPL = 23.2651 +/- 0.28000\n\nllama_perf_context_print:        load time =    1328.80 ms\nllama_perf_context_print: prompt eval time =   28861.40 ms / 131072 tokens (    0.22 ms per token,  4541.43 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   48036.22 ms / 131073 tokens\nFinal estimate: PPL = 21.2879 +/- 0.25264\n\nllama_perf_context_print:        load time =    2577.67 ms\nllama_perf_context_print: prompt eval time =   38928.87 ms / 131072 tokens (    0.30 ms per token,  3366.96 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   58266.64 ms / 131073 tokens\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption \n\nLlama-bench shows performance and resource consumption. \nThere are three types of test\n- Prompt processing (pp): processing a prompt in batches (-p)\n- Text generation (tg): generating a sequence of tokens (-n)\n- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n\nIn the following test, we run text generation and prompt processing\n","metadata":{}},{"cell_type":"code","source":"!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M-v2.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/q4_k_m/1/SmolLM-135M-Instruct-Q4_K_M.gguf -m /kaggle/input/smollm-135m-instruct-gguf/gguf/fp16/1/SmolLM-135M-Instruc-fp16.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 ","metadata":{"execution":{"iopub.status.busy":"2024-09-29T01:38:10.310690Z","iopub.execute_input":"2024-09-29T01:38:10.311010Z","iopub.status.idle":"2024-09-29T01:45:48.131345Z","shell.execute_reply.started":"2024-09-29T01:38:10.310976Z","shell.execute_reply":"2024-09-29T01:45:48.130368Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\n| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      7222.64 ± 85.29 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         86.45 ± 4.96 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         88.31 ± 0.79 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         82.95 ± 2.48 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9488.26 ± 263.33 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        113.86 ± 1.46 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        108.70 ± 5.52 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        106.15 ± 1.50 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12585.63 ± 398.38 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        161.09 ± 0.64 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        155.30 ± 6.77 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        155.51 ± 0.75 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13831.80 ± 45.66 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        221.35 ± 0.33 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        217.73 ± 2.44 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        212.76 ± 0.66 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |     7831.11 ± 124.12 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         89.67 ± 1.62 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         84.98 ± 2.64 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         83.86 ± 1.69 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9256.26 ± 239.13 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        112.12 ± 1.22 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        108.27 ± 1.82 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        104.68 ± 2.40 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12302.04 ± 162.52 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        162.29 ± 0.88 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        159.66 ± 0.91 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        153.48 ± 4.96 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13687.49 ± 27.44 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        222.27 ± 0.35 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        218.89 ± 1.63 |\n| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        213.50 ± 0.60 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      5442.34 ± 91.71 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         72.96 ± 3.49 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         73.54 ± 0.51 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         70.20 ± 0.88 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     7717.50 ± 235.77 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |         93.72 ± 5.89 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |         96.40 ± 0.43 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |         92.28 ± 2.12 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    11973.29 ± 277.24 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        147.46 ± 0.87 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        146.56 ± 0.97 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        142.66 ± 1.94 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13750.93 ± 34.72 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        238.72 ± 0.47 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        235.37 ± 0.70 |\n| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        226.91 ± 0.86 |\n\nbuild: 95bc82fb (3828)\n","output_type":"stream"}]}]}