{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":120938,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":101735,"modelId":125952}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Background\n\nThis node will run some examples of using llama cpp python bind and provide samples\n\n# Install python binding example","metadata":{}},{"cell_type":"code","source":"!CMAKE_ARGS=\"-DGGML_CUDA=on\" \n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:28:04.900742Z","iopub.execute_input":"2024-10-01T14:28:04.901141Z","iopub.status.idle":"2024-10-01T14:28:06.026489Z","shell.execute_reply.started":"2024-10-01T14:28:04.901101Z","shell.execute_reply":"2024-10-01T14:28:06.025107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:28:06.029105Z","iopub.execute_input":"2024-10-01T14:28:06.029561Z","iopub.status.idle":"2024-10-01T14:30:29.022936Z","shell.execute_reply.started":"2024-10-01T14:28:06.029512Z","shell.execute_reply":"2024-10-01T14:30:29.021725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# High level API","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\n\nllm = Llama(\n      model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n      # seed=1337, # Uncomment to set a specific seed\n      # n_ctx=2048, # Uncomment to increase the context window\n)\noutput = llm(\n      \"Q: Name the planets in the solar system? A: \", # Prompt\n      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n      echo=True # Echo the prompt back in the output\n) # Generate a completion, can also call create_completion\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:30:29.024764Z","iopub.execute_input":"2024-10-01T14:30:29.025168Z","iopub.status.idle":"2024-10-01T14:30:53.086721Z","shell.execute_reply.started":"2024-10-01T14:30:29.025125Z","shell.execute_reply":"2024-10-01T14:30:53.081505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pulling models from Hugging Face Hub\n\nLlama cpp python could pull image from hugging face.  For example\n","metadata":{}},{"cell_type":"code","source":"llm = Llama.from_pretrained(\n    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n    filename=\"*q8_0.gguf\",\n    verbose=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:30:53.101237Z","iopub.execute_input":"2024-10-01T14:30:53.102563Z","iopub.status.idle":"2024-10-01T14:30:58.743552Z","shell.execute_reply.started":"2024-10-01T14:30:53.102490Z","shell.execute_reply":"2024-10-01T14:30:58.742488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chat Completion\n\nThe high-level API also provides a simple interface for chat completion.\n\nChat completion requires that the model knows how to format the messages into a single prompt. The Llama class does this using pre-registered chat formats (ie. chatml, llama-2, gemma, etc) or by providing a custom chat handler object.","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\nllm = Llama(\n      model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n      chat_format=\"llama-2\"\n)\nllm.create_chat_completion(\n      messages = [\n          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n          {\n              \"role\": \"user\",\n              \"content\": \"Describe this image in detail please.\"\n          }\n      ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:30:58.744766Z","iopub.execute_input":"2024-10-01T14:30:58.745097Z","iopub.status.idle":"2024-10-01T14:31:00.720237Z","shell.execute_reply.started":"2024-10-01T14:30:58.745062Z","shell.execute_reply":"2024-10-01T14:31:00.719291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# JSON and JSON Schema Mode\n\nTo constrain chat responses to only valid JSON or a specific JSON Schema use the response_format argument in create_chat_completion.\n\n## JSON Mode\n\nThe following example will constrain the response to valid JSON strings only.","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\nllm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml\")\nllm.create_chat_completion(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n        },\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n    ],\n    response_format={\n        \"type\": \"json_object\",\n    },\n    temperature=0.7,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:31:00.721479Z","iopub.execute_input":"2024-10-01T14:31:00.722325Z","iopub.status.idle":"2024-10-01T14:31:03.020319Z","shell.execute_reply.started":"2024-10-01T14:31:00.722280Z","shell.execute_reply":"2024-10-01T14:31:03.018770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Json schema mode\nTo constrain the response further to a specific JSON Schema add the schema to the schema property of the response_format argument.","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\nllm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml\")\nllm.create_chat_completion(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n        },\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n    ],\n    response_format={\n        \"type\": \"json_object\",\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\"team_name\": {\"type\": \"string\"}},\n            \"required\": [\"team_name\"],\n        },\n    },\n    temperature=0.7,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:31:03.021875Z","iopub.execute_input":"2024-10-01T14:31:03.022413Z","iopub.status.idle":"2024-10-01T14:31:11.734921Z","shell.execute_reply.started":"2024-10-01T14:31:03.022343Z","shell.execute_reply":"2024-10-01T14:31:11.733613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function Calling\n\nThe high-level API supports OpenAI compatible function and tool calling. This is possible through the functionary pre-trained models chat format or through the generic chatml-function-calling chat format.","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\nllm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml-function-calling\")\nllm.create_chat_completion(\n      messages = [\n        {\n          \"role\": \"system\",\n          \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"Extract Jason is 25 years old\"\n        }\n      ],\n      tools=[{\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"UserDetail\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"title\": \"UserDetail\",\n            \"properties\": {\n              \"name\": {\n                \"title\": \"Name\",\n                \"type\": \"string\"\n              },\n              \"age\": {\n                \"title\": \"Age\",\n                \"type\": \"integer\"\n              }\n            },\n            \"required\": [ \"name\", \"age\" ]\n          }\n        }\n      }],\n      tool_choice={\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"UserDetail\"\n        }\n      }\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:31:11.736323Z","iopub.execute_input":"2024-10-01T14:31:11.736682Z","iopub.status.idle":"2024-10-01T14:31:26.256485Z","shell.execute_reply.started":"2024-10-01T14:31:11.736644Z","shell.execute_reply":"2024-10-01T14:31:26.255434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Speculative Decoding\n\nllama-cpp-python supports speculative decoding which allows the model to generate completions based on a draft model.\n\nThe fastest way to use speculative decoding is through the LlamaPromptLookupDecoding class.\n\nJust pass this as a draft model to the Llama class during initialization.","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n\nllama = Llama(\n    model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10) # num_pred_tokens is the number of tokens to predict 10 is the default and generally good for gpu, 2 performs better for cpu-only machines.\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:31:26.257884Z","iopub.execute_input":"2024-10-01T14:31:26.258344Z","iopub.status.idle":"2024-10-01T14:31:26.858341Z","shell.execute_reply.started":"2024-10-01T14:31:26.258283Z","shell.execute_reply":"2024-10-01T14:31:26.857290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embeddings\n\nTo generate text embeddings use create_embedding or embed. Note that you must pass embedding=True to the constructor upon model creation for these to work properly.","metadata":{}},{"cell_type":"code","source":"import llama_cpp\n\nllm = llama_cpp.Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", embedding=True)\n\nembeddings = llm.create_embedding(\"Hello, world!\")\n\n# or create multiple embeddings at once\n\nembeddings = llm.create_embedding([\"Hello, world!\", \"Goodbye, world!\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:32:01.735702Z","iopub.execute_input":"2024-10-01T14:32:01.736168Z","iopub.status.idle":"2024-10-01T14:32:03.155724Z","shell.execute_reply.started":"2024-10-01T14:32:01.736114Z","shell.execute_reply":"2024-10-01T14:32:03.154673Z"},"trusted":true},"execution_count":null,"outputs":[]}]}