{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f00ae25",
   "metadata": {
    "papermill": {
     "duration": 0.005732,
     "end_time": "2024-10-02T15:25:11.687030",
     "exception": false,
     "start_time": "2024-10-02T15:25:11.681298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Background\n",
    "\n",
    "This node will run some examples of using llama cpp python bind and provide samples\n",
    "\n",
    "# Install python binding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d8f573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:25:11.699153Z",
     "iopub.status.busy": "2024-10-02T15:25:11.698480Z",
     "iopub.status.idle": "2024-10-02T15:25:12.694817Z",
     "shell.execute_reply": "2024-10-02T15:25:12.693623Z"
    },
    "papermill": {
     "duration": 1.005046,
     "end_time": "2024-10-02T15:25:12.697211",
     "exception": false,
     "start_time": "2024-10-02T15:25:11.692165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DGGML_CUDA=on\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee97b5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:25:12.709173Z",
     "iopub.status.busy": "2024-10-02T15:25:12.708431Z",
     "iopub.status.idle": "2024-10-02T15:27:08.724987Z",
     "shell.execute_reply": "2024-10-02T15:27:08.723703Z"
    },
    "papermill": {
     "duration": 116.024598,
     "end_time": "2024-10-02T15:27:08.726997",
     "exception": false,
     "start_time": "2024-10-02T15:25:12.702399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\r\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp310-cp310-linux_x86_64.whl size=3511094 sha256=115df5db63232d815d00b168907fd3d0a747842451cb906d0658d88f5f056763\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f8/b0/a2/f47d952aec7ab061b9e2a345e23a1e1e137beb7891259e3d0c\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc203e17",
   "metadata": {
    "papermill": {
     "duration": 0.011245,
     "end_time": "2024-10-02T15:27:08.749286",
     "exception": false,
     "start_time": "2024-10-02T15:27:08.738041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# High level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd806d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:27:08.772616Z",
     "iopub.status.busy": "2024-10-02T15:27:08.772259Z",
     "iopub.status.idle": "2024-10-02T15:27:44.310281Z",
     "shell.execute_reply": "2024-10-02T15:27:44.309276Z"
    },
    "papermill": {
     "duration": 35.552589,
     "end_time": "2024-10-02T15:27:44.312680",
     "exception": false,
     "start_time": "2024-10-02T15:27:08.760091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n",
      "llama_perf_context_print:        load time =     456.31 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     457.36 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-4719ef76-a0f5-4d9d-858d-95f30e2060a3', 'object': 'text_completion', 'created': 1727882863, 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 1, 'total_tokens': 14}}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084aea04",
   "metadata": {
    "papermill": {
     "duration": 0.031999,
     "end_time": "2024-10-02T15:27:44.377744",
     "exception": false,
     "start_time": "2024-10-02T15:27:44.345745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pulling models from Hugging Face Hub\n",
    "\n",
    "Llama cpp python could pull image from hugging face.  For example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7793610c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:27:44.443896Z",
     "iopub.status.busy": "2024-10-02T15:27:44.442873Z",
     "iopub.status.idle": "2024-10-02T15:27:54.553482Z",
     "shell.execute_reply": "2024-10-02T15:27:54.552672Z"
    },
    "papermill": {
     "duration": 10.145318,
     "end_time": "2024-10-02T15:27:54.555702",
     "exception": false,
     "start_time": "2024-10-02T15:27:44.410384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950db2636bc44ff4a7130178f6ff7d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2-0_5b-instruct-q8_0.gguf:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe42ce",
   "metadata": {
    "papermill": {
     "duration": 0.02584,
     "end_time": "2024-10-02T15:27:54.608692",
     "exception": false,
     "start_time": "2024-10-02T15:27:54.582852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chat Completion\n",
    "\n",
    "The high-level API also provides a simple interface for chat completion.\n",
    "\n",
    "Chat completion requires that the model knows how to format the messages into a single prompt. The Llama class does this using pre-registered chat formats (ie. chatml, llama-2, gemma, etc) or by providing a custom chat handler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afdd63cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:27:54.662605Z",
     "iopub.status.busy": "2024-10-02T15:27:54.662256Z",
     "iopub.status.idle": "2024-10-02T15:27:56.267077Z",
     "shell.execute_reply": "2024-10-02T15:27:56.266304Z"
    },
    "papermill": {
     "duration": 1.634148,
     "end_time": "2024-10-02T15:27:56.269004",
     "exception": false,
     "start_time": "2024-10-02T15:27:54.634856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =     995.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    34 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     996.08 ms /    35 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-0eac9526-0a15-4ea8-bcf1-8b85619c30cb',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727882875,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': ''},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 34, 'completion_tokens': 0, 'total_tokens': 34}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "      model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n",
    "      chat_format=\"llama-2\"\n",
    ")\n",
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db67b8",
   "metadata": {
    "papermill": {
     "duration": 0.050989,
     "end_time": "2024-10-02T15:27:56.371320",
     "exception": false,
     "start_time": "2024-10-02T15:27:56.320331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JSON and JSON Schema Mode\n",
    "\n",
    "To constrain chat responses to only valid JSON or a specific JSON Schema use the response_format argument in create_chat_completion.\n",
    "\n",
    "## JSON Mode\n",
    "\n",
    "The following example will constrain the response to valid JSON strings only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fdb0095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:27:56.470101Z",
     "iopub.status.busy": "2024-10-02T15:27:56.469738Z",
     "iopub.status.idle": "2024-10-02T15:27:58.362027Z",
     "shell.execute_reply": "2024-10-02T15:27:58.361113Z"
    },
    "papermill": {
     "duration": 1.942508,
     "end_time": "2024-10-02T15:27:58.364197",
     "exception": false,
     "start_time": "2024-10-02T15:27:56.421689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =    1077.89 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1287.67 ms /    36 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-c04c2005-67d0-4ccd-a8f7-acec9fc6c831',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727882877,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': '{}'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 35, 'completion_tokens': 1, 'total_tokens': 36}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml\")\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede8956",
   "metadata": {
    "papermill": {
     "duration": 0.056891,
     "end_time": "2024-10-02T15:27:58.482313",
     "exception": false,
     "start_time": "2024-10-02T15:27:58.425422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Json schema mode\n",
    "To constrain the response further to a specific JSON Schema add the schema to the schema property of the response_format argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0199bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:27:58.598016Z",
     "iopub.status.busy": "2024-10-02T15:27:58.597627Z",
     "iopub.status.idle": "2024-10-02T15:28:06.009645Z",
     "shell.execute_reply": "2024-10-02T15:28:06.008749Z"
    },
    "papermill": {
     "duration": 7.471826,
     "end_time": "2024-10-02T15:28:06.011614",
     "exception": false,
     "start_time": "2024-10-02T15:27:58.539788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =    1046.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6822.63 ms /    66 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-59f78abc-2cf9-43c3-a459-797dc78e27b4',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727882879,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '{\"team_name\": \"Los Angeles Dodgers (NL) vs. New York Mets (AL) 2020 World Series\"}'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 35, 'completion_tokens': 31, 'total_tokens': 66}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml\")\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"team_name\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"team_name\"],\n",
    "        },\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e83bdd",
   "metadata": {
    "papermill": {
     "duration": 0.072694,
     "end_time": "2024-10-02T15:28:06.158804",
     "exception": false,
     "start_time": "2024-10-02T15:28:06.086110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Function Calling\n",
    "\n",
    "The high-level API supports OpenAI compatible function and tool calling. This is possible through the functionary pre-trained models chat format or through the generic chatml-function-calling chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad97334a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:28:06.307688Z",
     "iopub.status.busy": "2024-10-02T15:28:06.306837Z",
     "iopub.status.idle": "2024-10-02T15:28:18.381574Z",
     "shell.execute_reply": "2024-10-02T15:28:18.380668Z"
    },
    "papermill": {
     "duration": 12.151046,
     "end_time": "2024-10-02T15:28:18.383687",
     "exception": false,
     "start_time": "2024-10-02T15:28:06.232641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =    8667.21 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   273 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11455.37 ms /   288 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-7da505cb-6edd-4566-a40a-d189ca5cf011',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727882886,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'function_call': {'name': 'UserDetail',\n",
       "     'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'},\n",
       "    'tool_calls': [{'id': 'call__0_UserDetail_cmpl-7da505cb-6edd-4566-a40a-d189ca5cf011',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'UserDetail',\n",
       "       'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}}]},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'tool_calls'}],\n",
       " 'usage': {'prompt_tokens': 273, 'completion_tokens': 15, 'total_tokens': 288}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml-function-calling\")\n",
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Extract Jason is 25 years old\"\n",
    "        }\n",
    "      ],\n",
    "      tools=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "          \"name\": \"UserDetail\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"UserDetail\",\n",
    "            \"properties\": {\n",
    "              \"name\": {\n",
    "                \"title\": \"Name\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"age\": {\n",
    "                \"title\": \"Age\",\n",
    "                \"type\": \"integer\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [ \"name\", \"age\" ]\n",
    "          }\n",
    "        }\n",
    "      }],\n",
    "      tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "          \"name\": \"UserDetail\"\n",
    "        }\n",
    "      }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163c898",
   "metadata": {
    "papermill": {
     "duration": 0.086647,
     "end_time": "2024-10-02T15:28:18.561237",
     "exception": false,
     "start_time": "2024-10-02T15:28:18.474590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Speculative Decoding\n",
    "\n",
    "llama-cpp-python supports speculative decoding which allows the model to generate completions based on a draft model.\n",
    "\n",
    "The fastest way to use speculative decoding is through the LlamaPromptLookupDecoding class.\n",
    "\n",
    "Just pass this as a draft model to the Llama class during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cdd0e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:28:18.775427Z",
     "iopub.status.busy": "2024-10-02T15:28:18.775047Z",
     "iopub.status.idle": "2024-10-02T15:28:19.288444Z",
     "shell.execute_reply": "2024-10-02T15:28:19.287477Z"
    },
    "papermill": {
     "duration": 0.60401,
     "end_time": "2024-10-02T15:28:19.290448",
     "exception": false,
     "start_time": "2024-10-02T15:28:18.686438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "\n",
    "llama = Llama(\n",
    "    model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n",
    "    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10) # num_pred_tokens is the number of tokens to predict 10 is the default and generally good for gpu, 2 performs better for cpu-only machines.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a064f02",
   "metadata": {
    "papermill": {
     "duration": 0.102202,
     "end_time": "2024-10-02T15:28:19.495713",
     "exception": false,
     "start_time": "2024-10-02T15:28:19.393511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "To generate text embeddings use create_embedding or embed. Note that you must pass embedding=True to the constructor upon model creation for these to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67c0d89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:28:19.704488Z",
     "iopub.status.busy": "2024-10-02T15:28:19.703684Z",
     "iopub.status.idle": "2024-10-02T15:28:20.882008Z",
     "shell.execute_reply": "2024-10-02T15:28:20.881098Z"
    },
    "papermill": {
     "duration": 1.283655,
     "end_time": "2024-10-02T15:28:20.884237",
     "exception": false,
     "start_time": "2024-10-02T15:28:19.600582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n",
      "llama_perf_context_print:        load time =     175.47 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.03 ms /     5 tokens\n",
      "llama_perf_context_print:        load time =     175.47 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     370.95 ms /    10 tokens\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "\n",
    "llm = llama_cpp.Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", embedding=True)\n",
    "\n",
    "embeddings = llm.create_embedding(\"Hello, world!\")\n",
    "\n",
    "# or create multiple embeddings at once\n",
    "\n",
    "embeddings = llm.create_embedding([\"Hello, world!\", \"Goodbye, world!\"])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 125952,
     "modelInstanceId": 101735,
     "sourceId": 120938,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 192.603379,
   "end_time": "2024-10-02T15:28:21.551869",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-02T15:25:08.948490",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1629597cfb504c0e828e9dcd5064da89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_adede7ea09e748b584e64abf9a2e7bb1",
       "placeholder": "​",
       "style": "IPY_MODEL_339220ae92dc454db7b41245ab0a8895",
       "value": " 531M/531M [00:07&lt;00:00, 177MB/s]"
      }
     },
     "1dd466a461094744bc9e249856a9da78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "213074abf6a247ec87959ffdb38a05f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2c6bf6f59a484750b10ef994bf9fb3be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "339220ae92dc454db7b41245ab0a8895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3ace336f9693459da27d9c7c8a5b97a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e5058681dc04f17b4496c13ecfd1057": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c6bf6f59a484750b10ef994bf9fb3be",
       "max": 531065536.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_213074abf6a247ec87959ffdb38a05f1",
       "value": 531065536.0
      }
     },
     "7548afe770b04f1fad78a5415a1e2667": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9364c9f3bbcf4b3990981605428bae7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1dd466a461094744bc9e249856a9da78",
       "placeholder": "​",
       "style": "IPY_MODEL_7548afe770b04f1fad78a5415a1e2667",
       "value": "qwen2-0_5b-instruct-q8_0.gguf: 100%"
      }
     },
     "950db2636bc44ff4a7130178f6ff7d0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9364c9f3bbcf4b3990981605428bae7e",
        "IPY_MODEL_4e5058681dc04f17b4496c13ecfd1057",
        "IPY_MODEL_1629597cfb504c0e828e9dcd5064da89"
       ],
       "layout": "IPY_MODEL_3ace336f9693459da27d9c7c8a5b97a9"
      }
     },
     "adede7ea09e748b584e64abf9a2e7bb1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
