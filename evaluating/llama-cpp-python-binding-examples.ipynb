{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee400c2",
   "metadata": {
    "papermill": {
     "duration": 0.00611,
     "end_time": "2024-10-02T15:27:49.858065",
     "exception": false,
     "start_time": "2024-10-02T15:27:49.851955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Background\n",
    "\n",
    "This node will run some examples of using llama cpp python bind and provide samples\n",
    "\n",
    "# Install python binding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614f0b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:27:49.869787Z",
     "iopub.status.busy": "2024-10-02T15:27:49.869412Z",
     "iopub.status.idle": "2024-10-02T15:27:50.885140Z",
     "shell.execute_reply": "2024-10-02T15:27:50.883933Z"
    },
    "papermill": {
     "duration": 1.024315,
     "end_time": "2024-10-02T15:27:50.887575",
     "exception": false,
     "start_time": "2024-10-02T15:27:49.863260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DGGML_CUDA=on\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6088d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:27:50.899218Z",
     "iopub.status.busy": "2024-10-02T15:27:50.898867Z",
     "iopub.status.idle": "2024-10-02T15:29:46.950331Z",
     "shell.execute_reply": "2024-10-02T15:29:46.949204Z"
    },
    "papermill": {
     "duration": 116.059716,
     "end_time": "2024-10-02T15:29:46.952527",
     "exception": false,
     "start_time": "2024-10-02T15:27:50.892811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\r\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp310-cp310-linux_x86_64.whl size=3511096 sha256=9e71af2e961115134a9544ed1b8a8053682cad4a1408a096dbb82c3bb06205e1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f8/b0/a2/f47d952aec7ab061b9e2a345e23a1e1e137beb7891259e3d0c\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca440e6",
   "metadata": {
    "papermill": {
     "duration": 0.01064,
     "end_time": "2024-10-02T15:29:46.974010",
     "exception": false,
     "start_time": "2024-10-02T15:29:46.963370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# High level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1c825d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:29:46.997049Z",
     "iopub.status.busy": "2024-10-02T15:29:46.996676Z",
     "iopub.status.idle": "2024-10-02T15:30:14.029291Z",
     "shell.execute_reply": "2024-10-02T15:30:14.028187Z"
    },
    "papermill": {
     "duration": 27.049379,
     "end_time": "2024-10-02T15:30:14.033952",
     "exception": false,
     "start_time": "2024-10-02T15:29:46.984573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n",
      "llama_perf_context_print:        load time =     446.79 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.82 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-e9040afe-f13c-47b4-8fb2-a8b050d448c8', 'object': 'text_completion', 'created': 1727883013, 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 1, 'total_tokens': 14}}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b6e8b",
   "metadata": {
    "papermill": {
     "duration": 0.032866,
     "end_time": "2024-10-02T15:30:14.099833",
     "exception": false,
     "start_time": "2024-10-02T15:30:14.066967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pulling models from Hugging Face Hub\n",
    "\n",
    "Llama cpp python could pull image from hugging face.  For example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425badb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:30:14.166202Z",
     "iopub.status.busy": "2024-10-02T15:30:14.165435Z",
     "iopub.status.idle": "2024-10-02T15:30:20.159501Z",
     "shell.execute_reply": "2024-10-02T15:30:20.158653Z"
    },
    "papermill": {
     "duration": 6.029783,
     "end_time": "2024-10-02T15:30:20.161920",
     "exception": false,
     "start_time": "2024-10-02T15:30:14.132137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5783e2423d994a71ae320cdcba947a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2-0_5b-instruct-q8_0.gguf:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8fd8d7",
   "metadata": {
    "papermill": {
     "duration": 0.02593,
     "end_time": "2024-10-02T15:30:20.214725",
     "exception": false,
     "start_time": "2024-10-02T15:30:20.188795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chat Completion\n",
    "\n",
    "The high-level API also provides a simple interface for chat completion.\n",
    "\n",
    "Chat completion requires that the model knows how to format the messages into a single prompt. The Llama class does this using pre-registered chat formats (ie. chatml, llama-2, gemma, etc) or by providing a custom chat handler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f554274e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:30:20.268469Z",
     "iopub.status.busy": "2024-10-02T15:30:20.268061Z",
     "iopub.status.idle": "2024-10-02T15:30:21.892972Z",
     "shell.execute_reply": "2024-10-02T15:30:21.892237Z"
    },
    "papermill": {
     "duration": 1.654038,
     "end_time": "2024-10-02T15:30:21.894967",
     "exception": false,
     "start_time": "2024-10-02T15:30:20.240929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =     983.83 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    34 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     984.44 ms /    35 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-ffdc6882-5d0e-439e-9bb5-59398fdad094',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727883020,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': ''},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 34, 'completion_tokens': 0, 'total_tokens': 34}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(\n",
    "      model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n",
    "      chat_format=\"llama-2\"\n",
    ")\n",
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2266d1",
   "metadata": {
    "papermill": {
     "duration": 0.051593,
     "end_time": "2024-10-02T15:30:21.999015",
     "exception": false,
     "start_time": "2024-10-02T15:30:21.947422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JSON and JSON Schema Mode\n",
    "\n",
    "To constrain chat responses to only valid JSON or a specific JSON Schema use the response_format argument in create_chat_completion.\n",
    "\n",
    "## JSON Mode\n",
    "\n",
    "The following example will constrain the response to valid JSON strings only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9903d3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:30:22.102875Z",
     "iopub.status.busy": "2024-10-02T15:30:22.102511Z",
     "iopub.status.idle": "2024-10-02T15:30:23.965229Z",
     "shell.execute_reply": "2024-10-02T15:30:23.964324Z"
    },
    "papermill": {
     "duration": 1.913275,
     "end_time": "2024-10-02T15:30:23.967414",
     "exception": false,
     "start_time": "2024-10-02T15:30:22.054139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =     993.97 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1260.84 ms /    36 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-e71e225a-ffcf-40dc-8c9f-6b556080fbeb',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727883022,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': '{}'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 35, 'completion_tokens': 1, 'total_tokens': 36}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml\")\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c5751",
   "metadata": {
    "papermill": {
     "duration": 0.056637,
     "end_time": "2024-10-02T15:30:24.083816",
     "exception": false,
     "start_time": "2024-10-02T15:30:24.027179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Json schema mode\n",
    "To constrain the response further to a specific JSON Schema add the schema to the schema property of the response_format argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e7fbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:30:24.198651Z",
     "iopub.status.busy": "2024-10-02T15:30:24.198272Z",
     "iopub.status.idle": "2024-10-02T15:30:31.406855Z",
     "shell.execute_reply": "2024-10-02T15:30:31.406034Z"
    },
    "papermill": {
     "duration": 7.268559,
     "end_time": "2024-10-02T15:30:31.408912",
     "exception": false,
     "start_time": "2024-10-02T15:30:24.140353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =     990.32 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6584.10 ms /    66 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f7025a69-782f-4299-bd96-dc4f8a8d1693',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727883024,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '{\"team_name\": \"Los Angeles Dodgers (NL) vs. New York Mets (AL) 2020 World Series\"}'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 35, 'completion_tokens': 31, 'total_tokens': 66}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml\")\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"team_name\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"team_name\"],\n",
    "        },\n",
    "    },\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4ed72",
   "metadata": {
    "papermill": {
     "duration": 0.073362,
     "end_time": "2024-10-02T15:30:31.557334",
     "exception": false,
     "start_time": "2024-10-02T15:30:31.483972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Function Calling\n",
    "\n",
    "The high-level API supports OpenAI compatible function and tool calling. This is possible through the functionary pre-trained models chat format or through the generic chatml-function-calling chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d2bf94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:30:31.705784Z",
     "iopub.status.busy": "2024-10-02T15:30:31.705144Z",
     "iopub.status.idle": "2024-10-02T15:30:43.533224Z",
     "shell.execute_reply": "2024-10-02T15:30:43.532106Z"
    },
    "papermill": {
     "duration": 11.905377,
     "end_time": "2024-10-02T15:30:43.536171",
     "exception": false,
     "start_time": "2024-10-02T15:30:31.630794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "llama_perf_context_print:        load time =    8355.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   273 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11202.76 ms /   288 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-5ca2e2b7-70c9-4c41-a9a7-3c6aaca331ce',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1727883032,\n",
       " 'model': '/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': None,\n",
       "    'function_call': {'name': 'UserDetail',\n",
       "     'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'},\n",
       "    'tool_calls': [{'id': 'call__0_UserDetail_cmpl-5ca2e2b7-70c9-4c41-a9a7-3c6aaca331ce',\n",
       "      'type': 'function',\n",
       "      'function': {'name': 'UserDetail',\n",
       "       'arguments': '{ \"name\": \"Jason\", \"age\": 25 }'}}]},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'tool_calls'}],\n",
       " 'usage': {'prompt_tokens': 273, 'completion_tokens': 15, 'total_tokens': 288}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", chat_format=\"chatml-function-calling\")\n",
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"Extract Jason is 25 years old\"\n",
    "        }\n",
    "      ],\n",
    "      tools=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "          \"name\": \"UserDetail\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"UserDetail\",\n",
    "            \"properties\": {\n",
    "              \"name\": {\n",
    "                \"title\": \"Name\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"age\": {\n",
    "                \"title\": \"Age\",\n",
    "                \"type\": \"integer\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [ \"name\", \"age\" ]\n",
    "          }\n",
    "        }\n",
    "      }],\n",
    "      tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "          \"name\": \"UserDetail\"\n",
    "        }\n",
    "      }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f655e",
   "metadata": {
    "papermill": {
     "duration": 0.131507,
     "end_time": "2024-10-02T15:30:43.796156",
     "exception": false,
     "start_time": "2024-10-02T15:30:43.664649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Speculative Decoding\n",
    "\n",
    "llama-cpp-python supports speculative decoding which allows the model to generate completions based on a draft model.\n",
    "\n",
    "The fastest way to use speculative decoding is through the LlamaPromptLookupDecoding class.\n",
    "\n",
    "Just pass this as a draft model to the Llama class during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7535e09b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:30:44.117803Z",
     "iopub.status.busy": "2024-10-02T15:30:44.117259Z",
     "iopub.status.idle": "2024-10-02T15:30:44.749429Z",
     "shell.execute_reply": "2024-10-02T15:30:44.747961Z"
    },
    "papermill": {
     "duration": 0.764258,
     "end_time": "2024-10-02T15:30:44.751675",
     "exception": false,
     "start_time": "2024-10-02T15:30:43.987417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "\n",
    "llama = Llama(\n",
    "    model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\",\n",
    "    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10) # num_pred_tokens is the number of tokens to predict 10 is the default and generally good for gpu, 2 performs better for cpu-only machines.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b047e58",
   "metadata": {
    "papermill": {
     "duration": 0.102732,
     "end_time": "2024-10-02T15:30:44.957969",
     "exception": false,
     "start_time": "2024-10-02T15:30:44.855237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "To generate text embeddings use create_embedding or embed. Note that you must pass embedding=True to the constructor upon model creation for these to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39aaaa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T15:30:45.169638Z",
     "iopub.status.busy": "2024-10-02T15:30:45.168984Z",
     "iopub.status.idle": "2024-10-02T15:30:46.362167Z",
     "shell.execute_reply": "2024-10-02T15:30:46.360975Z"
    },
    "papermill": {
     "duration": 1.302965,
     "end_time": "2024-10-02T15:30:46.364723",
     "exception": false,
     "start_time": "2024-10-02T15:30:45.061758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 218 tensors from /kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Cosmo2 1.7B Webinst Sc2\n",
      "llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = webinst-sc2\n",
      "llama_model_loader: - kv   5:                           general.basename str              = cosmo2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 17\n",
      "llm_load_vocab: token to piece cache size = 0.3170 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49152\n",
      "llm_load_print_meta: n_merges         = 48900\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.71 B\n",
      "llm_load_print_meta: model size       = 3.19 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Cosmo2 1.7B Webinst Sc2\n",
      "llm_load_print_meta: BOS token        = 1 '<|im_start|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: PAD token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: LF token         = 143 'Ä'\n",
      "llm_load_print_meta: EOT token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 0 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 2 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 162\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3264.38 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   100.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '64', 'llama.vocab_size': '49152', 'general.file_type': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'llama.attention.head_count_kv': '32', 'llama.block_count': '24', 'tokenizer.ggml.padding_token_id': '2', 'general.basename': 'cosmo2', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'smollm', 'llama.context_length': '2048', 'general.name': 'Cosmo2 1.7B Webinst Sc2', 'general.organization': 'HuggingFaceTB', 'general.finetune': 'webinst-sc2', 'general.type': 'model', 'general.size_label': '1.7B', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '2048'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n",
      "llama_perf_context_print:        load time =     170.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.86 ms /     5 tokens\n",
      "llama_perf_context_print:        load time =     170.38 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     368.08 ms /    10 tokens\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "\n",
    "llm = llama_cpp.Llama(model_path=\"/kaggle/input/smollm-1.7b-instruct-gguf/gguf/fp16/1/SmolLM-1.7B-Instruct-f16.gguf\", embedding=True)\n",
    "\n",
    "embeddings = llm.create_embedding(\"Hello, world!\")\n",
    "\n",
    "# or create multiple embeddings at once\n",
    "\n",
    "embeddings = llm.create_embedding([\"Hello, world!\", \"Goodbye, world!\"])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 125952,
     "modelInstanceId": 101735,
     "sourceId": 120938,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 179.871315,
   "end_time": "2024-10-02T15:30:47.032458",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-02T15:27:47.161143",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02e5a4ca268e4c54973184e47be9acc1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24f9dd9ee09d4c32ae456ac2d51068c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be4b9ebd12a446738fe4acafc9b3f37f",
       "max": 531065536.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d1f01aa7a42e46bbaf637d5069286589",
       "value": 531065536.0
      }
     },
     "3326d70634774ebbaa569be2cd41c1bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5783e2423d994a71ae320cdcba947a1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8da9d35447374537b45b6fe1a2f1b61b",
        "IPY_MODEL_24f9dd9ee09d4c32ae456ac2d51068c5",
        "IPY_MODEL_edd5cac606344d64a68f572c91bb5dc4"
       ],
       "layout": "IPY_MODEL_02e5a4ca268e4c54973184e47be9acc1"
      }
     },
     "8da9d35447374537b45b6fe1a2f1b61b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b17b83a541c04db0bb940786c4b3475a",
       "placeholder": "​",
       "style": "IPY_MODEL_3326d70634774ebbaa569be2cd41c1bc",
       "value": "qwen2-0_5b-instruct-q8_0.gguf: 100%"
      }
     },
     "ad01afff810b4e0e975b36ff5b1948c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b17b83a541c04db0bb940786c4b3475a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b93fd6e40798493091aab3f4425384c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "be4b9ebd12a446738fe4acafc9b3f37f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1f01aa7a42e46bbaf637d5069286589": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "edd5cac606344d64a68f572c91bb5dc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ad01afff810b4e0e975b36ff5b1948c7",
       "placeholder": "​",
       "style": "IPY_MODEL_b93fd6e40798493091aab3f4425384c9",
       "value": " 531M/531M [00:04&lt;00:00, 281MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
