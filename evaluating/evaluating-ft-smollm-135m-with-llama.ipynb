{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a039e6a",
   "metadata": {
    "papermill": {
     "duration": 0.003423,
     "end_time": "2024-10-01T13:00:58.769760",
     "exception": false,
     "start_time": "2024-10-01T13:00:58.766337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "This note evaluate ft-smollm 135m model with llama.cpp\n",
    "\n",
    "# Env and tools prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e73b540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T13:00:58.776712Z",
     "iopub.status.busy": "2024-10-01T13:00:58.776370Z",
     "iopub.status.idle": "2024-10-01T13:01:48.746313Z",
     "shell.execute_reply": "2024-10-01T13:01:48.745087Z"
    },
    "papermill": {
     "duration": 49.976948,
     "end_time": "2024-10-01T13:01:48.749750",
     "exception": false,
     "start_time": "2024-10-01T13:00:58.772802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/llama-cpp-binary-compiled-with-gpu/llama.cpp /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff6489d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T13:01:48.758805Z",
     "iopub.status.busy": "2024-10-01T13:01:48.758457Z",
     "iopub.status.idle": "2024-10-01T13:02:11.279960Z",
     "shell.execute_reply": "2024-10-01T13:02:11.278505Z"
    },
    "papermill": {
     "duration": 22.528389,
     "end_time": "2024-10-01T13:02:11.282858",
     "exception": false,
     "start_time": "2024-10-01T13:01:48.754469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "!chmod +x -R llama.cpp\n",
    "%cd llama.cpp\n",
    "!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c358f0",
   "metadata": {
    "papermill": {
     "duration": 0.002817,
     "end_time": "2024-10-01T13:02:11.289222",
     "exception": false,
     "start_time": "2024-10-01T13:02:11.286405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run perplexity\n",
    "\n",
    "Perplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\n",
    "Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n",
    "\n",
    "Perplexity benchmark shows how accurate the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab5764d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T13:02:11.297653Z",
     "iopub.status.busy": "2024-10-01T13:02:11.297148Z",
     "iopub.status.idle": "2024-10-01T13:04:52.463148Z",
     "shell.execute_reply": "2024-10-01T13:04:52.462056Z"
    },
    "papermill": {
     "duration": 161.173451,
     "end_time": "2024-10-01T13:04:52.465668",
     "exception": false,
     "start_time": "2024-10-01T13:02:11.292217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/ft-smollm-135m-instruct-on-hf-ultrafeedback-gguf/gguf/fp16/1/ft-smollm-135M-instruct-on-hf-ultrafeedback-f16.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_fp16.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/ft-smollm-135m-instruct-on-hf-ultrafeedback-gguf/gguf/q4/1/ft-smollm-135M-instruct-on-hf-ultrafeedback-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_v2.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/ft-smollm-135m-instruct-on-hf-ultrafeedback-gguf/gguf/q4/1/ft-smollm-135M-instruct-on-hf-ultrafeedback-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873cb6fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T13:04:52.473672Z",
     "iopub.status.busy": "2024-10-01T13:04:52.473075Z",
     "iopub.status.idle": "2024-10-01T13:04:55.438792Z",
     "shell.execute_reply": "2024-10-01T13:04:55.437658Z"
    },
    "papermill": {
     "duration": 2.97211,
     "end_time": "2024-10-01T13:04:55.441117",
     "exception": false,
     "start_time": "2024-10-01T13:04:52.469007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final estimate: PPL = 42.4663 +/- 0.67752\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    2574.87 ms\r\n",
      "llama_perf_context_print: prompt eval time =   37989.66 ms / 131072 tokens (    0.29 ms per token,  3450.20 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   57647.41 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 50.7943 +/- 0.82164\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    1200.24 ms\r\n",
      "llama_perf_context_print: prompt eval time =   28016.71 ms / 131072 tokens (    0.21 ms per token,  4678.35 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   47563.67 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 50.7943 +/- 0.82164\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =    1246.01 ms\r\n",
      "llama_perf_context_print: prompt eval time =   27930.64 ms / 131072 tokens (    0.21 ms per token,  4692.77 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =   47237.90 ms / 131073 tokens\r\n"
     ]
    }
   ],
   "source": [
    "!tail -6 ppl_fp16.log\n",
    "!tail -6 ppl_Q4_v2.log\n",
    "!tail -6 ppl_Q4.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab833964",
   "metadata": {
    "papermill": {
     "duration": 0.003006,
     "end_time": "2024-10-01T13:04:55.447582",
     "exception": false,
     "start_time": "2024-10-01T13:04:55.444576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking the Inference Throughput and Memory Consumption \n",
    "\n",
    "Llama-bench shows performance and resource consumption. \n",
    "There are three types of test\n",
    "- Prompt processing (pp): processing a prompt in batches (-p)\n",
    "- Text generation (tg): generating a sequence of tokens (-n)\n",
    "- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n",
    "\n",
    "In the following test, we run text generation and prompt processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f16bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-01T13:04:55.455543Z",
     "iopub.status.busy": "2024-10-01T13:04:55.455201Z",
     "iopub.status.idle": "2024-10-01T13:12:07.453970Z",
     "shell.execute_reply": "2024-10-01T13:12:07.452995Z"
    },
    "papermill": {
     "duration": 432.005632,
     "end_time": "2024-10-01T13:12:07.456355",
     "exception": false,
     "start_time": "2024-10-01T13:04:55.450723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\n",
      "ggml_cuda_init: found 1 CUDA devices:\r\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\r\n",
      "| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\r\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      6049.36 ± 80.80 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         79.87 ± 2.05 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         79.42 ± 0.40 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         75.73 ± 0.93 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     8095.20 ± 154.06 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        102.57 ± 5.22 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        104.69 ± 1.09 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        100.15 ± 2.14 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12174.12 ± 409.85 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        151.72 ± 3.40 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        153.61 ± 1.80 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        151.00 ± 3.23 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13948.26 ± 14.45 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        241.65 ± 1.38 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        239.79 ± 0.35 |\r\n",
      "| llama ?B F16                   | 310.63 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        232.95 ± 0.41 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      8216.13 ± 44.85 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         94.47 ± 2.82 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         92.83 ± 4.28 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         88.83 ± 2.29 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9816.70 ± 241.40 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        119.91 ± 1.70 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        118.00 ± 0.76 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        110.19 ± 3.94 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12297.41 ± 351.54 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        162.78 ± 2.25 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        163.77 ± 2.02 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        158.15 ± 3.48 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13725.10 ± 38.03 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        227.10 ± 0.59 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        224.38 ± 1.21 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        218.04 ± 0.79 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         pp512 |      8224.10 ± 14.73 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg128 |         95.19 ± 1.58 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg256 |         93.30 ± 0.67 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  10 |          1 |         tg512 |         89.94 ± 2.33 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         pp512 |     9771.70 ± 301.35 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg128 |        121.21 ± 2.96 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg256 |        115.08 ± 5.62 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  20 |          1 |         tg512 |        111.90 ± 1.45 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         pp512 |    12211.76 ± 341.05 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg128 |        167.30 ± 3.58 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg256 |        164.73 ± 6.48 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  30 |          1 |         tg512 |        162.86 ± 1.25 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         pp512 |     13739.89 ± 12.93 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg128 |        227.03 ± 0.33 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg256 |        224.16 ± 1.11 |\r\n",
      "| llama ?B Q4_K - Medium         | 127.55 MiB |   162.83 M | CUDA       |  35 |          1 |         tg512 |        217.78 ± 1.54 |\r\n",
      "\r\n",
      "build: 95bc82fb (3828)\r\n"
     ]
    }
   ],
   "source": [
    "!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/ft-smollm-135m-instruct-on-hf-ultrafeedback-gguf/gguf/fp16/1/ft-smollm-135M-instruct-on-hf-ultrafeedback-f16.gguf -m /kaggle/input/ft-smollm-135m-instruct-on-hf-ultrafeedback-gguf/gguf/q4/1/ft-smollm-135M-instruct-on-hf-ultrafeedback-Q4_K_M.gguf -m /kaggle/input/ft-smollm-135m-instruct-on-hf-ultrafeedback-gguf/gguf/q4/1/ft-smollm-135M-instruct-on-hf-ultrafeedback-Q4_K_M-v2.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 198397091,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125955,
     "modelInstanceId": 101740,
     "sourceId": 120945,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125955,
     "modelInstanceId": 101741,
     "sourceId": 120946,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 672.615596,
   "end_time": "2024-10-01T13:12:07.882234",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-01T13:00:55.266638",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
