{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis note compile llama.cpp with GPU. And will also download wiki text which will be used for perplexity. The output of this node can be used as input of other notes","metadata":{}},{"cell_type":"markdown","source":"# Compile llama.cpp","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp\n%cd llama.cpp\n# !export GGML_CUDA=1 \n!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ \n!make GGML_CUDA=1 CUDA_PATH=/usr/local/nvidia  > make.log 2>&1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tail -1 make.log\n!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n!tail -1 pip_install.log","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download wiki text","metadata":{}},{"cell_type":"code","source":"!wget https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\n!gunzip en.txt.gz\n!head -n 10000 en.txt > en-h10000.txt\n!sh /kaggle/working/llama.cpp/scripts/get-wikitext-2.sh","metadata":{},"execution_count":null,"outputs":[]}]}