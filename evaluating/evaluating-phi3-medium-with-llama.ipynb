{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027172fb",
   "metadata": {
    "papermill": {
     "duration": 0.00348,
     "end_time": "2024-09-29T06:49:49.235938",
     "exception": false,
     "start_time": "2024-09-29T06:49:49.232458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "This note evaluate phi3-medium model with llama.cpp\n",
    "\n",
    "# Env and tools prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15f92e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T06:49:49.243613Z",
     "iopub.status.busy": "2024-09-29T06:49:49.243225Z",
     "iopub.status.idle": "2024-09-29T06:50:38.860284Z",
     "shell.execute_reply": "2024-09-29T06:50:38.859141Z"
    },
    "papermill": {
     "duration": 49.623603,
     "end_time": "2024-09-29T06:50:38.862768",
     "exception": false,
     "start_time": "2024-09-29T06:49:49.239165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/llama-cpp-binary-compiled-with-gpu/llama.cpp /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58252a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T06:50:38.870496Z",
     "iopub.status.busy": "2024-09-29T06:50:38.869847Z",
     "iopub.status.idle": "2024-09-29T06:50:59.399372Z",
     "shell.execute_reply": "2024-09-29T06:50:59.398399Z"
    },
    "papermill": {
     "duration": 20.535978,
     "end_time": "2024-09-29T06:50:59.401874",
     "exception": false,
     "start_time": "2024-09-29T06:50:38.865896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "!chmod +x -R llama.cpp\n",
    "%cd llama.cpp\n",
    "!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ba0ee",
   "metadata": {
    "papermill": {
     "duration": 0.002763,
     "end_time": "2024-09-29T06:50:59.408015",
     "exception": false,
     "start_time": "2024-09-29T06:50:59.405252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run perplexity\n",
    "\n",
    "Perplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\n",
    "Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n",
    "\n",
    "Perplexity benchmark shows how accurate the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b95154ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T06:50:59.415695Z",
     "iopub.status.busy": "2024-09-29T06:50:59.415091Z",
     "iopub.status.idle": "2024-09-29T08:10:37.197718Z",
     "shell.execute_reply": "2024-09-29T08:10:37.185774Z"
    },
    "papermill": {
     "duration": 4777.797369,
     "end_time": "2024-09-29T08:10:37.208322",
     "exception": false,
     "start_time": "2024-09-29T06:50:59.410953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/fp16/1/phi3-medium-128k-instruct.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_fp16.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_v2.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3aa4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T08:10:37.219000Z",
     "iopub.status.busy": "2024-09-29T08:10:37.218597Z",
     "iopub.status.idle": "2024-09-29T08:10:40.294283Z",
     "shell.execute_reply": "2024-09-29T08:10:40.293102Z"
    },
    "papermill": {
     "duration": 3.083755,
     "end_time": "2024-09-29T08:10:40.296900",
     "exception": false,
     "start_time": "2024-09-29T08:10:37.213145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: cannot open 'ppl_fp6.log' for reading: No such file or directory\r\n",
      "Final estimate: PPL = 4.4018 +/- 0.03803\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =   51194.66 ms\r\n",
      "llama_perf_context_print: prompt eval time = 1146232.30 ms / 131072 tokens (    8.75 ms per token,   114.35 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time = 1162115.80 ms / 131073 tokens\r\n",
      "tail: cannot open 'pip_Q4.log' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!tail -6 ppl_fp6.log\n",
    "!tail -6 ppl_Q4_v2.log\n",
    "!tail -6 pip_Q4.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a4c52",
   "metadata": {
    "papermill": {
     "duration": 0.003035,
     "end_time": "2024-09-29T08:10:40.303615",
     "exception": false,
     "start_time": "2024-09-29T08:10:40.300580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking the Inference Throughput and Memory Consumption \n",
    "\n",
    "Llama-bench shows performance and resource consumption. \n",
    "There are three types of test\n",
    "- Prompt processing (pp): processing a prompt in batches (-p)\n",
    "- Text generation (tg): generating a sequence of tokens (-n)\n",
    "- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n",
    "\n",
    "In the following test, we run text generation and prompt processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb13dc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T08:10:40.312794Z",
     "iopub.status.busy": "2024-09-29T08:10:40.312407Z",
     "iopub.status.idle": "2024-09-29T10:24:31.380400Z",
     "shell.execute_reply": "2024-09-29T10:24:31.379307Z"
    },
    "papermill": {
     "duration": 8031.077133,
     "end_time": "2024-09-29T10:24:31.383951",
     "exception": false,
     "start_time": "2024-09-29T08:10:40.306818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\n",
      "ggml_cuda_init: found 1 CUDA devices:\r\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\r\n",
      "| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\r\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         pp512 |         75.35 ± 0.42 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         tg128 |          0.98 ± 0.01 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         tg256 |          0.97 ± 0.01 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         tg512 |          0.97 ± 0.02 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         pp512 |         97.88 ± 0.24 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         tg128 |          1.42 ± 0.00 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         tg256 |          1.42 ± 0.01 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         tg512 |          1.40 ± 0.01 |\r\n",
      "main: error: failed to load model '/kaggle/input/phi3-medium-128k-instruct-gguf/gguf/fp16/1/phi3-medium-128k-instruct.gguf'\r\n"
     ]
    }
   ],
   "source": [
    "!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/fp16/1/phi3-medium-128k-instruct.gguf -m /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M-v2.gguf -m /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 198397091,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125978,
     "modelInstanceId": 101771,
     "sourceId": 120983,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125978,
     "modelInstanceId": 101778,
     "sourceId": 120991,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12885.528363,
   "end_time": "2024-09-29T10:24:31.920802",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-29T06:49:46.392439",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
