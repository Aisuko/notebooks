{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d1db01",
   "metadata": {
    "papermill": {
     "duration": 0.003394,
     "end_time": "2024-09-29T14:02:16.890831",
     "exception": false,
     "start_time": "2024-09-29T14:02:16.887437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "This note evaluate phi3-medium model with llama.cpp\n",
    "\n",
    "# Env and tools prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e534c18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:02:16.898281Z",
     "iopub.status.busy": "2024-09-29T14:02:16.897916Z",
     "iopub.status.idle": "2024-09-29T14:03:20.407187Z",
     "shell.execute_reply": "2024-09-29T14:03:20.405856Z"
    },
    "papermill": {
     "duration": 63.515844,
     "end_time": "2024-09-29T14:03:20.409896",
     "exception": false,
     "start_time": "2024-09-29T14:02:16.894052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/llama-cpp-binary-compiled-with-gpu/llama.cpp /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8075dab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:03:20.417742Z",
     "iopub.status.busy": "2024-09-29T14:03:20.417331Z",
     "iopub.status.idle": "2024-09-29T14:03:40.374325Z",
     "shell.execute_reply": "2024-09-29T14:03:40.373095Z"
    },
    "papermill": {
     "duration": 19.964636,
     "end_time": "2024-09-29T14:03:40.377664",
     "exception": false,
     "start_time": "2024-09-29T14:03:20.413028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "!chmod +x -R llama.cpp\n",
    "%cd llama.cpp\n",
    "!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884aa283",
   "metadata": {
    "papermill": {
     "duration": 0.00292,
     "end_time": "2024-09-29T14:03:40.386677",
     "exception": false,
     "start_time": "2024-09-29T14:03:40.383757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run perplexity\n",
    "\n",
    "Perplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\n",
    "Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n",
    "\n",
    "Perplexity benchmark shows how accurate the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e94c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:03:40.398325Z",
     "iopub.status.busy": "2024-09-29T14:03:40.397101Z",
     "iopub.status.idle": "2024-09-29T15:25:18.162740Z",
     "shell.execute_reply": "2024-09-29T15:25:18.149761Z"
    },
    "papermill": {
     "duration": 4897.781696,
     "end_time": "2024-09-29T15:25:18.172958",
     "exception": false,
     "start_time": "2024-09-29T14:03:40.391262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/fp16/1/phi3-medium-128k-instruct.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_fp16.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M-v2.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4_v2.log 2>&1\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m  /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefb92cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T15:25:18.200836Z",
     "iopub.status.busy": "2024-09-29T15:25:18.200389Z",
     "iopub.status.idle": "2024-09-29T15:25:21.252635Z",
     "shell.execute_reply": "2024-09-29T15:25:21.251419Z"
    },
    "papermill": {
     "duration": 3.061399,
     "end_time": "2024-09-29T15:25:21.255076",
     "exception": false,
     "start_time": "2024-09-29T15:25:18.193677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final estimate: PPL = 4.1981 +/- 0.03582\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =  260727.61 ms\r\n",
      "llama_perf_context_print: prompt eval time = 2157650.93 ms / 131072 tokens (   16.46 ms per token,    60.75 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time = 2172436.78 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 4.4018 +/- 0.03803\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =   64308.35 ms\r\n",
      "llama_perf_context_print: prompt eval time = 1147874.80 ms / 131072 tokens (    8.76 ms per token,   114.19 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time = 1163920.29 ms / 131073 tokens\r\n",
      "Final estimate: PPL = 4.4018 +/- 0.03803\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =   67686.25 ms\r\n",
      "llama_perf_context_print: prompt eval time = 1146733.20 ms / 131072 tokens (    8.75 ms per token,   114.30 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time = 1162483.82 ms / 131073 tokens\r\n"
     ]
    }
   ],
   "source": [
    "!tail -6 ppl_fp16.log\n",
    "!tail -6 ppl_Q4_v2.log\n",
    "!tail -6 ppl_Q4.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4120466",
   "metadata": {
    "papermill": {
     "duration": 0.003139,
     "end_time": "2024-09-29T15:25:21.261617",
     "exception": false,
     "start_time": "2024-09-29T15:25:21.258478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking the Inference Throughput and Memory Consumption \n",
    "\n",
    "Llama-bench shows performance and resource consumption. \n",
    "There are three types of test\n",
    "- Prompt processing (pp): processing a prompt in batches (-p)\n",
    "- Text generation (tg): generating a sequence of tokens (-n)\n",
    "- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n",
    "\n",
    "In the following test, we run text generation and prompt processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3881634f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T15:25:21.270268Z",
     "iopub.status.busy": "2024-09-29T15:25:21.269903Z",
     "iopub.status.idle": "2024-09-29T17:39:32.826167Z",
     "shell.execute_reply": "2024-09-29T17:39:32.825030Z"
    },
    "papermill": {
     "duration": 8051.565296,
     "end_time": "2024-09-29T17:39:32.829994",
     "exception": false,
     "start_time": "2024-09-29T15:25:21.264698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\n",
      "ggml_cuda_init: found 1 CUDA devices:\r\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\r\n",
      "| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\r\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         pp512 |         76.09 ± 0.28 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         tg128 |          0.99 ± 0.00 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         tg256 |          0.99 ± 0.00 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  10 |          1 |         tg512 |          0.98 ± 0.00 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         pp512 |         97.78 ± 0.10 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         tg128 |          1.41 ± 0.00 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         tg256 |          1.41 ± 0.01 |\r\n",
      "| phi3 14B F16                   |  26.00 GiB |    13.96 B | CUDA       |  20 |          1 |         tg512 |          1.40 ± 0.00 |\r\n",
      "main: error: failed to load model '/kaggle/input/phi3-medium-128k-instruct-gguf/gguf/fp16/1/phi3-medium-128k-instruct.gguf'\r\n"
     ]
    }
   ],
   "source": [
    "!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/fp16/1/phi3-medium-128k-instruct.gguf -m /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M-v2.gguf -m /kaggle/input/phi3-medium-128k-instruct-gguf/gguf/q4_k_m/1/phi3-medium-128k-instruct-Q4_K_M.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 198397091,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125978,
     "modelInstanceId": 101771,
     "sourceId": 120983,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125978,
     "modelInstanceId": 101778,
     "sourceId": 120991,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13039.169129,
   "end_time": "2024-09-29T17:39:33.255374",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-29T14:02:14.086245",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
