{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nSemScore offers a way to **monitor semantic similarity of prediction versus reference throughout training**, providing insights into the true progress of the model, beyond what the traditional loss metrics can offer. In this notebook, we will put SemScore into a typical Huggingface Trainer training run.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.38.2\n!pip install accelerate==0.27.2\n!pip install datasets==2.18.0\n!pip install peft==0.9.0\n!pip install bitsandbytes==0.42.0\n!pip install trl==0.7.11","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning tinyllama1-1b-chat\"\nos.environ[\"WANDB_NAME\"] = \"ft-tinyllama1-1b-chat-on-oasst2-top4k\"\n\nos.environ[\"MODEL_NAME\"]=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nos.environ[\"DATASET\"]=\"g-ronimo/oasst2_top4k_en\"\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset=load_dataset(os.getenv(\"DATASET\"), split=[\"train:5000\"])\ndataset=dataset.train_test_split(test=0.1)\ndataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCusalLM\n\ntorch.manual_seed(2024)\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv(MODEL_NAME), use_fast=False)\nmodel=AutoModel.from_pretrained(os.getenv(MODEL_NAME), device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.device","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\nWe define the customize evaluation function and append it through callback function.","metadata":{}},{"cell_type":"code","source":"from transfromers import TrainerCallback\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, BitsAndBytesConfig, set_seed\nfrom peft import LoraConfig\nfrom trl import SFTTrainer, setup_chat_format, DataCollatorForCompletionOnlyLM, \n\nmodel, tokenizer=setup_chat_format(model, tokenizer)\nif tokenizer.pad_token in [None, tokenizer.eos_token]:\n    tokenizer.pad_token=tokenizer.unk_token\n    \nargs=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    evaluation_strategy=\"steps\",\n    label_names=[\"labels\"],\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=4,\n    save_steps=250,\n    eval_steps=250,\n    logging_steps=1,\n    learning_rate=1e-5,\n    num_train_epochs=2,\n    lr_scheduler_type=\"constant\",\n    optim=\"paged_adamw_32bit\",\n    fp16=True,\n    gradient_checkpointing=True,\n    group_by_length=True\n)\n\nsf_trainer=SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=DataCollatorForCompletionOnlyLM(\n        instruction_template=\"<|im_start|>user\",\n        response_template=\"<|im_start|>assistant\",\n        tokenier=tokenizer,\n        mlm=False\n    ),\n    max_seq_length=512,\n    args=args,\n    callbacks=[]\n)\n","metadata":{},"execution_count":null,"outputs":[]}]}