{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We use llama_cpp to evaluate LLM followed by this article:\n\nhttps://towardsdatascience.com/gguf-quantization-with-imatrix-and-k-quantization-to-run-llms-on-your-cpu-02356b531926\n\nOnly use the evaluation part.\n\n# Install llama cpp","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp\n%cd llama.cpp\n# !export GGML_CUDA=1 \n!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ \n!make GGML_CUDA=1 CUDA_PATH=/usr/local/nvidia  > make.log 2>&1","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:26:21.739553Z","iopub.execute_input":"2024-09-24T04:26:21.740493Z","iopub.status.idle":"2024-09-24T04:48:17.389735Z","shell.execute_reply.started":"2024-09-24T04:26:21.740431Z","shell.execute_reply":"2024-09-24T04:48:17.388178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tail make.log\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:48:17.392819Z","iopub.execute_input":"2024-09-24T04:48:17.393329Z","iopub.status.idle":"2024-09-24T04:48:18.386566Z","shell.execute_reply.started":"2024-09-24T04:48:17.393272Z","shell.execute_reply":"2024-09-24T04:48:18.385626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --force-reinstall -r requirements.txt > pip_install.log 2>&1\n!tail pip_install.log","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:48:18.387939Z","iopub.execute_input":"2024-09-24T04:48:18.388277Z","iopub.status.idle":"2024-09-24T04:50:05.344820Z","shell.execute_reply.started":"2024-09-24T04:48:18.388241Z","shell.execute_reply":"2024-09-24T04:50:05.343685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download gguf model\n\n## Prepare huggingface token","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n\nuser_secrets = UserSecretsClient()\n\nos.environ[\"HF_TOKEN\"]=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(os.environ[\"HF_TOKEN\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:50:05.346470Z","iopub.execute_input":"2024-09-24T04:50:05.346786Z","iopub.status.idle":"2024-09-24T04:50:06.026222Z","shell.execute_reply.started":"2024-09-24T04:50:05.346753Z","shell.execute_reply":"2024-09-24T04:50:06.025297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download gguf model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nmodel_name = \"google/gemma-2-2b-it\" # the model we want to quantize\nmethods = ['Q4_K_S','Q4_K_M'] #the methods to be used for quantization\nbase_model = \"./original_model_gemma2-2b/\" # where the FP16 GGUF model will be stored\nquantized_path = \"./quantized_model_gemma2-2b/\" #where the quantized GGUF model will be stored\noriginal_model = quantized_path + 'FP16.gguf'\n\nsnapshot_download(repo_id=model_name, local_dir=base_model , local_dir_use_symlinks=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:50:06.028512Z","iopub.execute_input":"2024-09-24T04:50:06.028822Z","iopub.status.idle":"2024-09-24T04:50:39.437944Z","shell.execute_reply.started":"2024-09-24T04:50:06.028789Z","shell.execute_reply":"2024-09-24T04:50:39.437010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert model to gguf ","metadata":{}},{"cell_type":"code","source":"!mkdir -p /kaggle/working/llama.cpp/quantized_model_gemma2-2b/\n!python convert_hf_to_gguf.py \"/kaggle/working/llama.cpp/original_model_gemma2-2b/\" --outfile \"/kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf\"","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:50:39.439125Z","iopub.execute_input":"2024-09-24T04:50:39.439480Z","iopub.status.idle":"2024-09-24T04:51:11.587689Z","shell.execute_reply.started":"2024-09-24T04:50:39.439446Z","shell.execute_reply":"2024-09-24T04:51:11.586557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get wiki text as dataset","metadata":{}},{"cell_type":"code","source":"!wget https://object.pouta.csc.fi/OPUS-Wikipedia/v1.0/mono/en.txt.gz\n!gunzip en.txt.gz\n!head -n 10000 en.txt > en-h10000.txt\n!sh scripts/get-wikitext-2.sh","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:51:11.589293Z","iopub.execute_input":"2024-09-24T04:51:11.590209Z","iopub.status.idle":"2024-09-24T04:52:00.587234Z","shell.execute_reply.started":"2024-09-24T04:51:11.590139Z","shell.execute_reply":"2024-09-24T04:52:00.586118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmarking the Perplexity \n\nPerplexity can be used to compare the models before and after quantization or other method. Here is a explaination of why we could not compare different models by benchmarking perplexity\n\nhttps://thesalt.substack.com/p/why-cant-we-compare-the-perplexity\n","metadata":{}},{"cell_type":"code","source":"!./llama-perplexity -m /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf -f wikitext-2-raw/wiki.test.raw --chunks 16","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:52:00.588881Z","iopub.execute_input":"2024-09-24T04:52:00.589258Z","iopub.status.idle":"2024-09-24T04:52:38.128005Z","shell.execute_reply.started":"2024-09-24T04:52:00.589220Z","shell.execute_reply":"2024-09-24T04:52:38.126891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Benchmarking the Inference Throughput and Memory Consumption ","metadata":{}},{"cell_type":"code","source":"!./llama-bench -m /kaggle/working/llama.cpp/quantized_model_gemma2-2b/FP16.gguf -n 16 -mg 1","metadata":{"execution":{"iopub.status.busy":"2024-09-24T04:52:38.129446Z","iopub.execute_input":"2024-09-24T04:52:38.129787Z","iopub.status.idle":"2024-09-24T04:52:45.434278Z","shell.execute_reply.started":"2024-09-24T04:52:38.129751Z","shell.execute_reply":"2024-09-24T04:52:45.433245Z"},"trusted":true},"execution_count":null,"outputs":[]}]}