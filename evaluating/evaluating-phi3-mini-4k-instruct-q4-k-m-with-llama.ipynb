{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2326e90",
   "metadata": {
    "papermill": {
     "duration": 0.003241,
     "end_time": "2024-09-29T02:41:09.189881",
     "exception": false,
     "start_time": "2024-09-29T02:41:09.186640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "This note evaluate phi3-mini-4k-instruct with llama.cpp\n",
    "\n",
    "# Env and tools prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5446b512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T02:41:09.197662Z",
     "iopub.status.busy": "2024-09-29T02:41:09.196779Z",
     "iopub.status.idle": "2024-09-29T02:42:16.726295Z",
     "shell.execute_reply": "2024-09-29T02:42:16.725282Z"
    },
    "papermill": {
     "duration": 67.535757,
     "end_time": "2024-09-29T02:42:16.728626",
     "exception": false,
     "start_time": "2024-09-29T02:41:09.192869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/llama-cpp-binary-compiled-with-gpu/llama.cpp /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb961803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T02:42:16.736105Z",
     "iopub.status.busy": "2024-09-29T02:42:16.735728Z",
     "iopub.status.idle": "2024-09-29T02:42:36.783847Z",
     "shell.execute_reply": "2024-09-29T02:42:36.782626Z"
    },
    "papermill": {
     "duration": 20.054823,
     "end_time": "2024-09-29T02:42:36.786417",
     "exception": false,
     "start_time": "2024-09-29T02:42:16.731594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "!chmod +x -R llama.cpp\n",
    "%cd llama.cpp\n",
    "!cp -r /usr/local/cuda-12.3/targets /usr/local/nvidia/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdfa38d",
   "metadata": {
    "papermill": {
     "duration": 0.002691,
     "end_time": "2024-09-29T02:42:36.792302",
     "exception": false,
     "start_time": "2024-09-29T02:42:36.789611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run perplexity\n",
    "\n",
    "Perplexity is the main evaluation metric for large language models (LLMs). It measures how well the model predicts a given sequence of tokens.\n",
    "Formally, the perplexity of LLMs is the exponentiated average negative log-likelihood,  which means **perplexity smaller is better** \n",
    "\n",
    "Perplexity benchmark shows how accurate the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c4bdb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T02:42:36.799399Z",
     "iopub.status.busy": "2024-09-29T02:42:36.799014Z",
     "iopub.status.idle": "2024-09-29T02:49:24.215679Z",
     "shell.execute_reply": "2024-09-29T02:49:24.214422Z"
    },
    "papermill": {
     "duration": 407.423219,
     "end_time": "2024-09-29T02:49:24.218314",
     "exception": false,
     "start_time": "2024-09-29T02:42:36.795095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!/kaggle/working/llama.cpp/llama-perplexity -m /kaggle/input/phi3-mini-4k-instruct-q4_k_m/gguf/q4_k_m/1/Phi3-mini-4k-instruct-Q4.gguf -f /kaggle/working/llama.cpp/wikitext-2-raw/wiki.test.raw --chunks 256 > ppl_Q4.log 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bb0484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T02:49:24.225836Z",
     "iopub.status.busy": "2024-09-29T02:49:24.225490Z",
     "iopub.status.idle": "2024-09-29T02:49:25.202043Z",
     "shell.execute_reply": "2024-09-29T02:49:25.201123Z"
    },
    "papermill": {
     "duration": 0.982819,
     "end_time": "2024-09-29T02:49:25.204291",
     "exception": false,
     "start_time": "2024-09-29T02:49:24.221472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final estimate: PPL = 7.1613 +/- 0.07725\r\n",
      "\r\n",
      "llama_perf_context_print:        load time =   14828.09 ms\r\n",
      "llama_perf_context_print: prompt eval time =  371183.33 ms / 131072 tokens (    2.83 ms per token,   353.12 tokens per second)\r\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\r\n",
      "llama_perf_context_print:       total time =  391160.00 ms / 131073 tokens\r\n"
     ]
    }
   ],
   "source": [
    "!tail -6 ppl_Q4.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440d5a0",
   "metadata": {
    "papermill": {
     "duration": 0.002755,
     "end_time": "2024-09-29T02:49:25.210186",
     "exception": false,
     "start_time": "2024-09-29T02:49:25.207431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmarking the Inference Throughput and Memory Consumption \n",
    "\n",
    "Llama-bench shows performance and resource consumption. \n",
    "There are three types of test\n",
    "- Prompt processing (pp): processing a prompt in batches (-p)\n",
    "- Text generation (tg): generating a sequence of tokens (-n)\n",
    "- Prompt processing + text generation (pg): processing a prompt followed by generating a sequence of tokens (-pg)\n",
    "\n",
    "In the following test, we run text generation and prompt processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a690e2fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T02:49:25.217586Z",
     "iopub.status.busy": "2024-09-29T02:49:25.217247Z",
     "iopub.status.idle": "2024-09-29T03:08:59.615435Z",
     "shell.execute_reply": "2024-09-29T03:08:59.614274Z"
    },
    "papermill": {
     "duration": 1174.404533,
     "end_time": "2024-09-29T03:08:59.617800",
     "exception": false,
     "start_time": "2024-09-29T02:49:25.213267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\r\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\r\n",
      "ggml_cuda_init: found 1 CUDA devices:\r\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\r\n",
      "| model                          |       size |     params | backend    | ngl |   main_gpu |          test |                  t/s |\r\n",
      "| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------: | -------------------: |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  10 |          1 |         pp512 |        462.15 ± 0.53 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  10 |          1 |         tg128 |          8.03 ± 0.13 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  10 |          1 |         tg256 |          7.87 ± 0.05 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  10 |          1 |         tg512 |          7.54 ± 0.03 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  20 |          1 |         pp512 |        561.79 ± 0.51 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  20 |          1 |         tg128 |         12.83 ± 0.21 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  20 |          1 |         tg256 |         12.66 ± 0.10 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  20 |          1 |         tg512 |         12.07 ± 0.09 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  30 |          1 |         pp512 |        716.22 ± 0.31 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  30 |          1 |         tg128 |         34.21 ± 0.08 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  30 |          1 |         tg256 |         33.64 ± 0.32 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  30 |          1 |         tg512 |         32.54 ± 0.23 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  35 |          1 |         pp512 |        766.76 ± 0.28 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  35 |          1 |         tg128 |         67.50 ± 0.06 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  35 |          1 |         tg256 |         66.95 ± 0.10 |\r\n",
      "| phi3 3B Q4_K - Medium          |   2.23 GiB |     3.82 B | CUDA       |  35 |          1 |         tg512 |         65.71 ± 0.03 |\r\n",
      "\r\n",
      "build: 95bc82fb (3828)\r\n"
     ]
    }
   ],
   "source": [
    "!/kaggle/working/llama.cpp/llama-bench -m /kaggle/input/phi3-mini-4k-instruct-q4_k_m/gguf/q4_k_m/1/Phi3-mini-4k-instruct-Q4.gguf -mg 1 -ngl 10,20,30,35 -n 128,256,512 "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 198397091,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 125956,
     "modelInstanceId": 101744,
     "sourceId": 120949,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1673.518686,
   "end_time": "2024-09-29T03:08:59.941118",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-29T02:41:06.422432",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
