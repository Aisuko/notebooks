{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/coding-the-multi-head-attention?scriptVersionId=160449289\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"d6c2396c","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.006021,"end_time":"2024-01-26T01:34:42.078175","exception":false,"start_time":"2024-01-26T01:34:42.072154","status":"completed"},"tags":[]},"source":["# Overview\n","\n","**Note: The images are from the Credit section**\n","\n","In the [Coding the Self-Attention Mechanism](https://www.kaggle.com/code/aisuko/coding-the-self-attention-mechanism) we implement self-attention machanim. And from [Encoder in Transformers Architecture](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture), we can see that transformers use a module called **multi-head-attention**. In this notebook, we will talk about How does that relate to the self attention again by implementing it in code.\n","\n","In the scaled dot-product attention(self-attention), the input sequence was transformed using three matrices representingt the query, key and values. These three matrices can be considered as a single attention head in the conext of multi-head attention. The figure below summarizes this single attention head we covered in the previously notebook above.\n","\n","<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/814/921/458/820/088/original/1fb77b4eb89e6718.png\" width=\"80%\" heigh=\"80%\" alt=\"Scaled-dot-product attention\"></div>"]},{"cell_type":"markdown","id":"7fac6d2a","metadata":{"papermill":{"duration":0.005306,"end_time":"2024-01-26T01:34:42.089736","exception":false,"start_time":"2024-01-26T01:34:42.08443","status":"completed"},"tags":[]},"source":["# Multi-Head Attention\n","\n","As its name implies, multi-head attention involves multiple such heads, each consisting of query, key, and value matrices. This concept is similar to the use of multiple kernels in convolutional neural networks.\n","\n","<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/814/925/242/665/171/original/821c33b401832d5d.png\" width=\"80%\" heigh=\"80%\" alt=\"multi-head attention\"></div>\n","\n","Here the code from the previouly notebook below:"]},{"cell_type":"code","execution_count":1,"id":"8c3e3995","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:42.103676Z","iopub.status.busy":"2024-01-26T01:34:42.103121Z","iopub.status.idle":"2024-01-26T01:34:46.092Z","shell.execute_reply":"2024-01-26T01:34:46.090682Z"},"papermill":{"duration":3.9989,"end_time":"2024-01-26T01:34:46.094619","exception":false,"start_time":"2024-01-26T01:34:42.095719","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([-2.1845, -3.4618, -2.5052, -3.4871, -2.2224, -3.4605, -3.9543, -4.4065,\n","        -4.7564, -4.1877, -2.8166, -4.1730, -3.3587, -3.0407, -4.5513, -4.7335,\n","        -1.3817, -2.6396, -2.3683, -2.7940, -3.4905, -4.4358, -5.2125, -4.3044,\n","        -3.0761, -3.4201, -4.7494, -3.3475], grad_fn=<SqueezeBackward4>)"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn.functional as F\n","\n","inputs=\"According to the news, it it hard to say Melbourne is safe now\"\n","d_q, d_k, d_v=24,24,28\n","\n","input_ids={s:i for i,s in enumerate(sorted(inputs.replace(',','').split()))}\n","input_tokens=torch.tensor([input_ids[s] for s in inputs.replace(',','').split()])\n","\n","torch.manual_seed(123)\n","embed=torch.nn.Embedding(13,16)\n","embedded_sentence=embed(input_tokens).detach()\n","d=embedded_sentence.shape[1]\n","\n","# defining the Weight Matrices\n","W_query=torch.nn.Parameter(torch.rand(d_q, d))\n","W_key=torch.nn.Parameter(torch.rand(d_k, d))\n","W_value=torch.nn.Parameter(torch.rand(d_v, d))\n","\n","# only computing the attention-vector for the second input element\n","# In this example, the second input element acts as the query\n","x_2 = embedded_sentence[1]\n","query_2 = W_query.matmul(x_2)\n","key_2 = W_key.matmul(x_2)\n","value_2 = W_value.matmul(x_2)\n","\n","# computing the key and value for all inputs\n","keys = W_key.matmul(embedded_sentence.T).T\n","values = W_value.matmul(embedded_sentence.T).T\n","\n","# computing the unnormalized attention wieghts w\n","\n","# in this example, we compute the query and the 5th input element(the index position is 4) as follows\n","w_2_4=query_2.dot(keys[4])\n","\n","# compute the unnormalized attention weight for all the input tokens\n","w_2=query_2.matmul(keys.T)\n","\n","attention_weights_2=F.softmax(w_2/d_k**0.5, dim=0)\n","\n","# The final context vector(an attention-weighted version of the original query input x_2)\n","context_vector_2=attention_weights_2.matmul(values)\n","context_vector_2"]},{"cell_type":"markdown","id":"65da6522","metadata":{"papermill":{"duration":0.00532,"end_time":"2024-01-26T01:34:46.105879","exception":false,"start_time":"2024-01-26T01:34:46.100559","status":"completed"},"tags":[]},"source":["And To illustrate this in code, suppose we have 3 attention heads, so we now extend the $d^{'}*d$ dimensional weight matrices so $3*d^{'}*d$:"]},{"cell_type":"code","execution_count":2,"id":"62a11c08","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.120686Z","iopub.status.busy":"2024-01-26T01:34:46.11924Z","iopub.status.idle":"2024-01-26T01:34:46.127118Z","shell.execute_reply":"2024-01-26T01:34:46.125985Z"},"papermill":{"duration":0.018326,"end_time":"2024-01-26T01:34:46.129764","exception":false,"start_time":"2024-01-26T01:34:46.111438","status":"completed"},"tags":[]},"outputs":[],"source":["h=3\n","multihead_W_query=torch.nn.Parameter(torch.rand(h, d_q, d))\n","multihead_W_key=torch.nn.Parameter(torch.rand(h, d_k, d))\n","multihead_W_value=torch.nn.Parameter(torch.rand(h, d_v, d))"]},{"cell_type":"markdown","id":"9ae3a8ef","metadata":{"papermill":{"duration":0.005293,"end_time":"2024-01-26T01:34:46.140906","exception":false,"start_time":"2024-01-26T01:34:46.135613","status":"completed"},"tags":[]},"source":["Consequently, each query element is now $3*d_{q}$ dimensional, where $d_{q}=24$ (here, let's keep the focus on the 3rd element corresponding to index position 2):"]},{"cell_type":"code","execution_count":3,"id":"10925c41","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.154663Z","iopub.status.busy":"2024-01-26T01:34:46.154063Z","iopub.status.idle":"2024-01-26T01:34:46.165199Z","shell.execute_reply":"2024-01-26T01:34:46.163938Z"},"papermill":{"duration":0.0215,"end_time":"2024-01-26T01:34:46.168001","exception":false,"start_time":"2024-01-26T01:34:46.146501","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[-3.8033, -1.8514, -3.0982, -1.6475, -1.7888, -3.1605, -2.3619, -0.5279,\n","         -3.6521, -3.4834, -3.6471, -3.2028, -0.6245, -1.6851, -1.0399, -3.3090,\n","         -2.1283, -5.2142, -1.6018, -0.4544, -3.1030, -0.0287, -4.3965, -2.2998],\n","        [-4.1517, -4.6697, -1.6747, -1.1715, -3.5441, -0.4090, -1.6129, -4.4261,\n","         -2.1847, -2.9327, -2.6157, -3.1685, -1.9501, -2.9855, -3.1613, -1.2670,\n","         -0.5295, -1.1895, -0.4661, -2.3916, -0.9902,  0.3367, -0.4596, -2.9863],\n","        [-1.6977, -1.6078, -3.5137, -4.9699, -4.1886, -0.7016, -3.3832, -3.2597,\n","         -2.1036, -4.3422, -1.9974, -1.7627, -2.9813, -1.5485,  0.0060, -1.7442,\n","         -5.0369, -4.2576, -1.7272, -0.5214, -2.1458, -2.9699, -1.4175, -0.9593]],\n","       grad_fn=<UnsafeViewBackward0>)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["multihead_query_2=multihead_W_query.matmul(x_2)\n","multihead_query_2"]},{"cell_type":"code","execution_count":4,"id":"4bd62258","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.182582Z","iopub.status.busy":"2024-01-26T01:34:46.18199Z","iopub.status.idle":"2024-01-26T01:34:46.190084Z","shell.execute_reply":"2024-01-26T01:34:46.188867Z"},"papermill":{"duration":0.018414,"end_time":"2024-01-26T01:34:46.192345","exception":false,"start_time":"2024-01-26T01:34:46.173931","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([3, 24])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["multihead_query_2.shape"]},{"cell_type":"markdown","id":"982b0c72","metadata":{"papermill":{"duration":0.005605,"end_time":"2024-01-26T01:34:46.203885","exception":false,"start_time":"2024-01-26T01:34:46.19828","status":"completed"},"tags":[]},"source":["Let's do the computing for keys and values"]},{"cell_type":"code","execution_count":5,"id":"b0b357dd","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.218697Z","iopub.status.busy":"2024-01-26T01:34:46.218105Z","iopub.status.idle":"2024-01-26T01:34:46.223889Z","shell.execute_reply":"2024-01-26T01:34:46.222831Z"},"papermill":{"duration":0.016183,"end_time":"2024-01-26T01:34:46.226033","exception":false,"start_time":"2024-01-26T01:34:46.20985","status":"completed"},"tags":[]},"outputs":[],"source":["multihead_key_2=multihead_W_key.matmul(x_2)\n","multihead_value_2=multihead_W_value.matmul(x_2)"]},{"cell_type":"markdown","id":"f49d84cb","metadata":{"papermill":{"duration":0.005459,"end_time":"2024-01-26T01:34:46.237286","exception":false,"start_time":"2024-01-26T01:34:46.231827","status":"completed"},"tags":[]},"source":["Now, these ket and value elements are specific to the query element. But. similar to earlier, we will also need the values and keys for the other sequence elements in order to compute the attention scores for the query. We can do this by expanding the input sequence embeddings to size 3(the number of attention heads):"]},{"cell_type":"code","execution_count":6,"id":"30d7d1b1","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.251844Z","iopub.status.busy":"2024-01-26T01:34:46.251285Z","iopub.status.idle":"2024-01-26T01:34:46.258221Z","shell.execute_reply":"2024-01-26T01:34:46.257225Z"},"papermill":{"duration":0.016979,"end_time":"2024-01-26T01:34:46.260425","exception":false,"start_time":"2024-01-26T01:34:46.243446","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([13, 16])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["embedded_sentence.shape"]},{"cell_type":"code","execution_count":7,"id":"9cf97ae3","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.274867Z","iopub.status.busy":"2024-01-26T01:34:46.274326Z","iopub.status.idle":"2024-01-26T01:34:46.2829Z","shell.execute_reply":"2024-01-26T01:34:46.281491Z"},"papermill":{"duration":0.019536,"end_time":"2024-01-26T01:34:46.285768","exception":false,"start_time":"2024-01-26T01:34:46.266232","status":"completed"},"tags":[]},"outputs":[],"source":["stacked_inputs=embedded_sentence.T.repeat(3,1,1)"]},{"cell_type":"code","execution_count":8,"id":"fa0434c6","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.301281Z","iopub.status.busy":"2024-01-26T01:34:46.300315Z","iopub.status.idle":"2024-01-26T01:34:46.309371Z","shell.execute_reply":"2024-01-26T01:34:46.307978Z"},"papermill":{"duration":0.01945,"end_time":"2024-01-26T01:34:46.311981","exception":false,"start_time":"2024-01-26T01:34:46.292531","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([3, 16, 13])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["stacked_inputs.shape"]},{"cell_type":"markdown","id":"c93b55ad","metadata":{"papermill":{"duration":0.005934,"end_time":"2024-01-26T01:34:46.324229","exception":false,"start_time":"2024-01-26T01:34:46.318295","status":"completed"},"tags":[]},"source":["Now, we cam compute all the keys and values using `torch.bmm()` (batch matrix multiplication)"]},{"cell_type":"code","execution_count":9,"id":"87709bda","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.340549Z","iopub.status.busy":"2024-01-26T01:34:46.339969Z","iopub.status.idle":"2024-01-26T01:34:46.355627Z","shell.execute_reply":"2024-01-26T01:34:46.354287Z"},"papermill":{"duration":0.027564,"end_time":"2024-01-26T01:34:46.358248","exception":false,"start_time":"2024-01-26T01:34:46.330684","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 24, 13])\n","torch.Size([3, 28, 13])\n"]}],"source":["multihead_keys=torch.bmm(multihead_W_key, stacked_inputs)\n","multihead_values=torch.bmm(multihead_W_value, stacked_inputs)\n","print(multihead_keys.shape)\n","print(multihead_values.shape)"]},{"cell_type":"markdown","id":"41208e8b","metadata":{"papermill":{"duration":0.00614,"end_time":"2024-01-26T01:34:46.370811","exception":false,"start_time":"2024-01-26T01:34:46.364671","status":"completed"},"tags":[]},"source":["We now have tensors that represent the three attention heads in their first dimension. The third and second dimensions refer to the number of words and the embedding size, respectively. To make the values and keys more intuitive to interpret, we will swap the second and third dimensions, resulting in tensors with the same dimensional structure as the original input sequence, **embedded_sentence**:"]},{"cell_type":"code","execution_count":10,"id":"35a61ba4","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.387505Z","iopub.status.busy":"2024-01-26T01:34:46.386174Z","iopub.status.idle":"2024-01-26T01:34:46.395063Z","shell.execute_reply":"2024-01-26T01:34:46.393291Z"},"papermill":{"duration":0.020417,"end_time":"2024-01-26T01:34:46.397437","exception":false,"start_time":"2024-01-26T01:34:46.37702","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 13, 24])\n","torch.Size([3, 13, 28])\n"]}],"source":["multihead_keys=multihead_keys.permute(0,2,1)\n","multihead_values=multihead_values.permute(0,2,1)\n","print(multihead_keys.shape)\n","print(multihead_values.shape)"]},{"cell_type":"markdown","id":"ce66deaf","metadata":{"papermill":{"duration":0.005948,"end_time":"2024-01-26T01:34:46.409939","exception":false,"start_time":"2024-01-26T01:34:46.403991","status":"completed"},"tags":[]},"source":["We follow the same steps as previously to compute the unscaled attention weights $w$ and attention weights $\\alpha$, followed the scaled-softmax computation to obtain an $h*d_{v}$(here $3*d_{v}$) dimensional context vector $z$ for the input element $x^{(2)}$.\n","\n","Let's summarize the code in a compact `MultiHeadAttentionWrapper` class:"]},{"cell_type":"code","execution_count":11,"id":"c1a14999","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.425373Z","iopub.status.busy":"2024-01-26T01:34:46.424364Z","iopub.status.idle":"2024-01-26T01:34:46.448041Z","shell.execute_reply":"2024-01-26T01:34:46.446759Z"},"papermill":{"duration":0.034448,"end_time":"2024-01-26T01:34:46.450566","exception":false,"start_time":"2024-01-26T01:34:46.416118","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[ 3.1373],\n","        [-2.6629],\n","        [ 3.2493],\n","        [-2.4250],\n","        [-2.3970],\n","        [-2.3970],\n","        [-2.6269],\n","        [-2.6629],\n","        [-2.4197],\n","        [-2.4037],\n","        [ 3.3277],\n","        [-2.4113],\n","        [-2.6131]], grad_fn=<MmBackward0>)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, d_in, d_out_kq, d_out_v):\n","        super().__init__()\n","        self.d_out_kq=d_out_kq\n","        self.W_query=nn.Parameter(torch.rand(d_in, d_out_kq))\n","        self.W_key=nn.Parameter(torch.rand(d_in, d_out_kq))\n","        self.W_value=nn.Parameter(torch.rand(d_in, d_out_v))\n","        \n","    def forward(self, x):\n","        keys=x.matmul(self.W_key)\n","        queries=x.matmul(self.W_query)\n","        values=x.matmul(self.W_value)\n","        \n","        # unnormalized attention weights\n","        attn_scores=queries.matmul(keys.T)\n","        \n","        attn_weights=torch.softmax(\n","            attn_scores/self.d_out_kq**0.5, dim=-1\n","        )\n","        \n","        context_vex=attn_weights.matmul(values)\n","        return context_vex\n","\n","class MultiHeadAttentionWrapper(nn.Module):\n","    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n","        super().__init__()\n","        self.heads=nn.ModuleList(\n","            [SelfAttention(d_in, d_out_kq, d_out_v) for _ in range(num_heads)]\n","        )\n","        \n","    def forward(self, x):\n","        return torch.cat([head(x) for head in self.heads], dim=-1) #dim=-1, the last dimension\n","    \n","\n","torch.manual_seed(123)\n","# Let's suppose we have a single Self-Attention head with output dimension 1 to keep it simple\n","# d_in: Dimension of the input feature vector(embedded vector, here is 16, see above)\n","# d_out_kq: Dimension for both query and key outputs\n","# d_out_v: Dimension for value outputs\n","# num_heads: Number of attention heads\n","d_in,d_out_kq,d_out_v=16,2,1\n","\n","sa=SelfAttention(d_in, d_out_kq, d_out_v)\n","sa(embedded_sentence)"]},{"cell_type":"code","execution_count":12,"id":"3ee30080","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:34:46.466386Z","iopub.status.busy":"2024-01-26T01:34:46.465438Z","iopub.status.idle":"2024-01-26T01:34:46.481672Z","shell.execute_reply":"2024-01-26T01:34:46.480566Z"},"papermill":{"duration":0.027311,"end_time":"2024-01-26T01:34:46.484328","exception":false,"start_time":"2024-01-26T01:34:46.457017","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 3.1373,  2.1516, -1.6622, -1.1571],\n","        [-2.6629, -1.9705, -2.2620, -3.2353],\n","        [ 3.2493, -2.0611, -2.0625, -3.0395],\n","        [-2.4250, -1.9014, -2.1996, -3.3473],\n","        [-2.3970, -1.8704, -2.0173, -3.3243],\n","        [-2.3970, -1.8704, -2.0173, -3.3243],\n","        [-2.6269, -2.3509, -2.2068, -3.2777],\n","        [-2.6629, -1.9705, -2.2620, -3.2353],\n","        [-2.4197, -1.7686, -2.1098, -3.1776],\n","        [-2.4037, -1.2218, -0.3614, -3.1320],\n","        [ 3.3277,  2.1627, -0.5280, -2.0840],\n","        [-2.4113, -1.7658, -2.2009, -3.4129],\n","        [-2.6131, -1.7475, -1.7935, -2.9953]], grad_fn=<CatBackward0>)\n","Shape:torch.Size([13, 4])\n"]}],"source":["torch.manual_seed(123)\n","\n","block_size=embedded_sentence.shape[1]\n","mha=MultiHeadAttentionWrapper(d_in, d_out_kq, d_out_v, num_heads=4)\n","\n","context_vecs=mha(embedded_sentence)\n","\n","print(context_vecs)\n","print(f\"Shape:{context_vecs.shape}\")"]},{"cell_type":"markdown","id":"926ab96a","metadata":{"papermill":{"duration":0.006087,"end_time":"2024-01-26T01:34:46.496968","exception":false,"start_time":"2024-01-26T01:34:46.490881","status":"completed"},"tags":[]},"source":["Based on the output above, we can see that the single self-attention head created earlier now represents the first column in the output tensor above.\n","\n","Notice that the multi-head attention result is a 13x4-dimensional tensor: We have 13 input tokens and 4 self-attention heads, where each self-attention head returns a 1-dimensional output(we set `d_out_v=1`). \n","\n","In practice, why do we even need multiple attention heads if we can regulate the output embedding size in the `SelfAttention` class itself?\n","\n","The distinction between increasing the output dimension of a single self-attention head multiple attention heads **lies in how the model processes and learns from the data**. While both approaches increase the capacity of the model to represent different features or aspects of the data, they do so infundamentally different ways.\n","\n","For instance, **each attention head in multi-head attention can potentially learn to focus on different parts of the input sequence**, **capturing various aspects** or **relationships within the data**. This **diversity in representation is key to the success of multi-head attention**.\n","\n","Multi-head attention can also **be more efficient, especially in terms of parallel computation**. Each head can be processed independently, making it well-suited for modern hardware accelerators like GPUs or TPUs that excel at parallel processing.\n","\n","In short, the use of multiple attention heads is not just about increasing the model's capacity but about enhacing its ability to learn a diverse set of features and relationships within the data. For example, the 7B Llama 2 modle uses 32 attention heads."]},{"cell_type":"markdown","id":"99ae9084","metadata":{"papermill":{"duration":0.005904,"end_time":"2024-01-26T01:34:46.509296","exception":false,"start_time":"2024-01-26T01:34:46.503392","status":"completed"},"tags":[]},"source":["# Credit\n","\n","* https://magazine.sebastianraschka.com?utm_source=navbar&utm_medium=web&r=fbe14\n","* https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":8.770424,"end_time":"2024-01-26T01:34:47.643031","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-26T01:34:38.872607","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}