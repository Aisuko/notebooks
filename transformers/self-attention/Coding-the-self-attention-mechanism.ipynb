{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/coding-the-self-attention-mechanism?scriptVersionId=160447271\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"6e3c4560","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.009276,"end_time":"2024-01-26T01:03:08.82057","exception":false,"start_time":"2024-01-26T01:03:08.811294","status":"completed"},"tags":[]},"source":["# Overview\n","\n","**Note: The images are from the Credit section**\n","\n","Please check the [Encoder In Transformers architecture](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture) and [Decoder In Transformers architecture](https://www.kaggle.com/code/aisuko/decoder-in-transformers-architecture) to familar **self-attention** in transformers architecture.\n","\n","In this notebook, we focus on the **scaled-dot product attention mechanism(referred to as self-attention)**, which remains the most populat and most widely used attention mechanism in practice. And there are existed other types of attention machanisms, like [2020 Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732) and the [2023 A Survey on Effcient Training of Transformers](https://arxiv.org/abs/2302.01107) review and the [FlashAttention](https://arxiv.org/abs/2205.14135) paper."]},{"cell_type":"markdown","id":"f990fa50","metadata":{"papermill":{"duration":0.008627,"end_time":"2024-01-26T01:03:08.837953","exception":false,"start_time":"2024-01-26T01:03:08.829326","status":"completed"},"tags":[]},"source":["# Embedding an Input Sentence\n","\n","Through the \"Encoder in Transformers architecture\", we know the first steps is tokenization, normalization and embedding the tokenzes. Here we do not need to normlize the inputs."]},{"cell_type":"code","execution_count":1,"id":"129a4aa4","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:08.858032Z","iopub.status.busy":"2024-01-26T01:03:08.857229Z","iopub.status.idle":"2024-01-26T01:03:08.875629Z","shell.execute_reply":"2024-01-26T01:03:08.874121Z"},"papermill":{"duration":0.031567,"end_time":"2024-01-26T01:03:08.878269","exception":false,"start_time":"2024-01-26T01:03:08.846702","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'According': 0,\n"," 'Melbourne': 1,\n"," 'hard': 2,\n"," 'is': 3,\n"," 'it': 5,\n"," 'news': 6,\n"," 'now': 7,\n"," 'safe': 8,\n"," 'say': 9,\n"," 'the': 10,\n"," 'to': 12}"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["inputs=\"According to the news, it it hard to say Melbourne is safe now\"\n","\n","input_ids={s:i for i,s in enumerate(sorted(inputs.replace(',','').split()))}\n","input_ids"]},{"cell_type":"markdown","id":"784a2c5e","metadata":{"papermill":{"duration":0.008874,"end_time":"2024-01-26T01:03:08.896114","exception":false,"start_time":"2024-01-26T01:03:08.88724","status":"completed"},"tags":[]},"source":["Let's convert them to the tokens(assign an integer index to each word)."]},{"cell_type":"code","execution_count":2,"id":"5242e540","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:08.916747Z","iopub.status.busy":"2024-01-26T01:03:08.916248Z","iopub.status.idle":"2024-01-26T01:03:12.928533Z","shell.execute_reply":"2024-01-26T01:03:12.927143Z"},"papermill":{"duration":4.025699,"end_time":"2024-01-26T01:03:12.931199","exception":false,"start_time":"2024-01-26T01:03:08.9055","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([ 0, 12, 10,  6,  5,  5,  2, 12,  9,  1,  3,  8,  7])"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","input_tokens=torch.tensor([input_ids[s] for s in inputs.replace(',','').split()])\n","input_tokens"]},{"cell_type":"markdown","id":"f9aad1cf","metadata":{"papermill":{"duration":0.008685,"end_time":"2024-01-26T01:03:12.948938","exception":false,"start_time":"2024-01-26T01:03:12.940253","status":"completed"},"tags":[]},"source":["Now, using the integer-vector reoresentation of the input sentence, we can use an embedding layer to **encode the inputs** into a real vector embedding. Here, we will use a 16-dimensional embedding such that each input word is represented by a 16-dimensional vector. Since the sentence consists of 13 words, this will result in a 13x16-dimentional embedding."]},{"cell_type":"code","execution_count":3,"id":"687e3a93","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:12.969375Z","iopub.status.busy":"2024-01-26T01:03:12.968732Z","iopub.status.idle":"2024-01-26T01:03:13.066958Z","shell.execute_reply":"2024-01-26T01:03:13.065428Z"},"papermill":{"duration":0.112225,"end_time":"2024-01-26T01:03:13.070034","exception":false,"start_time":"2024-01-26T01:03:12.957809","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[ 3.3737e-01, -1.7778e-01, -3.0353e-01, -5.8801e-01,  3.4861e-01,\n","          6.6034e-01, -2.1964e-01, -3.7917e-01,  7.6711e-01, -1.1925e+00,\n","          6.9835e-01, -1.4097e+00,  1.7938e-01,  1.8951e+00,  4.9545e-01,\n","          2.6920e-01],\n","        [-9.7969e-01, -2.1126e+00, -2.7214e-01, -3.5100e-01,  1.1152e+00,\n","         -6.1722e-01, -2.2708e+00, -1.3819e+00,  1.1721e+00, -4.3716e-01,\n","         -4.0527e-01,  7.0864e-01,  9.5331e-01, -1.3035e-02, -1.3009e-01,\n","         -8.7660e-02],\n","        [ 6.8508e-01,  2.0024e+00, -5.4688e-01,  1.6014e+00, -2.2577e+00,\n","         -1.8009e+00,  7.0147e-01,  5.7028e-01, -1.1766e+00, -2.0524e+00,\n","          1.1318e-01,  1.4353e+00,  8.8307e-02, -1.2037e+00,  1.0964e+00,\n","          2.4210e+00],\n","        [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1861e-02,\n","         -4.7896e-01,  7.6681e-01,  2.7468e-02,  1.9929e+00,  1.3708e+00,\n","         -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3745e-03, -9.8955e-01,\n","          7.0161e-01],\n","        [ 2.5529e-01, -5.4963e-01,  1.0042e+00,  8.2723e-01, -3.9481e-01,\n","          4.8923e-01, -2.1681e-01, -1.7472e+00, -1.6025e+00, -1.0764e+00,\n","          9.0315e-01, -7.2184e-01, -5.9508e-01, -7.1122e-01,  6.2296e-01,\n","         -1.3729e+00],\n","        [ 2.5529e-01, -5.4963e-01,  1.0042e+00,  8.2723e-01, -3.9481e-01,\n","          4.8923e-01, -2.1681e-01, -1.7472e+00, -1.6025e+00, -1.0764e+00,\n","          9.0315e-01, -7.2184e-01, -5.9508e-01, -7.1122e-01,  6.2296e-01,\n","         -1.3729e+00],\n","        [-1.3250e+00,  1.7843e-01, -2.1338e+00,  1.0524e+00, -3.8848e-01,\n","         -9.3435e-01, -4.9914e-01, -1.0867e+00,  8.8054e-01,  1.5542e+00,\n","          6.2662e-01, -1.7549e-01,  9.8284e-02, -9.3507e-02,  2.6621e-01,\n","         -5.8504e-01],\n","        [-9.7969e-01, -2.1126e+00, -2.7214e-01, -3.5100e-01,  1.1152e+00,\n","         -6.1722e-01, -2.2708e+00, -1.3819e+00,  1.1721e+00, -4.3716e-01,\n","         -4.0527e-01,  7.0864e-01,  9.5331e-01, -1.3035e-02, -1.3009e-01,\n","         -8.7660e-02],\n","        [-1.2743e+00,  4.5128e-01, -2.2801e-01,  9.2238e-01,  2.0561e-01,\n","         -4.9696e-01,  5.8206e-01,  2.0532e-01, -3.0177e-01, -6.7030e-01,\n","         -6.1710e-01, -8.3339e-01,  4.8387e-01, -1.3493e-01,  2.1187e-01,\n","         -8.7140e-01],\n","        [-7.7020e-02, -1.0205e+00, -1.6896e-01,  9.1776e-01,  1.5810e+00,\n","          1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n","          9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n","         -2.8400e+00],\n","        [ 8.7684e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n","          1.3139e+00,  1.0533e+00,  1.3881e-01,  2.2473e+00, -8.0364e-01,\n","         -2.8084e-01,  7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n","          2.2935e-01],\n","        [-2.5822e-01, -2.0407e+00, -8.0156e-01, -8.1830e-01, -1.1820e+00,\n","         -2.8774e-01, -6.0430e-01,  6.0024e-01, -1.4053e+00, -5.9217e-01,\n","         -2.5479e-01,  1.1517e+00, -1.7858e-02,  4.2640e-01, -7.6574e-01,\n","         -5.4514e-02],\n","        [-9.4053e-01, -4.6806e-01,  1.0322e+00, -2.8300e-01,  4.9275e-01,\n","         -1.4078e-02, -2.7466e-01, -7.6409e-01,  1.3966e+00, -9.9491e-01,\n","         -1.5822e-03,  1.2471e+00, -7.7105e-02,  1.2774e+00, -1.4596e+00,\n","         -2.1595e+00]])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# using the same seed to keep the same result\n","torch.manual_seed(123)\n","embed=torch.nn.Embedding(13,16)\n","embedded_sentence=embed(input_tokens).detach()\n","embedded_sentence"]},{"cell_type":"code","execution_count":4,"id":"be8ef730","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.092404Z","iopub.status.busy":"2024-01-26T01:03:13.091549Z","iopub.status.idle":"2024-01-26T01:03:13.100169Z","shell.execute_reply":"2024-01-26T01:03:13.098932Z"},"papermill":{"duration":0.022783,"end_time":"2024-01-26T01:03:13.102711","exception":false,"start_time":"2024-01-26T01:03:13.079928","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([13, 16])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["embedded_sentence.shape"]},{"cell_type":"markdown","id":"25460a64","metadata":{"papermill":{"duration":0.010746,"end_time":"2024-01-26T01:03:13.124663","exception":false,"start_time":"2024-01-26T01:03:13.113917","status":"completed"},"tags":[]},"source":["# Defining the Weight Matrices\n","\n","Self-attention mechanism(scaled dot-product) utilizes three weight matrices, referred to as $W_{q}$, $W_{k}$ and $W_{v}$, which are adjusted as model parameters during training. These matrices serve to project the inputs into query, key, and value components of the sequence, respectively.\n","\n","The respective query, key and value sequences are obtained via matrix multiplication between the weight matrices $W$ and the **embedded inputs x**:\n","\n","**Query sequence**\n","\n","$$q^{(i)}=W_{q}x^{(i)} for i \\in [1,T]$$\n","\n","**Key sequence**\n","\n","$$k^{(i)}=W_{k}x^{(i)} for i \\in [1,T]$$\n","\n","**Value sequence**\n","\n","$$v^{(i)}=W_{v}x^{(i)} for i \\in [1,T]$$\n","\n","\n","The index **i** refers to the token index position in the input sequence, which has length T.\n","\n","<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/814/464/564/081/956/original/9c952305a2b6d5a8.png\" width=\"20%\" heigh=\"20%\" alt=\"scaled dot-product attention\"></div>\n","\n","Here, both $q^{(i)}$ and $k^{(i)}$ are vectors of dimension $d_{k}$. The projection matrices $W_{q}$ and $W_{k}$ have a shape of $d_{k}*d$, while $W_{v}$ has the shape $d_{v}*d$. It is important to note that $d$ represents the size of each word vector x.\n","\n","According to the [dot-product illustration](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture), the computing between the query and key vectors, these two vectors have to contain the same number of elements($d_{q}$ and $d_{k}$). However, the number of elements in the value vector $v^{(i)}$, which determines the size of the resulting context vector, is arbitrary.\n","\n","Here is an example, we set $d_{q}$ and $d_{k}$=24 and use $d_{v}$=28, initializing the projection matrices as follows:"]},{"cell_type":"code","execution_count":5,"id":"81698e7e","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.147587Z","iopub.status.busy":"2024-01-26T01:03:13.147077Z","iopub.status.idle":"2024-01-26T01:03:13.156495Z","shell.execute_reply":"2024-01-26T01:03:13.155152Z"},"papermill":{"duration":0.024561,"end_time":"2024-01-26T01:03:13.159229","exception":false,"start_time":"2024-01-26T01:03:13.134668","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["16"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(123)\n","\n","d=embedded_sentence.shape[1]\n","d"]},{"cell_type":"code","execution_count":6,"id":"205c0e51","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.182039Z","iopub.status.busy":"2024-01-26T01:03:13.181501Z","iopub.status.idle":"2024-01-26T01:03:13.202969Z","shell.execute_reply":"2024-01-26T01:03:13.201948Z"},"papermill":{"duration":0.036059,"end_time":"2024-01-26T01:03:13.205459","exception":false,"start_time":"2024-01-26T01:03:13.1694","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Parameter containing:\n","tensor([[2.2383e-01, 3.0465e-01, 3.0185e-01, 7.7719e-01, 4.9632e-01, 3.8457e-01,\n","         9.4751e-02, 5.4099e-01, 8.0899e-01, 8.1570e-01, 5.4314e-01, 9.5954e-01,\n","         3.7638e-01, 8.8847e-01, 7.7945e-01, 9.4166e-01],\n","        [7.5758e-01, 4.9898e-02, 7.4476e-01, 1.3877e-01, 1.6512e-01, 1.4907e-01,\n","         2.6847e-01, 5.0905e-02, 9.2707e-01, 2.8936e-01, 8.2721e-01, 9.4828e-01,\n","         8.1707e-01, 8.7183e-01, 5.1264e-01, 8.6063e-03],\n","        [8.0527e-01, 7.8735e-02, 6.2932e-01, 2.9138e-01, 8.2026e-01, 8.3362e-01,\n","         4.7395e-01, 3.2585e-01, 8.8695e-01, 3.4264e-01, 1.1503e-01, 1.7675e-01,\n","         2.1455e-02, 8.6990e-01, 8.7559e-01, 3.7270e-01],\n","        [7.2059e-01, 7.8469e-01, 2.9878e-01, 5.8486e-01, 4.1490e-01, 2.5936e-01,\n","         1.8493e-01, 2.5396e-01, 4.6260e-01, 4.3994e-01, 1.2095e-01, 4.5656e-02,\n","         4.3196e-01, 6.9407e-01, 6.6612e-01, 1.4987e-01],\n","        [7.6967e-01, 1.5432e-01, 2.5701e-01, 9.0780e-01, 6.2522e-01, 6.8266e-01,\n","         1.8458e-02, 6.5164e-02, 4.0684e-01, 9.6096e-01, 6.7868e-03, 7.7611e-01,\n","         2.6442e-01, 1.4285e-01, 8.8732e-01, 1.2021e-01],\n","        [6.8133e-01, 3.7276e-01, 8.5715e-01, 2.5723e-02, 7.6188e-01, 8.0980e-01,\n","         6.1699e-01, 9.3682e-01, 9.1631e-01, 7.7474e-01, 8.4722e-01, 9.2658e-01,\n","         8.8112e-01, 7.4642e-01, 1.5909e-01, 7.7055e-01],\n","        [4.8425e-01, 7.0663e-01, 4.9884e-01, 7.7782e-01, 5.1604e-02, 1.7298e-01,\n","         3.3082e-01, 9.2754e-01, 1.7751e-01, 4.6892e-01, 3.3725e-01, 2.6404e-02,\n","         5.3384e-02, 9.0279e-02, 7.4531e-02, 9.7112e-02],\n","        [4.2062e-01, 9.6154e-01, 2.2990e-01, 3.2410e-01, 2.8857e-01, 6.1099e-01,\n","         8.0211e-01, 2.7098e-01, 2.6750e-01, 6.6369e-01, 3.2836e-02, 7.9643e-01,\n","         3.0695e-01, 7.2510e-01, 8.8686e-01, 6.1153e-01],\n","        [1.2812e-02, 3.5339e-01, 9.2658e-01, 3.8711e-02, 9.1733e-01, 1.1971e-01,\n","         3.9132e-01, 8.4936e-01, 2.8023e-02, 3.8628e-02, 3.9943e-01, 1.2943e-01,\n","         4.5934e-01, 8.2646e-01, 7.5343e-01, 4.4913e-02],\n","        [3.6892e-01, 1.2102e-01, 1.8199e-01, 7.3472e-01, 6.0274e-01, 9.7552e-01,\n","         6.8528e-01, 1.4519e-01, 6.3655e-01, 1.6826e-01, 2.7941e-02, 7.4109e-02,\n","         9.7227e-01, 2.5698e-01, 8.8271e-01, 1.6826e-01],\n","        [9.5139e-01, 2.0317e-01, 6.4285e-01, 8.4507e-04, 2.9253e-01, 3.5884e-01,\n","         4.6106e-01, 2.8258e-01, 3.8335e-01, 4.5231e-01, 8.4666e-01, 2.5868e-02,\n","         1.2855e-01, 6.8069e-01, 4.2331e-01, 3.8708e-01],\n","        [4.7674e-01, 9.5627e-01, 2.0533e-02, 8.6826e-01, 7.2311e-01, 2.0493e-01,\n","         3.9773e-01, 3.5077e-01, 3.3057e-01, 1.9374e-01, 9.0709e-01, 9.7089e-01,\n","         6.1383e-01, 7.6678e-01, 9.0303e-01, 7.4551e-01],\n","        [9.9696e-01, 7.1538e-01, 3.1272e-03, 9.9986e-01, 5.4591e-01, 2.6334e-01,\n","         5.9240e-01, 1.7805e-01, 2.8664e-01, 5.8276e-01, 3.4258e-01, 1.2603e-01,\n","         4.7862e-01, 9.6871e-01, 4.9844e-01, 5.6016e-01],\n","        [4.9144e-01, 1.9335e-01, 5.6547e-01, 1.3947e-01, 7.1792e-01, 9.7798e-01,\n","         9.8303e-01, 3.8717e-01, 5.4474e-02, 5.8483e-01, 8.7221e-01, 5.9142e-01,\n","         2.0964e-02, 9.8120e-01, 3.2919e-02, 6.0614e-01],\n","        [9.7452e-01, 2.3828e-01, 3.8495e-01, 5.9646e-01, 3.6891e-01, 7.5607e-01,\n","         6.2156e-01, 4.7269e-01, 4.8486e-01, 3.0533e-01, 4.2984e-01, 1.4502e-01,\n","         9.9630e-01, 4.4357e-01, 6.6873e-01, 6.8975e-01],\n","        [8.2953e-01, 3.5431e-01, 1.0604e-01, 4.3506e-01, 8.0488e-02, 3.4315e-01,\n","         2.4547e-01, 3.7389e-03, 7.1329e-01, 9.9120e-01, 5.1161e-01, 8.9661e-01,\n","         8.3075e-01, 9.1150e-01, 5.5886e-01, 8.2387e-01],\n","        [1.2988e-01, 2.5011e-02, 6.6547e-01, 2.6829e-01, 9.9158e-01, 5.7032e-01,\n","         3.3891e-01, 9.3563e-01, 9.7898e-01, 8.7122e-01, 8.0769e-01, 3.5685e-01,\n","         7.7640e-01, 6.4864e-01, 3.1118e-01, 1.4588e-01],\n","        [9.9964e-01, 5.9567e-02, 6.1331e-01, 2.5677e-01, 7.4479e-02, 2.6851e-01,\n","         3.0593e-01, 7.8749e-01, 4.7445e-01, 9.6823e-01, 5.9971e-01, 5.0136e-01,\n","         8.1705e-01, 4.3432e-01, 7.6012e-01, 6.5748e-01],\n","        [4.8680e-01, 1.4728e-01, 8.6035e-01, 1.3611e-01, 6.9213e-01, 7.9141e-03,\n","         5.5138e-01, 5.1960e-01, 6.9766e-01, 1.8325e-01, 8.8506e-02, 3.4613e-01,\n","         2.5921e-01, 3.6149e-02, 8.6256e-01, 5.5304e-01],\n","        [7.3236e-01, 7.4141e-01, 2.9814e-01, 3.2150e-01, 1.3247e-01, 9.1282e-01,\n","         8.0841e-01, 3.9760e-01, 3.6337e-01, 9.1087e-01, 3.6575e-01, 3.2266e-01,\n","         6.4184e-01, 3.1287e-01, 5.0166e-01, 8.7902e-01],\n","        [1.2604e-01, 8.0083e-01, 2.1954e-01, 3.8633e-01, 7.4799e-01, 6.0365e-01,\n","         8.1289e-01, 3.8198e-01, 8.9863e-01, 1.5036e-01, 6.4950e-01, 2.5026e-01,\n","         2.9899e-01, 8.3617e-01, 1.2037e-02, 7.5694e-01],\n","        [6.5635e-01, 3.5246e-01, 7.7813e-01, 8.9369e-01, 1.8693e-01, 8.2121e-01,\n","         1.7736e-01, 2.4842e-01, 8.7530e-01, 8.0604e-01, 7.2358e-01, 4.3165e-02,\n","         7.5033e-01, 6.8682e-01, 6.4769e-01, 7.5664e-01],\n","        [7.4272e-01, 9.1680e-01, 5.7238e-01, 4.4882e-01, 5.7173e-01, 4.0714e-01,\n","         8.9680e-01, 3.7979e-01, 1.9743e-01, 5.1343e-01, 3.0389e-01, 9.2211e-01,\n","         9.6849e-01, 3.1945e-02, 7.6185e-01, 1.8171e-01],\n","        [6.7478e-01, 7.9091e-02, 9.4587e-01, 8.1485e-02, 1.2109e-01, 8.0935e-01,\n","         5.1930e-01, 4.0196e-01, 8.4459e-01, 2.8276e-01, 2.6690e-01, 3.8023e-01,\n","         8.4015e-01, 8.6623e-02, 3.0478e-01, 2.3214e-01],\n","        [9.2976e-01, 1.3616e-01, 7.5748e-01, 8.2101e-01, 2.2708e-01, 3.5492e-01,\n","         8.1588e-01, 2.5691e-02, 6.7321e-01, 9.1517e-01, 1.8161e-01, 5.3942e-01,\n","         6.8074e-01, 3.1517e-01, 9.3807e-01, 3.8948e-01],\n","        [6.4266e-01, 7.2315e-01, 2.3211e-01, 3.4562e-01, 4.7240e-01, 9.3948e-01,\n","         1.2437e-01, 7.5231e-01, 9.1875e-01, 6.0086e-01, 1.9832e-01, 8.0096e-01,\n","         7.7884e-02, 6.4911e-01, 3.5634e-01, 4.1765e-01],\n","        [1.1352e-01, 5.9395e-01, 1.9092e-01, 9.6893e-01, 5.8215e-01, 8.0316e-02,\n","         2.3958e-01, 8.3629e-01, 8.2855e-01, 2.8586e-01, 6.4974e-01, 7.2764e-01,\n","         5.0989e-02, 7.3622e-01, 1.7100e-01, 3.6668e-01],\n","        [5.4811e-01, 8.1902e-01, 1.4077e-01, 8.3626e-01, 7.4442e-01, 9.7725e-01,\n","         4.3640e-01, 4.7173e-01, 8.2328e-01, 3.9218e-02, 2.3207e-01, 3.2994e-01,\n","         3.8522e-02, 2.7392e-01, 7.4236e-02, 3.4202e-02]], requires_grad=True)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["d_q, d_k, d_v=24,24,28\n","\n","W_query=torch.nn.Parameter(torch.rand(d_q, d))\n","W_key=torch.nn.Parameter(torch.rand(d_k, d))\n","W_value=torch.nn.Parameter(torch.rand(d_v, d))\n","W_value"]},{"cell_type":"markdown","id":"f86c819b","metadata":{"papermill":{"duration":0.009364,"end_time":"2024-01-26T01:03:13.224593","exception":false,"start_time":"2024-01-26T01:03:13.215229","status":"completed"},"tags":[]},"source":["# Computing the Unnormalized Attention Weights\n","\n","Now, let's suppose we are interested in computing the attention-vector for the second input element - the second input element acts as the query here:"]},{"cell_type":"code","execution_count":7,"id":"2420c395","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.247618Z","iopub.status.busy":"2024-01-26T01:03:13.24712Z","iopub.status.idle":"2024-01-26T01:03:13.263724Z","shell.execute_reply":"2024-01-26T01:03:13.262258Z"},"papermill":{"duration":0.032312,"end_time":"2024-01-26T01:03:13.266767","exception":false,"start_time":"2024-01-26T01:03:13.234455","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([-0.6497,  0.3101, -1.5242, -2.4824, -0.5965, -2.2526, -4.5416, -4.1824,\n","        -1.8672, -1.1036, -2.9178, -2.4902, -3.6235, -3.8396, -2.7322, -0.9615,\n","        -0.3936, -2.3660, -1.2402, -4.7051, -2.8151, -1.9909, -3.8078, -1.4460,\n","        -2.3606, -2.4327, -1.7750, -2.9069], grad_fn=<MvBackward0>)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["x_2=embedded_sentence[1]\n","query_2=W_query.matmul(x_2)\n","key_2=W_key.matmul(x_2)\n","value_2=W_value.matmul(x_2)\n","value_2"]},{"cell_type":"code","execution_count":8,"id":"1d4fbb54","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.290883Z","iopub.status.busy":"2024-01-26T01:03:13.289393Z","iopub.status.idle":"2024-01-26T01:03:13.296398Z","shell.execute_reply":"2024-01-26T01:03:13.295532Z"},"papermill":{"duration":0.022089,"end_time":"2024-01-26T01:03:13.299117","exception":false,"start_time":"2024-01-26T01:03:13.277028","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([28])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["value_2.shape"]},{"cell_type":"markdown","id":"c08400a2","metadata":{"papermill":{"duration":0.009867,"end_time":"2024-01-26T01:03:13.320014","exception":false,"start_time":"2024-01-26T01:03:13.310147","status":"completed"},"tags":[]},"source":["We can then generalize this to compute the remaining key, and value elements for all inputs as well, since we will need them in the next step when we compute the unnormalized attention weights $w$:"]},{"cell_type":"code","execution_count":9,"id":"9d848515","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.345118Z","iopub.status.busy":"2024-01-26T01:03:13.343359Z","iopub.status.idle":"2024-01-26T01:03:13.358103Z","shell.execute_reply":"2024-01-26T01:03:13.356649Z"},"papermill":{"duration":0.030355,"end_time":"2024-01-26T01:03:13.361041","exception":false,"start_time":"2024-01-26T01:03:13.330686","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([13, 24])\n","torch.Size([13, 28])\n"]}],"source":["keys=W_key.matmul(embedded_sentence.T).T\n","values=W_value.matmul(embedded_sentence.T).T\n","print(keys.shape)\n","print(values.shape)"]},{"cell_type":"markdown","id":"4e6d5e89","metadata":{"papermill":{"duration":0.011444,"end_time":"2024-01-26T01:03:13.384152","exception":false,"start_time":"2024-01-26T01:03:13.372708","status":"completed"},"tags":[]},"source":["Let's compute the unnormalized attention weights $w$, which are illustrated in the figure below:\n","\n","<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/814/578/611/132/625/original/a961fee9d7ca0131.png\" width=\"40%\" heigh=\"40%\" alt=\"computing the unnormalized attention weighs w\"></div>\n","\n","\n","As illustrated in the figure above, we compute $w_{ij}$ as the dot product between the query and key sequences $w_{ij}={q^{(i)}}^{T}k^{(j)}$.\n","\n","For example, we can compute the unnormalized attention weight for the query and 5th input element(corresponding to index position 4) as follows:"]},{"cell_type":"code","execution_count":10,"id":"f1cea8fd","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.410143Z","iopub.status.busy":"2024-01-26T01:03:13.409399Z","iopub.status.idle":"2024-01-26T01:03:13.418189Z","shell.execute_reply":"2024-01-26T01:03:13.416684Z"},"papermill":{"duration":0.02504,"end_time":"2024-01-26T01:03:13.420953","exception":false,"start_time":"2024-01-26T01:03:13.395913","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor(155.9239, grad_fn=<DotBackward0>)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["w_2_4=query_2.dot(keys[4])\n","w_2_4"]},{"cell_type":"code","execution_count":11,"id":"2ebfdeb8","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.448424Z","iopub.status.busy":"2024-01-26T01:03:13.447634Z","iopub.status.idle":"2024-01-26T01:03:13.461375Z","shell.execute_reply":"2024-01-26T01:03:13.460068Z"},"papermill":{"duration":0.030196,"end_time":"2024-01-26T01:03:13.463916","exception":false,"start_time":"2024-01-26T01:03:13.43372","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([ -24.6096,  151.2782,  -44.1470,  110.7908,  155.9239,  155.9239,\n","          70.0803,  151.2782,   71.0386,   69.2800, -144.1026,  185.6768,\n","          41.0362], grad_fn=<SqueezeBackward4>)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Here we compute the w values for all input tokens as illustrated in the previous figure\n","w_2=query_2.matmul(keys.T)\n","w_2"]},{"cell_type":"markdown","id":"c5c1c503","metadata":{"papermill":{"duration":0.010653,"end_time":"2024-01-26T01:03:13.485966","exception":false,"start_time":"2024-01-26T01:03:13.475313","status":"completed"},"tags":[]},"source":["# Computing the Attention Scores\n","\n","The subsequent step in self-attention is to normalize the unnormalized attention weights $w$, to obtain the normalized attention weights, $\\alpha$, by applying the softmax function. Additionally, $\\sqrt[1]{d_{k}}$ is used to scale $w$ before normalizing it through the softmax function, as shown below:\n","\n","<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/814/687/652/091/481/original/41644ce020deec4c.png\" width=\"80%\" heigh=\"80%\" alt=\"computing attention scores\"></div>\n","\n","The scaling by $d_{k}$ ensures that rhe Euclidean length of the weight vectors will be approximately in the same magnitude. This helps prevent the attention weights from becoming too smaller ot too large, which could lead to numerical instability or affect the model's ability to converge during training.\n","\n","The implement the computation of the attention weights as follows:"]},{"cell_type":"code","execution_count":12,"id":"962024dc","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.511053Z","iopub.status.busy":"2024-01-26T01:03:13.509112Z","iopub.status.idle":"2024-01-26T01:03:13.529029Z","shell.execute_reply":"2024-01-26T01:03:13.527307Z"},"papermill":{"duration":0.034988,"end_time":"2024-01-26T01:03:13.531711","exception":false,"start_time":"2024-01-26T01:03:13.496723","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([2.2665e-19, 8.8675e-04, 4.2010e-21, 2.2835e-07, 2.2890e-03, 2.2890e-03,\n","        5.6183e-11, 8.8675e-04, 6.8322e-11, 4.7716e-11, 5.7849e-30, 9.9365e-01,\n","        1.4957e-13], grad_fn=<SoftmaxBackward0>)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn.functional as F\n","\n","attention_weights_2=F.softmax(w_2/d_k**0.5, dim=0)\n","attention_weights_2"]},{"cell_type":"markdown","id":"bdcac100","metadata":{"papermill":{"duration":0.010924,"end_time":"2024-01-26T01:03:13.55388","exception":false,"start_time":"2024-01-26T01:03:13.542956","status":"completed"},"tags":[]},"source":["Finally, the last step is to compute the context vector $z^{(2)}$, which is an attention-weighted version of our original query input $x^{(2)}$, including all the other input elements as its context via the attention weights:\n","\n","<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/814/731/512/658/906/original/651e3e40601e50fa.png\" width=\"80%\" heigh=\"80%\" alt=\"computing context vector\"></div>"]},{"cell_type":"code","execution_count":13,"id":"4687ebd9","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.581641Z","iopub.status.busy":"2024-01-26T01:03:13.579728Z","iopub.status.idle":"2024-01-26T01:03:13.590859Z","shell.execute_reply":"2024-01-26T01:03:13.589381Z"},"papermill":{"duration":0.02735,"end_time":"2024-01-26T01:03:13.593325","exception":false,"start_time":"2024-01-26T01:03:13.565975","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([-2.9182, -2.0006, -3.9933, -4.1344, -3.2336, -3.3511, -2.9606, -3.6264,\n","        -2.5876, -3.9000, -2.7759, -3.8449, -4.1974, -2.1862, -3.4551, -2.5073,\n","        -3.4832, -2.2261, -3.4518, -3.9524, -4.4011, -4.7407, -4.1783, -2.8100,\n","        -4.1595, -3.3601, -3.0404, -4.5382], grad_fn=<SqueezeBackward4>)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["context_vector_2=attention_weights_2.matmul(values)\n","context_vector_2"]},{"cell_type":"markdown","id":"fc2ff133","metadata":{"papermill":{"duration":0.012027,"end_time":"2024-01-26T01:03:13.616906","exception":false,"start_time":"2024-01-26T01:03:13.604879","status":"completed"},"tags":[]},"source":["Note that this output vector has more dimensions $d_{v}=28$ than the original input vector $d=16$ since we specified $d_{v}>d$ earlier; however, the embedding size choice is arbitrary."]},{"cell_type":"code","execution_count":14,"id":"c51e28d3","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.644453Z","iopub.status.busy":"2024-01-26T01:03:13.643024Z","iopub.status.idle":"2024-01-26T01:03:13.650885Z","shell.execute_reply":"2024-01-26T01:03:13.649938Z"},"papermill":{"duration":0.024567,"end_time":"2024-01-26T01:03:13.6532","exception":false,"start_time":"2024-01-26T01:03:13.628633","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([28])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["context_vector_2.shape"]},{"cell_type":"markdown","id":"821f64ad","metadata":{"papermill":{"duration":0.011461,"end_time":"2024-01-26T01:03:13.676221","exception":false,"start_time":"2024-01-26T01:03:13.66476","status":"completed"},"tags":[]},"source":["Let's summarize the code in a compact `SelfAttention` class:"]},{"cell_type":"code","execution_count":15,"id":"6fd0164d","metadata":{"execution":{"iopub.execute_input":"2024-01-26T01:03:13.702118Z","iopub.status.busy":"2024-01-26T01:03:13.70125Z","iopub.status.idle":"2024-01-26T01:03:13.724278Z","shell.execute_reply":"2024-01-26T01:03:13.723249Z"},"papermill":{"duration":0.038702,"end_time":"2024-01-26T01:03:13.726738","exception":false,"start_time":"2024-01-26T01:03:13.688036","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[ 2.6896e+00,  2.4364e+00,  3.0323e+00,  1.8150e+00,  2.9260e+00,\n","          2.2877e+00,  1.1890e+00,  3.6908e+00,  2.9604e+00,  3.7647e+00,\n","          1.9424e+00,  1.1123e+00,  2.9210e+00,  1.3260e+00,  1.0155e+00,\n","          2.5780e+00,  2.5362e+00,  9.2516e-01,  3.5756e+00,  2.7663e+00,\n","          8.8226e-01,  3.2968e+00,  4.2811e-01, -2.1147e-01,  3.1204e+00,\n","          3.4462e+00, -1.2604e-01,  3.9690e+00],\n","        [-5.1748e+00, -5.0843e+00, -2.3271e+00, -2.6637e+00, -3.8654e+00,\n","         -2.6058e+00, -4.9214e+00, -3.2063e+00, -4.7985e+00, -4.4338e+00,\n","         -1.8168e+00, -3.3451e+00, -5.5119e+00, -3.7816e+00, -3.0789e+00,\n","         -3.4481e+00, -2.0107e+00, -3.4310e+00, -3.0708e+00, -3.5153e+00,\n","         -3.0055e+00, -3.2615e+00, -2.5158e+00, -2.4361e+00, -3.2705e+00,\n","         -2.0824e+00, -2.6549e+00, -3.0192e+00],\n","        [ 2.8131e+00,  2.5348e+00,  3.1355e+00,  1.8043e+00,  2.8770e+00,\n","          2.3039e+00,  1.2131e+00,  3.7537e+00,  3.0388e+00,  3.8491e+00,\n","          2.0245e+00,  1.1221e+00,  3.0828e+00,  1.2975e+00,  1.1422e+00,\n","          2.5792e+00,  2.6647e+00,  8.2513e-01,  3.6621e+00,  2.7107e+00,\n","          9.8882e-01,  3.3120e+00,  4.6935e-01, -2.3539e-01,  3.2273e+00,\n","          3.5356e+00,  3.9165e-05,  4.0955e+00],\n","        [-5.2216e+00, -5.1250e+00, -2.3081e+00, -2.7119e+00, -3.8849e+00,\n","         -2.6404e+00, -4.9727e+00, -3.2265e+00, -4.8205e+00, -4.4176e+00,\n","         -1.8175e+00, -3.3379e+00, -5.5438e+00, -3.8342e+00, -3.1169e+00,\n","         -3.4986e+00, -2.0119e+00, -3.4456e+00, -3.0424e+00, -3.5197e+00,\n","         -3.0190e+00, -3.2536e+00, -2.4972e+00, -2.4298e+00, -3.2682e+00,\n","         -2.0752e+00, -2.6908e+00, -3.0230e+00],\n","        [-4.1704e+00, -4.2758e+00, -2.7313e+00, -1.6025e+00, -3.4900e+00,\n","         -1.8507e+00, -3.8454e+00, -2.8090e+00, -4.3764e+00, -4.8012e+00,\n","         -1.8026e+00, -3.5267e+00, -4.8426e+00, -2.6456e+00, -2.2412e+00,\n","         -2.3581e+00, -1.9480e+00, -3.0920e+00, -3.6811e+00, -3.4724e+00,\n","         -2.6929e+00, -3.4668e+00, -2.9276e+00, -2.5705e+00, -3.3794e+00,\n","         -2.2084e+00, -1.9057e+00, -2.9259e+00],\n","        [-4.1704e+00, -4.2758e+00, -2.7313e+00, -1.6025e+00, -3.4900e+00,\n","         -1.8507e+00, -3.8454e+00, -2.8090e+00, -4.3764e+00, -4.8012e+00,\n","         -1.8026e+00, -3.5267e+00, -4.8426e+00, -2.6456e+00, -2.2412e+00,\n","         -2.3581e+00, -1.9480e+00, -3.0920e+00, -3.6811e+00, -3.4724e+00,\n","         -2.6929e+00, -3.4668e+00, -2.9276e+00, -2.5705e+00, -3.3794e+00,\n","         -2.2084e+00, -1.9057e+00, -2.9259e+00],\n","        [-5.1744e+00, -5.0832e+00, -2.3272e+00, -2.6635e+00, -3.8647e+00,\n","         -2.6056e+00, -4.9207e+00, -3.2056e+00, -4.7977e+00, -4.4337e+00,\n","         -1.8168e+00, -3.3448e+00, -5.5114e+00, -3.7812e+00, -3.0788e+00,\n","         -3.4478e+00, -2.0111e+00, -3.4311e+00, -3.0711e+00, -3.5147e+00,\n","         -3.0056e+00, -3.2611e+00, -2.5158e+00, -2.4362e+00, -3.2698e+00,\n","         -2.0827e+00, -2.6544e+00, -3.0193e+00],\n","        [-5.1748e+00, -5.0843e+00, -2.3271e+00, -2.6637e+00, -3.8654e+00,\n","         -2.6058e+00, -4.9214e+00, -3.2063e+00, -4.7985e+00, -4.4338e+00,\n","         -1.8168e+00, -3.3451e+00, -5.5119e+00, -3.7816e+00, -3.0789e+00,\n","         -3.4481e+00, -2.0107e+00, -3.4310e+00, -3.0708e+00, -3.5153e+00,\n","         -3.0055e+00, -3.2615e+00, -2.5158e+00, -2.4361e+00, -3.2705e+00,\n","         -2.0824e+00, -2.6549e+00, -3.0192e+00],\n","        [-4.9301e+00, -4.8104e+00, -2.4283e+00, -2.4358e+00, -3.7216e+00,\n","         -2.4360e+00, -4.6296e+00, -3.0651e+00, -4.6345e+00, -4.5006e+00,\n","         -1.8140e+00, -3.3552e+00, -5.3292e+00, -3.5139e+00, -2.9022e+00,\n","         -3.1914e+00, -2.0392e+00, -3.3779e+00, -3.2198e+00, -3.4430e+00,\n","         -2.9568e+00, -3.2675e+00, -2.6005e+00, -2.4697e+00, -3.2266e+00,\n","         -2.1470e+00, -2.4477e+00, -3.0113e+00],\n","        [-3.6358e+00, -3.8059e+00, -2.9438e+00, -1.0581e+00, -3.2624e+00,\n","         -1.4600e+00, -3.2579e+00, -2.5729e+00, -4.1203e+00, -4.9798e+00,\n","         -1.7970e+00, -3.6022e+00, -4.4744e+00, -2.0516e+00, -1.8116e+00,\n","         -1.7870e+00, -1.9389e+00, -2.9292e+00, -4.0016e+00, -3.4138e+00,\n","         -2.5408e+00, -3.5485e+00, -3.1367e+00, -2.6401e+00, -3.3983e+00,\n","         -2.2913e+00, -1.4994e+00, -2.8831e+00],\n","        [ 2.8138e+00,  2.5365e+00,  3.1370e+00,  1.8051e+00,  2.8774e+00,\n","          2.3052e+00,  1.2130e+00,  3.7553e+00,  3.0408e+00,  3.8503e+00,\n","          2.0250e+00,  1.1233e+00,  3.0837e+00,  1.2983e+00,  1.1428e+00,\n","          2.5798e+00,  2.6653e+00,  8.2577e-01,  3.6640e+00,  2.7119e+00,\n","          9.8928e-01,  3.3135e+00,  4.6938e-01, -2.3619e-01,  3.2285e+00,\n","          3.5372e+00, -7.6822e-04,  4.0978e+00],\n","        [-5.1185e+00, -5.0418e+00, -2.3496e+00, -2.6030e+00, -3.8462e+00,\n","         -2.5629e+00, -4.8621e+00, -3.1856e+00, -4.7770e+00, -4.4553e+00,\n","         -1.8160e+00, -3.3565e+00, -5.4750e+00, -3.7175e+00, -3.0309e+00,\n","         -3.3866e+00, -2.0056e+00, -3.4109e+00, -3.1051e+00, -3.5152e+00,\n","         -2.9869e+00, -3.2746e+00, -2.5395e+00, -2.4436e+00, -3.2792e+00,\n","         -2.0882e+00, -2.6138e+00, -3.0134e+00],\n","        [-4.7542e+00, -4.5572e+00, -2.5041e+00, -2.2933e+00, -3.5802e+00,\n","         -2.3233e+00, -4.3982e+00, -2.9318e+00, -4.4719e+00, -4.5334e+00,\n","         -1.8118e+00, -3.3385e+00, -5.1842e+00, -3.3264e+00, -2.7946e+00,\n","         -3.0120e+00, -2.0912e+00, -3.3607e+00, -3.3281e+00, -3.3468e+00,\n","         -2.9420e+00, -3.2408e+00, -2.6503e+00, -2.4952e+00, -3.1442e+00,\n","         -2.2188e+00, -2.2792e+00, -3.0165e+00]], grad_fn=<MmBackward0>)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, d_in, d_out_kq, d_out_v):\n","        super().__init__()\n","        self.d_out_kq=d_out_kq\n","        self.W_query=nn.Parameter(torch.rand(d_in, d_out_kq))\n","        self.W_key=nn.Parameter(torch.rand(d_in, d_out_kq))\n","        self.W_value=nn.Parameter(torch.rand(d_in, d_out_v))\n","        \n","    def forward(self, x):\n","        keys=x.matmul(self.W_key)\n","        queries=x.matmul(self.W_query)\n","        values=x.matmul(self.W_value)\n","        \n","        # unnormalized attention weights\n","        attn_scores=queries.matmul(keys.T)\n","        \n","        attn_weights=torch.softmax(\n","            attn_scores/self.d_out_kq**0.5, dim=-1\n","        )\n","        \n","        context_vex=attn_weights.matmul(values)\n","        return context_vex\n","    \n","    \n","torch.manual_seed(123)\n","d_in,d_out_kq,d_out_v=16,24,28\n","\n","sa=SelfAttention(d_in, d_out_kq, d_out_v)\n","sa(embedded_sentence)"]},{"cell_type":"markdown","id":"845cf880","metadata":{"papermill":{"duration":0.01097,"end_time":"2024-01-26T01:03:13.748952","exception":false,"start_time":"2024-01-26T01:03:13.737982","status":"completed"},"tags":[]},"source":["Next, about the multi-head attetnion. See the notebook [Coding the multi-head attention](https://www.kaggle.com/code/aisuko/coding-the-multi-head-attention/notebook)"]},{"cell_type":"markdown","id":"913cc87f","metadata":{"papermill":{"duration":0.011163,"end_time":"2024-01-26T01:03:13.772883","exception":false,"start_time":"2024-01-26T01:03:13.76172","status":"completed"},"tags":[]},"source":["# Credit\n","\n","* https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n","* https://magazine.sebastianraschka.com?utm_source=navbar&utm_medium=web&r=fbe14"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":11.154214,"end_time":"2024-01-26T01:03:14.908637","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-26T01:03:03.754423","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}