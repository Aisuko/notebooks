{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/causal-self-attention?scriptVersionId=163995871\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we are adapting the previously discussed self-attention mechanism into a causal self-attention mechanism, specifically for GPT-like(decoder-style) LLMs that are used to generate text. The previously notebooks are below:\n* [Encoder in Transformers Architecture](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture)\n* [Decoder in Transformers architecture](https://www.kaggle.com/code/aisuko/decoder-in-transformers-architecture)\n* [Coding the Self-Attention Mechanism](https://www.kaggle.com/code/aisuko/coding-the-self-attention-mechanism)\n* [Coding the Multi-Head Attention](https://www.kaggle.com/code/aisuko/coding-the-multi-head-attention)\n* [Coding Cross-Attention](https://www.kaggle.com/code/aisuko/coding-cross-attention)\n\n\nAnd the causal self-attention mechanism is also often referred to as **masked self-attention**. In the original transformer architecture, it corresponds to the \"masked multi-head attention\" module - for simplicity, we will look at a single attention head in this section, but the same concept generalizes to multiple heads.\n\n<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/570/310/327/483/original/9c619019f8f9a286.webp\" width=\"80%\" heigh=\"80%\" alt=\"Causal self-attetion/masked self-attention\"></div>\n\n**It ensures that the outputs for a certain position in a sequence is based only on the known outputs at previous positions and not on future positions**. In simper terms, it ensures that the prediction for each next word should only depend on the preceding words. To achieve this in GPT-like LLMs, for each token processed, we mask out the feature tokens, which come after the current token in the input text.\n\nThe application of a causal mask to the attention weights for hiding future input tokens in the inputs is illustrated in the figure below. Our input is \"Life is short, eat desert first.\"\n\n<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/654/573/649/346/original/ee97aa86264ac573.webp\" width=\"80%\" heigh=\"80%\" alt=\"causal mask to the attention weights for hiding future input tokens\"></div>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Self-Attention Mechanism(Scaled dot-product attention)\n\nTo illustrate and implement causal self-attention, let's work with the unweighted attention scores and attention weights.\n\n\n## Tokenization","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ntorch.manual_seed(123)\n\n#Tokenization\nsentence='Lieft is short, eat dessert first'\nsentence_ids={s:i for i,s in enumerate(sorted(sentence.replace(',','').split()))}\nsentence_tokens=torch.tensor([sentence_ids[s] for s in sentence.replace(',','').split()])\nprint(sentence_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:53.133339Z","iopub.execute_input":"2024-02-23T11:33:53.133803Z","iopub.status.idle":"2024-02-23T11:33:54.8614Z","shell.execute_reply.started":"2024-02-23T11:33:53.133764Z","shell.execute_reply":"2024-02-23T11:33:54.860105Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"tensor([0, 4, 5, 2, 1, 3])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating Embedding\n\nWe get the examples with input ids `[0,4,5,2,1,3]`. Let's create the token embeddings with `vocab_size=`$6$ and the embeddings size is $3$. This would result in a $6*3$ weight matrix.","metadata":{}},{"cell_type":"code","source":"# Embedding\nvocab_size=len(sentence_ids)\n\nembed=torch.nn.Embedding(vocab_size, 3)\nprint(embed.weight)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:54.863719Z","iopub.execute_input":"2024-02-23T11:33:54.864291Z","iopub.status.idle":"2024-02-23T11:33:54.938291Z","shell.execute_reply.started":"2024-02-23T11:33:54.864253Z","shell.execute_reply":"2024-02-23T11:33:54.937126Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Parameter containing:\ntensor([[ 0.3374, -0.1778, -0.1690],\n        [ 0.9178,  1.5810,  1.3010],\n        [ 1.2753, -0.2010, -0.1606],\n        [-0.4015,  0.9666, -1.1481],\n        [-1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Embedding the Inputs","metadata":{}},{"cell_type":"code","source":"embedded_sentence=embed(sentence_tokens).detach()\nprint(embedded_sentence)\nprint(embedded_sentence.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:54.939843Z","iopub.execute_input":"2024-02-23T11:33:54.940202Z","iopub.status.idle":"2024-02-23T11:33:54.957057Z","shell.execute_reply.started":"2024-02-23T11:33:54.940169Z","shell.execute_reply":"2024-02-23T11:33:54.956185Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensor([[ 0.3374, -0.1778, -0.1690],\n        [-1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096],\n        [ 1.2753, -0.2010, -0.1606],\n        [ 0.9178,  1.5810,  1.3010],\n        [-0.4015,  0.9666, -1.1481]])\ntorch.Size([6, 3])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Computing Q,K and V Vectors\n\n**Note: the image below illustrate with one input which is show as $x^{(1)}$. However, here we do the calculation for all the inputs tokens, not only for one input token**\n\nHere, we initialize the three weight matrices $[3,2 or 4]$. And the input is $[6,3]$. We projected the input tokens from a 3D onto a 2D/4D embedding space which is means we project the embedded input into query, key and value vectors via matrix multiplication:\n\n* Query vector: $q=W_{q}.x$\n* Key vector: $k=W_{k}.x$\n* Value vector: $v=W_{v}.x$\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/979/478/418/490/248/original/d160c70d1d85b897.png)\n\nThe embedding dimensions of the input $x$ and the query vector `q` can be the same or different, depending on the model's design and specific implementation.","metadata":{}},{"cell_type":"code","source":"d_in, d_out_kq, d_out_v=3,2,4\n\nW_query=nn.Parameter(torch.rand(d_in, d_out_kq))\nW_key=nn.Parameter(torch.rand(d_in, d_out_kq))\nW_value=nn.Parameter(torch.rand(d_in, d_out_v))\n\nx=embedded_sentence\nkeys=x.matmul(W_key)\nqueries=x.matmul(W_query)\nvalues=x.matmul(W_value)\n\nprint(keys)\nprint(queries)\nprint(values)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:54.959384Z","iopub.execute_input":"2024-02-23T11:33:54.960274Z","iopub.status.idle":"2024-02-23T11:33:54.98378Z","shell.execute_reply.started":"2024-02-23T11:33:54.960232Z","shell.execute_reply":"2024-02-23T11:33:54.98264Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tensor([[-0.0214, -0.1821],\n        [-0.6142, -0.2775],\n        [-2.1608, -2.1497],\n        [ 0.3533,  0.0171],\n        [ 1.6910,  2.4233],\n        [-0.2527,  0.2558]], grad_fn=<MmBackward0>)\ntensor([[ 0.2702, -0.0070],\n        [-1.1294, -0.7235],\n        [-2.8694, -2.2637],\n        [ 1.1285,  0.3962],\n        [ 1.1547,  1.6555],\n        [-0.4628, -0.4417]], grad_fn=<MmBackward0>)\ntensor([[-5.1625e-02,  8.1035e-02,  1.7690e-01, -7.9819e-03],\n        [-5.4351e-01, -4.9471e-01, -1.0151e+00, -1.2934e+00],\n        [-2.3440e+00, -2.1707e+00, -3.4391e+00, -4.1387e+00],\n        [ 3.3215e-01,  6.0992e-01,  1.0510e+00,  7.3947e-01],\n        [ 2.0248e+00,  1.5392e+00,  2.0929e+00,  3.0412e+00],\n        [-1.4491e-02,  3.1548e-01, -1.4274e-03, -7.1759e-01]],\n       grad_fn=<MmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Computing the Unnomalized Attention Scores\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/979/484/600/775/671/original/9b79607f37fcf96f.png)","metadata":{}},{"cell_type":"code","source":"# computing the unnomalized attention weights, atten_scores\nattn_scores=queries.matmul(keys.T)\n\nprint(attn_scores)\nprint(attn_scores.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:54.985393Z","iopub.execute_input":"2024-02-23T11:33:54.986068Z","iopub.status.idle":"2024-02-23T11:33:54.995102Z","shell.execute_reply.started":"2024-02-23T11:33:54.98603Z","shell.execute_reply":"2024-02-23T11:33:54.994103Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[-4.5107e-03, -1.6400e-01, -5.6877e-01,  9.5348e-02,  4.3990e-01,\n         -7.0085e-02],\n        [ 1.5593e-01,  8.9439e-01,  3.9957e+00, -4.1141e-01, -3.6631e+00,\n          1.0034e-01],\n        [ 4.7362e-01,  2.3904e+00,  1.1067e+01, -1.0525e+00, -1.0338e+01,\n          1.4611e-01],\n        [-9.6320e-02, -8.0302e-01, -3.2902e+00,  4.0549e-01,  2.8685e+00,\n         -1.8383e-01],\n        [-3.2614e-01, -1.1686e+00, -6.0539e+00,  4.3631e-01,  5.9644e+00,\n          1.3164e-01],\n        [ 9.0325e-02,  4.0680e-01,  1.9495e+00, -1.7108e-01, -1.8529e+00,\n          3.9907e-03]], grad_fn=<MmBackward0>)\ntorch.Size([6, 6])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The output above is 6x6 tensor containing these pairwide unnomalized attention weights(also called attention scores) for the 6 input tokens.\n\n\n## Computing the Attention Weights(Normalized Attention Scores)\n\nWe can computed the attention weights(**normalized attention scores** that sum up to 1) via the softmax function as follows. Here we scale the `attention scores` by dividing them by the square root of the embedding dimension $\\sqrt{doutkq}$.\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/979/489/636/461/465/original/4b73761fe18799f7.png)","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nattn_weights=torch.softmax(attn_scores/d_out_kq**0.5, dim=-1)\nattn_weights","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:54.996396Z","iopub.execute_input":"2024-02-23T11:33:54.997498Z","iopub.status.idle":"2024-02-23T11:33:55.015958Z","shell.execute_reply.started":"2024-02-23T11:33:54.997454Z","shell.execute_reply":"2024-02-23T11:33:55.014203Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([[1.6775e-01, 1.4985e-01, 1.1256e-01, 1.8002e-01, 2.2968e-01, 1.6014e-01],\n        [5.1306e-02, 8.6487e-02, 7.7508e-01, 3.4352e-02, 3.4465e-03, 4.9329e-02],\n        [5.5658e-04, 2.1586e-03, 9.9665e-01, 1.8917e-04, 2.6631e-07, 4.4152e-04],\n        [8.1872e-02, 4.9672e-02, 8.5569e-03, 1.1674e-01, 6.6619e-01, 7.6960e-02],\n        [1.1096e-02, 6.1158e-03, 1.9329e-04, 1.9024e-02, 9.4823e-01, 1.5337e-02],\n        [1.2501e-01, 1.5636e-01, 4.6546e-01, 1.0391e-01, 3.1637e-02, 1.1761e-01]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Computing the Context Vector for Input\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/979/523/121/939/552/original/591cc4fccdfb656c.png)","metadata":{}},{"cell_type":"code","source":"context_vec=attn_weights@values\nprint(context_vec)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.017772Z","iopub.execute_input":"2024-02-23T11:33:55.019003Z","iopub.status.idle":"2024-02-23T11:33:55.026071Z","shell.execute_reply.started":"2024-02-23T11:33:55.01895Z","shell.execute_reply":"2024-02-23T11:33:55.025063Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[ 0.1686,  0.2090,  0.1601,  0.0557],\n        [-1.8487, -1.6793, -2.7011, -3.3196],\n        [-2.3373, -2.1642, -3.4295, -4.1279],\n        [ 1.3353,  1.0844,  1.4515,  1.9568],\n        [ 1.9218,  1.4734,  1.9996,  2.8780],\n        [-1.0856, -0.9284, -1.5621, -2.0410]], grad_fn=<MmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Implementing a Compact SelfAttention class\n\nWe can also use `nn.Linear` over the manual `nn.Parameter(torch.rand())` approach is that `nn.Linear` has preferred weight initialization scheme, whcih leads to more stable model training.","metadata":{}},{"cell_type":"code","source":"class SelfAttention_v1(nn.Module):\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.d_out=d_out\n        self.W_query=nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key=nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value=nn.Parameter(torch.rand(d_in, d_out))\n        \n    def forward(self, x):\n        keys=x@self.W_key\n        queries=x@self.W_query\n        values=x@self.W_value\n        \n        attn_scores=queries@keys.T # omega\n        atten_weights=torch.softmax(attn_scores/d_out_kq**0.5, dim=-1)\n        \n        context_vec=attn_weights@values\n        return context_vec\n\nsa_vl=SelfAttention_v1(d_in, d_out_kq)\nprint(sa_vl(embedded_sentence))\n\n\nclass SelfAttention_v2(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.d_out=d_out\n        self.W_query=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value=nn.Linear(d_in, d_out, bias=qkv_bias)\n    \n    def forward(self, x):\n        keys=self.W_key(x)\n        queries=self.W_query(x)\n        values=self.W_value(x)\n        \n        attn_scores=queries@keys.T\n        attn_weights=torch.softmax(attn_scores/d_out_kq**0.5, dim=-1)\n        \n        context_vec=attn_weights@values\n        return context_vec\n\nsa_v2=SelfAttention_v2(d_in, d_out_kq)\nprint(sa_v2(embedded_sentence))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.027851Z","iopub.execute_input":"2024-02-23T11:33:55.028879Z","iopub.status.idle":"2024-02-23T11:33:55.057537Z","shell.execute_reply.started":"2024-02-23T11:33:55.028835Z","shell.execute_reply":"2024-02-23T11:33:55.056401Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([[ 0.1027, -0.0081],\n        [-0.4864, -1.6660],\n        [-0.6283, -2.0349],\n        [ 0.4821,  1.1554],\n        [ 0.6878,  1.7792],\n        [-0.2683, -1.0762]], grad_fn=<MmBackward0>)\ntensor([[-0.1454,  0.0954],\n        [-0.2835,  0.0154],\n        [-0.4602, -0.1037],\n        [-0.0793,  0.1291],\n        [-0.1149,  0.1911],\n        [-0.2499,  0.0251]], grad_fn=<MmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Diding future words with causal attention\n\n## Applying a Mask to the Attention Weight Matrix\n\nIn GPT-like LLMs, we train the model to read and generate one token (or word) at a time, from left to right. If we have a training text sample like \"Life is short eat desert first\" we have the following setup, where the context vectors for the word to the right side of the arrow should only incorportate itself and the previous words:\n* \"Life\" -> \"is\"\n* \"Life is\" -> \"short\"\n* \"Life is short\" -> \"eat\"\n* \"Life is short eat\" -> \"desert\"\n* \"Life is short eat desert\" -> \"first\"\n\nThe simplest way to achieve this setup above is **to mask out all future tokens by applying a mask to the attention weight matrix above the diagonal**, as illustrated in the figure below. This way, \"future\" words will not be included when creating the context vectors, which are craeted as a attention-weighted sum over the inputs.\n\n<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/820/858/969/543/original/2e0ec840a282071d.webp\" width=\"80%\" heigh=\"80%\" alt=\"mask to the attention weight matrix above the diagonal\"></div>\n\n\nWe can achieve this via PyTorch's [tril](https://pytorch.org/docs/stable/generated/torch.tril.html#) funciton, which we first use to create a mask of 1's and 0's.","metadata":{}},{"cell_type":"code","source":"block_size=attn_scores.shape[0]\nmask_simple=torch.tril(torch.ones(block_size, block_size))\nmask_simple","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.058969Z","iopub.execute_input":"2024-02-23T11:33:55.059727Z","iopub.status.idle":"2024-02-23T11:33:55.073288Z","shell.execute_reply.started":"2024-02-23T11:33:55.05968Z","shell.execute_reply":"2024-02-23T11:33:55.072132Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, we multiply the attention weights with this mask to zero out all the attention weights above the diagonal:","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\nmasked_simple=attn_weights*mask_simple\nmasked_simple","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.077607Z","iopub.execute_input":"2024-02-23T11:33:55.078883Z","iopub.status.idle":"2024-02-23T11:33:55.087994Z","shell.execute_reply.started":"2024-02-23T11:33:55.078833Z","shell.execute_reply":"2024-02-23T11:33:55.086743Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([[1.6775e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.1306e-02, 8.6487e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.5658e-04, 2.1586e-03, 9.9665e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [8.1872e-02, 4.9672e-02, 8.5569e-03, 1.1674e-01, 0.0000e+00, 0.0000e+00],\n        [1.1096e-02, 6.1158e-03, 1.9329e-04, 1.9024e-02, 9.4823e-01, 0.0000e+00],\n        [1.2501e-01, 1.5636e-01, 4.6546e-01, 1.0391e-01, 3.1637e-02, 1.1761e-01]],\n       grad_fn=<MulBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Normalize the attention weights\n\nWhile the above is one wat to masked out future words, notice that the attention weights in each row don't sum to one anymore. To mitigate that, we can normalize the rows such that they sum up to 1 again, which is a standard convention for attention weights:","metadata":{}},{"cell_type":"code","source":"row_sums=masked_simple.sum(dim=1, keepdim=True)\nmasked_simple_norm=masked_simple/row_sums\nmasked_simple_norm","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.089465Z","iopub.execute_input":"2024-02-23T11:33:55.090505Z","iopub.status.idle":"2024-02-23T11:33:55.104883Z","shell.execute_reply.started":"2024-02-23T11:33:55.090459Z","shell.execute_reply":"2024-02-23T11:33:55.103625Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [3.7234e-01, 6.2766e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.5693e-04, 2.1600e-03, 9.9728e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [3.1876e-01, 1.9339e-01, 3.3315e-02, 4.5453e-01, 0.0000e+00, 0.0000e+00],\n        [1.1268e-02, 6.2110e-03, 1.9630e-04, 1.9320e-02, 9.6300e-01, 0.0000e+00],\n        [1.2501e-01, 1.5636e-01, 4.6546e-01, 1.0391e-01, 3.1637e-02, 1.1761e-01]],\n       grad_fn=<DivBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"As we can see, the attention weights in each row now sum up to 1.\n\nNormalizing attention weights in neural networks, such as intransformer models, is advantageous over unnormalized weights for two main reasons. \n\nFirst, normalized attention weights that sum to 1 resemble a probability distribution. This makes it easier to interpret the model's attention to various parts of the input in terms of proportions.\n\nSecond, by constraining the attention weights to sum to 1, this normalization helps control the scale of the weights and gradients to improve the training dynamics.\n\n\n## More efficient masking without renormalization\n\nIn the causal self-attention procesure we codede above, we first compute the attention scores, then compute the attention weights, mask out attention weights above the diagonal, and lastly renormalize the attention weights. This is summarized in the figure below:\n\n<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/915/897/930/128/original/d318a712263ef680.webp\" width=\"80%\" heigh=\"80%\" alt=\"implementing causal self-attention procedure\"></div>\n\n\nAlternatively, there is a more efficient way to achieve the same results. In this approach, we take the attention scores and replace the values above the diagonal with negative infinity before the value are input into the softmax function to compute the attention weights. This is summatized in the figure below:\n\n<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/964/024/803/792/original/ca94af87a2bfe55b.webp\" width=\"80%\" heigh=\"80%\" alt=\"more efficient approach to implementing causal self-attention\"></div>","metadata":{}},{"cell_type":"code","source":"mask=torch.triu(torch.ones(block_size, block_size), diagonal=1)\nmasked=attn_scores.masked_fill(mask.bool(), -torch.inf)\nmasked","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.106334Z","iopub.execute_input":"2024-02-23T11:33:55.106837Z","iopub.status.idle":"2024-02-23T11:33:55.123515Z","shell.execute_reply.started":"2024-02-23T11:33:55.106783Z","shell.execute_reply":"2024-02-23T11:33:55.122529Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([[-4.5107e-03,        -inf,        -inf,        -inf,        -inf,\n                -inf],\n        [ 1.5593e-01,  8.9439e-01,        -inf,        -inf,        -inf,\n                -inf],\n        [ 4.7362e-01,  2.3904e+00,  1.1067e+01,        -inf,        -inf,\n                -inf],\n        [-9.6320e-02, -8.0302e-01, -3.2902e+00,  4.0549e-01,        -inf,\n                -inf],\n        [-3.2614e-01, -1.1686e+00, -6.0539e+00,  4.3631e-01,  5.9644e+00,\n                -inf],\n        [ 9.0325e-02,  4.0680e-01,  1.9495e+00, -1.7108e-01, -1.8529e+00,\n          3.9907e-03]], grad_fn=<MaskedFillBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"Then, all we have to do is to apply the softmax function as usual to obtain the normalized and masked attention weights.","metadata":{}},{"cell_type":"code","source":"attn_weights=torch.softmax(masked/d_out_kq**0.5, dim=1)\nattn_weights","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.125036Z","iopub.execute_input":"2024-02-23T11:33:55.125705Z","iopub.status.idle":"2024-02-23T11:33:55.133982Z","shell.execute_reply.started":"2024-02-23T11:33:55.125649Z","shell.execute_reply":"2024-02-23T11:33:55.132822Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [3.7234e-01, 6.2766e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [5.5693e-04, 2.1600e-03, 9.9728e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [3.1876e-01, 1.9339e-01, 3.3315e-02, 4.5453e-01, 0.0000e+00, 0.0000e+00],\n        [1.1268e-02, 6.2110e-03, 1.9630e-04, 1.9320e-02, 9.6300e-01, 0.0000e+00],\n        [1.2501e-01, 1.5636e-01, 4.6546e-01, 1.0391e-01, 3.1637e-02, 1.1761e-01]],\n       grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Masking Additional Attention Weights With Dropout\n\nWe also apply dropout to reduce overfitting during training. Dropout can be applied in several places:\n* After computing the attention weights\n* After multiplying the attention weights with the value vectors\n\nHere we will apply the **dropout mask after computing the attention weights** because it's more common. Furthermore, in this specific example, we used a dropout rate of 50%, which means randomly masking out half of the attention weights.\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/980/465/043/909/999/original/579cbc84220685e2.png)\n\nIf we apply a dropout rate of 0.5(50%), the non-dropped values will be scaled accordingly by a factor of $1/0.5=2$","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\ndropout=torch.nn.Dropout(0.5)\nexample=torch.ones(6,6)\nprint(dropout(example))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.135271Z","iopub.execute_input":"2024-02-23T11:33:55.135704Z","iopub.status.idle":"2024-02-23T11:33:55.151521Z","shell.execute_reply.started":"2024-02-23T11:33:55.13565Z","shell.execute_reply":"2024-02-23T11:33:55.150526Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([[2., 2., 2., 2., 2., 2.],\n        [0., 2., 0., 0., 0., 0.],\n        [0., 0., 2., 0., 2., 0.],\n        [2., 2., 0., 0., 0., 2.],\n        [2., 0., 0., 0., 0., 2.],\n        [0., 2., 0., 0., 0., 0.]])\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(123)\nprint(dropout(attn_weights))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.153006Z","iopub.execute_input":"2024-02-23T11:33:55.153679Z","iopub.status.idle":"2024-02-23T11:33:55.161236Z","shell.execute_reply.started":"2024-02-23T11:33:55.153624Z","shell.execute_reply":"2024-02-23T11:33:55.160055Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 1.2553, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.9946, 0.0000, 0.0000, 0.0000],\n        [0.6375, 0.3868, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0225, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.3127, 0.0000, 0.0000, 0.0000, 0.0000]],\n       grad_fn=<MulBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implementing a Compact Causal Self-Attention Class\n\nLet's implement self-attention, including the causal and dropout masks. One more thing is to implement the code to handle batches consisting of more than one input so taht our `CausualAttention` class supports the batch outputs. For simplicy, we duplicate `embedded_sentence` which is defined on the top.","metadata":{}},{"cell_type":"code","source":"batch=torch.stack((embedded_sentence,embedded_sentence), dim=0)\nprint(batch.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.164496Z","iopub.execute_input":"2024-02-23T11:33:55.165317Z","iopub.status.idle":"2024-02-23T11:33:55.175451Z","shell.execute_reply.started":"2024-02-23T11:33:55.16528Z","shell.execute_reply":"2024-02-23T11:33:55.174337Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"torch.Size([2, 6, 3])\n","output_type":"stream"}]},{"cell_type":"code","source":"class CausalAttention(nn.Module):\n    \n    def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out=d_out\n        self.W_query=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout=nn.Dropout(dropout) # new\n        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1)) # new\n    \n    def forward(self, x):\n        b, num_tokens, d_in=x.shape # new batch dimension b\n        keys=self.W_key(x)\n        queries=self.W_query(x)\n        values=self.W_value(x)\n        \n        attn_scores=queries@keys.transpose(1,2)\n        attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n        attn_weights=torch.softmax(attn_scores/d_out_kq**0.5, dim=1)\n        attn_weights=self.dropout(attn_weights) # new\n        \n        context_vec=attn_weights @ values\n        return context_vec\n\n\ntorch.manual_seed(123)\n\nblock_size=batch.shape[1]\nca=CausalAttention(d_in, d_out_kq, block_size, 0.0)\n\ncontext_vecs=ca(batch)\n\nprint(context_vecs)\nprint('context_vecs.shape:', context_vecs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:33:55.178286Z","iopub.execute_input":"2024-02-23T11:33:55.179064Z","iopub.status.idle":"2024-02-23T11:33:55.196514Z","shell.execute_reply.started":"2024-02-23T11:33:55.179026Z","shell.execute_reply":"2024-02-23T11:33:55.195424Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"tensor([[[-0.1567, -0.2143],\n         [ 0.2843, -0.2274],\n         [ 0.9825, -0.2057],\n         [-0.2977, -0.2224],\n         [-0.4175, -0.2595],\n         [ 0.3927, -0.2499]],\n\n        [[-0.1567, -0.2143],\n         [ 0.2843, -0.2274],\n         [ 0.9825, -0.2057],\n         [-0.2977, -0.2224],\n         [-0.4175, -0.2595],\n         [ 0.3927, -0.2499]]], grad_fn=<UnsafeViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Extending Single-head Attention to Multi-head Attention\n\nBelow is a summary of the self-attention implemented previously (causal and dropout masks not shown for simplicity), this is also called single-head attention:\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/980/619/336/586/573/original/31d2f35596068266.png)\n\n\nWe simply stack multiple single-head attention modules to obtain a multi-head attention module. The main idea behind multi-head attention is to run the attention mechanism multiple times(om parallel) with different, learned lienar projections. This allows the model to jointly attend to information form different representation subspaces at different positions.\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/111/980/625/578/877/206/original/127f6ca33fd35d7c.png)","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads=nn.ModuleList(\n            [CausalAttention(d_in, d_out, block_size, dropout, qkv_bias) for _ in range(num_heads)]\n        )\n    \n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n\n\ntorch.manual_seed(123)\n\nblock_size=batch.shape[1] # this is thew number of tokens\nmha=MultiHeadAttentionWrapper(d_in, d_out_kq, block_size, 0.0, num_heads=2)\n\ncontext_vecs=mha(batch)\n\nprint(context_vecs)\nprint('context_vecs.shape:', context_vecs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:54:00.472268Z","iopub.execute_input":"2024-02-23T11:54:00.472674Z","iopub.status.idle":"2024-02-23T11:54:00.487464Z","shell.execute_reply.started":"2024-02-23T11:54:00.472626Z","shell.execute_reply":"2024-02-23T11:54:00.486246Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"tensor([[[-0.1567, -0.2143, -0.1012,  0.1098],\n         [ 0.2843, -0.2274, -0.2223,  0.0359],\n         [ 0.9825, -0.2057, -0.3437, -0.0636],\n         [-0.2977, -0.2224, -0.0402,  0.1407],\n         [-0.4175, -0.2595, -0.1181,  0.2790],\n         [ 0.3927, -0.2499, -0.1829,  0.0432]],\n\n        [[-0.1567, -0.2143, -0.1012,  0.1098],\n         [ 0.2843, -0.2274, -0.2223,  0.0359],\n         [ 0.9825, -0.2057, -0.3437, -0.0636],\n         [-0.2977, -0.2224, -0.0402,  0.1407],\n         [-0.4175, -0.2595, -0.1181,  0.2790],\n         [ 0.3927, -0.2499, -0.1829,  0.0432]]], grad_fn=<CatBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 4])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the implementation above, the embedding dimension is 4, because we $d_out_kq=2$ as the embedding dimension for the key, query, andd value vectors as well as the context vector. And since we have 2 attention heads, we have the output embedding dimension $2*2=4$.\n\nIf we want to have an output dimension of 2, as earlier in single-head attention, we can have to change the projection dimension `d_out` to 1:","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)\n\nd_out=1\nmha=MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\n\ncontext_vecs=mha(batch)\n\nprint(context_vecs)\nprint('context_vecs.shape:', context_vecs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:58:52.190516Z","iopub.execute_input":"2024-02-23T11:58:52.191143Z","iopub.status.idle":"2024-02-23T11:58:52.207735Z","shell.execute_reply.started":"2024-02-23T11:58:52.191099Z","shell.execute_reply":"2024-02-23T11:58:52.205364Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"tensor([[[ 0.1757, -0.2133],\n         [ 0.0478, -0.2435],\n         [-0.0677, -0.2676],\n         [ 0.2460, -0.1999],\n         [ 0.3556, -0.2085],\n         [ 0.0518, -0.2464]],\n\n        [[ 0.1757, -0.2133],\n         [ 0.0478, -0.2435],\n         [-0.0677, -0.2676],\n         [ 0.2460, -0.1999],\n         [ 0.3556, -0.2085],\n         [ 0.0518, -0.2464]]], grad_fn=<CatBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Implementing Multi-head Attention with Weight Splits\n\nWe can write a stand-alone class called `MultiHeadAttention` to achieve the same. We don't concatenate single attention heads for this stand-alone `MultiHeadAttention` class. Instead, we create single W_query, W_key, and W_value weight matrices and then split those into individual matrices for each attention head:","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads==0, \"d_out must be divisibel by n_heads\"\n        \n        self.d_out=d_out\n        self.num_heads=num_heads\n        self.head_dim=d_out//num_heads # reduce the projection dim to match desired output dim\n        \n        self.W_query=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value=nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj=nn.Linear(d_out, d_out) # linear layer to combine head outputs\n        #TODO","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why does this work? The softmax function, applied in the last step, converts the input values into a probability distribution. When -inf is present in the inputs, softmax effectively treats them as zero probability. This is because e^(inf) approaches 0, and thus these positions contribute nothing to the output probabilites.\n\n**Note: For the real word case, please consider optimize the self-attention by using Flash Attention, it helps to reducing memory footprint and computational load.**\n\n\n# Credit\n\n* https://magazine.sebastianraschka.com?utm_source=navbar&utm_medium=web&r=fbe14\n* https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb","metadata":{}}]}