{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42438e62",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004494,
     "end_time": "2024-01-26T07:20:51.706801",
     "exception": false,
     "start_time": "2024-01-26T07:20:51.702307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we are adapting the previously discussed self-attention mechanism into a causal self-attention mechanism, specifically for GPT-like(decoder-style) LLMs that are used to generate text. The previously notebooks are below:\n",
    "* [Encoder in Transformers Architecture](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture)\n",
    "* [Decoder in Transformers architecture](https://www.kaggle.com/code/aisuko/decoder-in-transformers-architecture)\n",
    "* [Coding the Self-Attention Mechanism](https://www.kaggle.com/code/aisuko/coding-the-self-attention-mechanism)\n",
    "* [Coding the Multi-Head Attention](https://www.kaggle.com/code/aisuko/coding-the-multi-head-attention)\n",
    "* [Coding Cross-Attention](https://www.kaggle.com/code/aisuko/coding-cross-attention)\n",
    "\n",
    "\n",
    "And the causal self-attention mechanism is also often referred to as **masked self-attention**. In the original transformer architecture, it corresponds to the \"masked multi-head attention\" module - for simplicity, we will look at a single attention head in this section, but the same concept generalizes to multiple heads.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/570/310/327/483/original/9c619019f8f9a286.webp\" width=\"80%\" heigh=\"80%\" alt=\"Causal self-attetion/masked self-attention\"></div>\n",
    "\n",
    "**It ensures that the outputs for a certain position in a sequence is based only on the known outputs at previous positions and not on future positions**. In simper terms, it ensures that the prediction for each next word should only depend on the preceding words. To achieve this in GPT-like LLMs, for each token processed, we mask out the feature tokens, which come after the current token in the input text.\n",
    "\n",
    "The application of a causal mask to the attention weights for hiding future input tokens in the inputs is illustrated in the figure below. Our input is \"Life is short, eat desert first.\"\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/654/573/649/346/original/ee97aa86264ac573.webp\" width=\"80%\" heigh=\"80%\" alt=\"causal mask to the attention weights for hiding future input tokens\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f33f0",
   "metadata": {
    "papermill": {
     "duration": 0.003628,
     "end_time": "2024-01-26T07:20:51.714697",
     "exception": false,
     "start_time": "2024-01-26T07:20:51.711069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To illustrate and implement causal self-attention, let's work with the unweighted attention scores and attention weights from the previous section. First, we quickly recap the computation of the attention scores from the previous Self-Attention section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452a72c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T07:20:51.724946Z",
     "iopub.status.busy": "2024-01-26T07:20:51.724145Z",
     "iopub.status.idle": "2024-01-26T07:20:55.249150Z",
     "shell.execute_reply": "2024-01-26T07:20:55.248375Z"
    },
    "papermill": {
     "duration": 3.534035,
     "end_time": "2024-01-26T07:20:55.252644",
     "exception": false,
     "start_time": "2024-01-26T07:20:51.718609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [-0.4015,  0.9666, -1.1481]])\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 1.8214e-02,  1.7208e-02,  1.3132e-01,  1.5569e-02, -1.6673e-01,\n",
      "          1.8989e-03],\n",
      "        [ 2.0647e-01,  3.8309e-01,  1.9873e+00,  8.9122e-02, -2.2238e+00,\n",
      "          1.9144e-01],\n",
      "        [ 7.2581e-01,  1.3610e+00,  7.0241e+00,  3.0666e-01, -7.8430e+00,\n",
      "          6.8589e-01],\n",
      "        [-9.2898e-02, -2.1432e-01, -1.0055e+00, -2.0607e-02,  1.0751e+00,\n",
      "         -1.2405e-01],\n",
      "        [-5.9726e-01, -1.0785e+00, -5.6701e+00, -2.7160e-01,  6.3803e+00,\n",
      "         -5.2696e-01],\n",
      "        [ 1.1150e-01,  1.5801e-01,  9.4357e-01,  7.0838e-02, -1.1142e+00,\n",
      "          5.9216e-02]], grad_fn=<MmBackward0>)\n",
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#Tokenization\n",
    "sentence='Lieft is short, eat dessert first'\n",
    "sentence_ids={s:i for i,s in enumerate(sorted(sentence.replace(',','').split()))}\n",
    "sentence_tokens=torch.tensor([sentence_ids[s] for s in sentence.replace(',','').split()])\n",
    "print(sentence_tokens)\n",
    "\n",
    "\n",
    "# Embedding\n",
    "vocab_size=len(sentence_ids)\n",
    "embed=torch.nn.Embedding(vocab_size, 3)\n",
    "embedded_sentence=embed(sentence_tokens).detach()\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "d_in, d_out_kq, d_out_v=3,2,4\n",
    "\n",
    "W_query=nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "W_key=nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "W_value=nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "\n",
    "x=embedded_sentence\n",
    "\n",
    "keys=x.matmul(W_key)\n",
    "queries=x.matmul(W_query)\n",
    "values=x.matmul(W_value)\n",
    "\n",
    "# computing the unnomalized attention weights, atten_scores\n",
    "attn_scores=queries.matmul(keys.T)\n",
    "\n",
    "print(attn_scores)\n",
    "print(attn_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b624b3",
   "metadata": {
    "papermill": {
     "duration": 0.004309,
     "end_time": "2024-01-26T07:20:55.262492",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.258183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The output above is 6x6 tensor containing these pairwide unnomalized attention weights(also called attention scores) for the 6 input tokens.\n",
    "\n",
    "\n",
    "# Computing the attention weights\n",
    "\n",
    "We can computed the scaled dot-product attention via the softmax function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7281e837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T07:20:55.271869Z",
     "iopub.status.busy": "2024-01-26T07:20:55.271358Z",
     "iopub.status.idle": "2024-01-26T07:20:55.284536Z",
     "shell.execute_reply": "2024-01-26T07:20:55.283547Z"
    },
    "papermill": {
     "duration": 0.020109,
     "end_time": "2024-01-26T07:20:55.286524",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.266415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6816e-01, 1.6804e-01, 1.8216e-01, 1.6785e-01, 1.4755e-01, 1.6623e-01],\n",
       "        [1.2912e-01, 1.4629e-01, 4.5485e-01, 1.1884e-01, 2.3156e-02, 1.2775e-01],\n",
       "        [1.1084e-02, 1.7369e-02, 9.5250e-01, 8.2413e-03, 2.5901e-05, 1.0776e-02],\n",
       "        [1.4800e-01, 1.3582e-01, 7.7628e-02, 1.5576e-01, 3.3802e-01, 1.4477e-01],\n",
       "        [6.9948e-03, 4.9772e-03, 1.9362e-04, 8.8060e-03, 9.7168e-01, 7.3512e-03],\n",
       "        [1.6155e-01, 1.6695e-01, 2.9095e-01, 1.5697e-01, 6.7906e-02, 1.5568e-01]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights=torch.softmax(attn_scores/d_out_kq**0.5, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dc216",
   "metadata": {
    "papermill": {
     "duration": 0.0038,
     "end_time": "2024-01-26T07:20:55.294493",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.290693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Applying a mask to the attention weight matrix\n",
    "\n",
    "In GPT-like LLMs, we train the model to read and generate one token (or word) at a time, from left to right. If we have a training text sample like \"Life is short eat desert first\" we have the following setup, where the context vectors for the word to the right side of the arrow should only incorportate itself and the previous words:\n",
    "* \"Life\" -> \"is\"\n",
    "* \"Life is\" -> \"short\"\n",
    "* \"Life is short\" -> \"eat\"\n",
    "* \"Life is short eat\" -> \"desert\"\n",
    "* \"Life is short eat desert\" -> \"first\"\n",
    "\n",
    "The simplest way to achieve this setup above is **to mask out all future tokens by applying a mask to the attention weight matrix above the diagonal**, as illustrated in the figure below. This way, \"future\" words will not be included when creating the context vectors, which are craeted as a attention-weighted sum over the inputs.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/820/858/969/543/original/2e0ec840a282071d.webp\" width=\"80%\" heigh=\"80%\" alt=\"mask to the attention weight matrix above the diagonal\"></div>\n",
    "\n",
    "\n",
    "We can achieve this via PyTorch's [tril](https://pytorch.org/docs/stable/generated/torch.tril.html#) funciton, which we first use to create a mask of 1's and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a49008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T07:20:55.304232Z",
     "iopub.status.busy": "2024-01-26T07:20:55.303816Z",
     "iopub.status.idle": "2024-01-26T07:20:55.314355Z",
     "shell.execute_reply": "2024-01-26T07:20:55.313299Z"
    },
    "papermill": {
     "duration": 0.018003,
     "end_time": "2024-01-26T07:20:55.316466",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.298463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=attn_scores.shape[0]\n",
    "mask_simple=torch.tril(torch.ones(block_size, block_size))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37856c",
   "metadata": {
    "papermill": {
     "duration": 0.003999,
     "end_time": "2024-01-26T07:20:55.324792",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.320793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we multiply the attention weights with this mask to zero out all the attention weights above the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e33481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T07:20:55.335060Z",
     "iopub.status.busy": "2024-01-26T07:20:55.334628Z",
     "iopub.status.idle": "2024-01-26T07:20:55.342428Z",
     "shell.execute_reply": "2024-01-26T07:20:55.341557Z"
    },
    "papermill": {
     "duration": 0.015403,
     "end_time": "2024-01-26T07:20:55.344388",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.328985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6816e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.2912e-01, 1.4629e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.1084e-02, 1.7369e-02, 9.5250e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.4800e-01, 1.3582e-01, 7.7628e-02, 1.5576e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [6.9948e-03, 4.9772e-03, 1.9362e-04, 8.8060e-03, 9.7168e-01, 0.0000e+00],\n",
       "        [1.6155e-01, 1.6695e-01, 2.9095e-01, 1.5697e-01, 6.7906e-02, 1.5568e-01]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple=attn_weights*mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591be489",
   "metadata": {
    "papermill": {
     "duration": 0.00421,
     "end_time": "2024-01-26T07:20:55.353251",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.349041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Normalize the attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547583f",
   "metadata": {
    "papermill": {
     "duration": 0.00469,
     "end_time": "2024-01-26T07:20:55.362394",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.357704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "While the above is one wat to masked out future words, notice that the attention weights in each row don't sum to one anymore. To mitigate that, we can normalize the rows such that they sum up to 1 again, which is a standard convention for attention weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4144ec84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T07:20:55.373200Z",
     "iopub.status.busy": "2024-01-26T07:20:55.372746Z",
     "iopub.status.idle": "2024-01-26T07:20:55.382197Z",
     "shell.execute_reply": "2024-01-26T07:20:55.381156Z"
    },
    "papermill": {
     "duration": 0.01739,
     "end_time": "2024-01-26T07:20:55.384373",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.366983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [4.6882e-01, 5.3118e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.1300e-02, 1.7706e-02, 9.7099e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.8615e-01, 2.6261e-01, 1.5009e-01, 3.0116e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [7.0466e-03, 5.0141e-03, 1.9505e-04, 8.8712e-03, 9.7887e-01, 0.0000e+00],\n",
       "        [1.6155e-01, 1.6695e-01, 2.9095e-01, 1.5697e-01, 6.7906e-02, 1.5568e-01]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums=masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm=masked_simple/row_sums\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467443a8",
   "metadata": {
    "papermill": {
     "duration": 0.004347,
     "end_time": "2024-01-26T07:20:55.393446",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.389099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see, the attention weights in each row now sum up to 1.\n",
    "\n",
    "Normalizing attention weights in neural networks, such as intransformer models, is advantageous over unnormalized weights for two main reasons. \n",
    "\n",
    "First, normalized attention weights that sum to 1 resemble a probability distribution. This makes it easier to interpret the model's attention to various parts of the input in terms of proportions.\n",
    "\n",
    "Second, by constraining the attention weights to sum to 1, this normalization helps control the scale of the weights and gradients to improve the training dynamics.\n",
    "\n",
    "**More efficient masking without renormalization**\n",
    "\n",
    "In the causal self-attention procesure we codede above, we first compute the attention scores, then compute the attention weights, mask out attention weights above the diagonal, and lastly renormalize the attention weights. This is summarized in the figure below:\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/915/897/930/128/original/d318a712263ef680.webp\" width=\"80%\" heigh=\"80%\" alt=\"implementing causal self-attention procedure\"></div>\n",
    "\n",
    "\n",
    "Alternatively, there is a more efficient way to achieve the same results. In this approach, we take the attention scores and replace the values above the diagonal with negative infinity before the value are input into the softmax function to compute the attention weights. This is summatized in the figure below:\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/820/964/024/803/792/original/ca94af87a2bfe55b.webp\" width=\"80%\" heigh=\"80%\" alt=\"more efficient approach to implementing causal self-attention\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411d5cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T07:20:55.404645Z",
     "iopub.status.busy": "2024-01-26T07:20:55.404269Z",
     "iopub.status.idle": "2024-01-26T07:20:55.415232Z",
     "shell.execute_reply": "2024-01-26T07:20:55.414299Z"
    },
    "papermill": {
     "duration": 0.019234,
     "end_time": "2024-01-26T07:20:55.417399",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.398165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0182,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.2065,  0.3831,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.7258,  1.3610,  7.0241,    -inf,    -inf,    -inf],\n",
       "        [-0.0929, -0.2143, -1.0055, -0.0206,    -inf,    -inf],\n",
       "        [-0.5973, -1.0785, -5.6701, -0.2716,  6.3803,    -inf],\n",
       "        [ 0.1115,  0.1580,  0.9436,  0.0708, -1.1142,  0.0592]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "masked=attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f871b8f",
   "metadata": {
    "papermill": {
     "duration": 0.00463,
     "end_time": "2024-01-26T07:20:55.426834",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.422204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then, all we have to do is to apply the softmax function as usual to obtain the normalized and masked attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773a1064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T07:20:55.438715Z",
     "iopub.status.busy": "2024-01-26T07:20:55.437906Z",
     "iopub.status.idle": "2024-01-26T07:20:55.446140Z",
     "shell.execute_reply": "2024-01-26T07:20:55.445312Z"
    },
    "papermill": {
     "duration": 0.016544,
     "end_time": "2024-01-26T07:20:55.448265",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.431721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [4.6882e-01, 5.3118e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.1300e-02, 1.7706e-02, 9.7099e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.8615e-01, 2.6261e-01, 1.5009e-01, 3.0116e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [7.0466e-03, 5.0141e-03, 1.9505e-04, 8.8712e-03, 9.7887e-01, 0.0000e+00],\n",
       "        [1.6155e-01, 1.6695e-01, 2.9095e-01, 1.5697e-01, 6.7906e-02, 1.5568e-01]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights=torch.softmax(masked/d_out_kq**0.5, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bd855",
   "metadata": {
    "papermill": {
     "duration": 0.00471,
     "end_time": "2024-01-26T07:20:55.458051",
     "exception": false,
     "start_time": "2024-01-26T07:20:55.453341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Why does this work? The softmax function, applied in the last step, converts the input values into a probability distribution. When -inf is present in the inputs, softmax effectively treats them as zero probability. This is because e^(inf) approaches 0, and thus these positions contribute nothing to the output probabilites.\n",
    "\n",
    "Note: For the real word case, please consider optimize the self-attention by using Flash Attention, it helps to reducing memory footprint and computational load.\n",
    "\n",
    "\n",
    "# Credit\n",
    "\n",
    "* https://magazine.sebastianraschka.com?utm_source=navbar&utm_medium=web&r=fbe14"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.590083,
   "end_time": "2024-01-26T07:20:56.284499",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-26T07:20:47.694416",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
