{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43f06a9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004301,
     "end_time": "2024-01-26T00:00:42.911787",
     "exception": false,
     "start_time": "2024-01-26T00:00:42.907486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "In the [previously notebook](https://www.kaggle.com/code/aisuko/coding-the-multi-head-attention/notebook), we set $d_{q}=d_{k}=24$ and $d_{v}=28$. Or in other words, we used the same dimensions for query and key sequences. While the value matrix $W_{v}$ is often chosen to have the same dimension as the query and key matrices (such as in PyTorch's MultiHeadAttention class), we can select an arbitrary number size for the value dimensions.\n",
    "\n",
    "Since the dimensions are sometimes a bit tricky to keep track of, let's summarize everything we have covered so far in the figure below, which depicts the various tensor sizes for a single attention head.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/819/100/204/990/207/original/09d504e16eb77ccc.png\" width=\"80%\" heigh=\"80%\" alt=\"Single-Attention-head\"></div>\n",
    "\n",
    "Now, the illustration above corresponds to the self-attention mechanism used in transformers. One particulat favor of thie attention mechanism we have yet to discusss is cross-attention.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/819/117/595/682/670/original/ae132461563b60ef.png\" width=\"80%\" heigh=\"80%\" alt=\"Cross-attention in transformers\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf6c73",
   "metadata": {
    "papermill": {
     "duration": 0.003322,
     "end_time": "2024-01-26T00:00:42.918968",
     "exception": false,
     "start_time": "2024-01-26T00:00:42.915646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross-attention\n",
    "\n",
    "> What is cross-attention, and how does it differ from self-attention?\n",
    "\n",
    "In Self-attention, we work with the same input sequqnce. In cross-attention, we mix or combine two **different** input sequences. In the case of the original transformer architecture above, that's the sequence returned by the encoder module on the left and the input sequence beding processed by the decoder part on the right.\n",
    "\n",
    "Note that in cross-attention, the two input sequences $x_{1}$ and $x_{2}$ can have different number of elements. However, their embedding dimensions must match.\n",
    "\n",
    "The figure below illustrates the concept of cross-attention. If we set $x_{1}=x_{2}$, this is equivalent to self-attention.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/819/188/074/548/534/original/4c78ecc1a1d67280.png\" width=\"80%\" heigh=\"80%\" alt=\"Cross-attention\"></div>\n",
    "\n",
    "**Note: The queries usually come from the decoder, and the keys and values usually come from the encoder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8ff97",
   "metadata": {
    "papermill": {
     "duration": 0.003117,
     "end_time": "2024-01-26T00:00:42.925660",
     "exception": false,
     "start_time": "2024-01-26T00:00:42.922543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We implemented the self-attention machnism in [Coding the self atttention mechanism](https://www.kaggle.com/code/aisuko/coding-the-self-attention-mechanism) the code like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8922335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T00:00:42.935176Z",
     "iopub.status.busy": "2024-01-26T00:00:42.934706Z",
     "iopub.status.idle": "2024-01-26T00:00:47.113885Z",
     "shell.execute_reply": "2024-01-26T00:00:47.112282Z"
    },
    "papermill": {
     "duration": 4.187954,
     "end_time": "2024-01-26T00:00:47.117019",
     "exception": false,
     "start_time": "2024-01-26T00:00:42.929065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 16])\n",
      "torch.Size([24])\n",
      "torch.Size([13, 24])\n",
      "torch.Size([13, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs=\"According to the news, it it hard to say Melbourne is safe now\"\n",
    "d_q, d_k, d_v=24,24,28\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "input_ids={s:i for i,s in enumerate(sorted(inputs.replace(',','').split()))}\n",
    "input_tokens=torch.tensor([input_ids[s] for s in inputs.replace(',','').split()])\n",
    "\n",
    "embed=torch.nn.Embedding(13,16)\n",
    "embedded_sentence=embed(input_tokens).detach()\n",
    "d=embedded_sentence.shape[1]\n",
    "\n",
    "W_query=torch.rand(d_q, d)\n",
    "W_key=torch.rand(d_k, d)\n",
    "W_value=torch.rand(d_v, d)\n",
    "\n",
    "x_2=embedded_sentence[1]\n",
    "query_2=W_query.matmul(x_2)\n",
    "\n",
    "keys=W_key.matmul(embedded_sentence.T).T\n",
    "values=W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(embedded_sentence.shape)\n",
    "print(query_2.shape)\n",
    "print(keys.shape)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7adfb2",
   "metadata": {
    "papermill": {
     "duration": 0.003476,
     "end_time": "2024-01-26T00:00:47.124651",
     "exception": false,
     "start_time": "2024-01-26T00:00:47.121175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the coding of cross attention, the only different part is that we have a second input sequence. Here is an example, a second setence with 8 instead of 6 input elements. Here, suppose this is a sentence with 8 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790bb28a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-26T00:00:47.134473Z",
     "iopub.status.busy": "2024-01-26T00:00:47.133847Z",
     "iopub.status.idle": "2024-01-26T00:00:47.142653Z",
     "shell.execute_reply": "2024-01-26T00:00:47.140908Z"
    },
    "papermill": {
     "duration": 0.017573,
     "end_time": "2024-01-26T00:00:47.145930",
     "exception": false,
     "start_time": "2024-01-26T00:00:47.128357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 24])\n",
      "torch.Size([8, 28])\n"
     ]
    }
   ],
   "source": [
    "embedded_sentence_2=torch.rand(8,16)\n",
    "\n",
    "keys=W_key.matmul(embedded_sentence_2.T).T\n",
    "values=W_value.matmul(embedded_sentence_2.T).T\n",
    "\n",
    "print(keys.shape)\n",
    "print(values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aae325",
   "metadata": {
    "papermill": {
     "duration": 0.003254,
     "end_time": "2024-01-26T00:00:47.153035",
     "exception": false,
     "start_time": "2024-01-26T00:00:47.149781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice that compared to self-attention, the keys and values now have 8 instead of 6 rows. Everything else stays the same.\n",
    "\n",
    "We talked a lot about language transformers above. In the original transformer architecture, cross-attention is useful when we go from an input sentence to an output sentence in the context of language translation. The input sentence represents one input sequence, and the translation represent the second input sequence(the two sentences can different number of words).\n",
    "\n",
    "Another popular model where cross-attention is used is Stable Diffusion. Stable Diffusion uses cross-attention between the generated image in the U-Net model and the text prompts used for conditioning as described in High_resolution Image Synthesis with Latent DIffusion Models - the original paper that describes the Stable Diffusion model that was later adopted by Stability AI to implement the popular Stable Diffusion model.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/819/275/403/095/727/small/6a9c96fe1c7f32ac.png\" width=\"80%\" heigh=\"80%\" alt=\"Cross-attention in diffusion\"></div>\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "We discussed how self-attention works using a step-by-step coding approach. We then extended this concept to multi-head-attention, the widely used component of large-language transformers. After discussing self-attetnion and multi-head attetnion, we introduced yet another concept: cross-attention, which is a flavor of self-attention what we can apply between two different sequences. Finally, thanks for [Sebastian Raschka, PhD](https://www.linkedin.com/in/sebastianraschka/)'s beautiful articles(see it in credit section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa013dd",
   "metadata": {
    "papermill": {
     "duration": 0.00322,
     "end_time": "2024-01-26T00:00:47.159708",
     "exception": false,
     "start_time": "2024-01-26T00:00:47.156488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Credit\n",
    "\n",
    "* https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.347405,
   "end_time": "2024-01-26T00:00:48.488248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-26T00:00:39.140843",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
