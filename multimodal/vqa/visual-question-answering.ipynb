{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/visual-question-answering?scriptVersionId=164950683\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nVisual Question Answering(VQA) is the task of answering open-ended questions based on an image. The input to model supporting this task is typically a combination of an image and a question, and the output is an answer expressed in natural language. Some noteworthy use case examples for VQA include:\n\n- Accessibility applications for visually impaired individuals\n- `Education:` posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n- `Customer service and e-commerce:` VQA can enhance user experience by letting users ask questions about products.\n- `Image retrieval:` VQA models can be used to retrieve images with specific characteristics. For examples, the user can ask \"Is there a dog?\" to find all images with dogs from a set of images.\n\nLet's fine-tune **a classification** VQA model, specifically ViLT(Vision and Language Transformer Without Convolution or Region Supervision), on the VQA dataset.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Fine-tuning ViLT\n\nViLT model incorporates text embeddings into a Vision Transformer(ViT), allowing it to have a minial design for Vision-and-Language Pre-training(VLP). This model can be used for severl downstream tasks. For the VQA task, a classifier head is placed on top(a linear layer on top of the final hidden state of the CLS token) and randomly initialized.\n\nVisual Question Answering is thus treated as a **classification problem**. However, more recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.35.2\n!pip install datasets==2.15.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning vilt-b32-mlm\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune model distilbert base uncased\"\nos.environ[\"WANDB_NAME\"] = \"ft-vilt-b32-mlm\"\nos.environ[\"MODEL_NAME\"] = \"dandelin/vilt-b32-mlm\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data\n\nFeel free to choose different size datasets by using `split` parameter.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset=load_dataset(\"aisuko/vqa\", split=\"validation[:500]\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These features belows relevant to the task, so we can remove other features.\n- `question`: the question to be answerted from the image\n- `image_id`: the path to the image the question refers to\n- `label`: the annotations","metadata":{}},{"cell_type":"code","source":"# dataset=dataset.remove_columns(['question_type', 'question_id','answer_type'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The label feature contains several answers to the same question(called `ids` here) collected by different human annotators. This is because the answer to a question can be subjective. In this case, the question is \"where is he looking?\".\n\nSome poeple annotated this with \"down\", others with \"at table\", another one with \"skateboard\", etc.","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nimage=Image.open(dataset[0]['image_id'])\nimage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to the questions' and answer's ambiguity, datasets like this are treated as a multi-label classification problem( as multiple answers are possibly valid). Moreover, rather than just creating a one-hot encoded vector, one creates a soft encoding, based on the number of times a certain answer appeared in the annotations. For instance, in the example above, because the answer \"down\" is selected way more often than other answers, it has a score (called `weight` in the dataset) of 1.0, and the rest of the answers have scores<1.0. To later instantiate the model with an appropriate classification head, let's create two dictionaries: one that maps the label name to an integer and vice versa:","metadata":{}},{"cell_type":"code","source":"import itertools\n\nlabels=[item['ids'] for item in dataset['label']]\nflattened_labels=list(itertools.chain(*labels))\nunique_labels=list(set(flattened_labels))\n\nlabel2id={label:idx for idx,label in enumerate(unique_labels)}\nid2label={idx: label for label, idx in label2id.items()}\n\ndef replace_ids(inputs):\n    inputs[\"label\"][\"ids\"]=[label2id[x] for x in inputs[\"label\"][\"ids\"]]\n    return inputs\n\n\ndataset=dataset.map(replace_ids)\nflat_dataset=dataset.flatten()\nflat_dataset.features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing data\n\nHere we are going to load a ViLT processor to prepare the image and text data for the model. **ViltProcessor** wraps a BERT tokenizer and ViLT image processor into a convenient single processor:","metadata":{}},{"cell_type":"code","source":"from transformers import ViltProcessor\n\nprocessor=ViltProcessor.from_pretrained(os.getenv('MODEL_NAME'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To preprocess the data we need to encode the images and questions using the **ViltProcessor**. The processor will use the **BertTokenizeFast** to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids` for the text data. As for images, the processor will leverage **ViltImageProcessor** to resize and normalize the image, and create `pixel_values` and `pixel_mask`.\n\nAll these preprocessing steps are done under the hood, we only need to call the processor. However, we still need to prepare the target labels. In this representation, each element corresponds to a possible answer(label). For correct answers, the element holds their respective score (weight), while the remaining elements are set to zero. The following function applies the processor to the images and questions and formats the labels as described above:\n\nWe will apply the preprocessing function over the entire dataset. Setting `batched=True` to speed up `map` to process multiple elements of the dataset at once. And we can also remove the columns we don't need:","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef preprocess_data(examples):\n    image_paths=examples['image_id']\n    images=[Image.open(image_path) for image_path in image_paths]\n    texts=examples['question']\n    \n    encoding=processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    \n    for k,v in encoding.items():\n        encoding[k]=v.squeeze()\n    \n    targets=[]\n    \n    for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n        target=torch.zeros(len(id2label))\n        \n        for label, score in zip(labels, scores):\n            target[label]=score\n            \n        targets.append(target)\n    encoding[\"labels\"]=targets\n    \n    return encoding\n\nprocessed_dataset=flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type','question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])\nprocessed_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Batch of Data\n\nCreating a batch of examples using DefaultDataCollator","metadata":{}},{"cell_type":"code","source":"from transformers import DefaultDataCollator\n\ndata_collator=DefaultDataCollator()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import ViltForQuestionAnswering\n\nmodel=ViltForQuestionAnswering.from_pretrained(os.getenv('MODEL_NAME'), num_labels=len(id2label), id2label=id2label, label2id=label2id, device_map='auto')\nprint(model.config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    per_device_train_batch_size=4,\n    num_train_epochs=5,\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=5e-5,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=False,\n    fp16=True,\n    report_to=\"wandb\", # or report_to=\"tensorboard\"\n    run_name=os.getenv(\"WANDB_NAME\"),\n)\n\ntrainer=Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=processed_dataset,\n    tokenizer=processor,\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(os.getenv(\"WANDB_NAME\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\npipe=pipeline(\"visual-question-answering\", model=os.getenv(\"WANDB_NAME\"), device_map='auto')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example=dataset[0]\nimage=Image.open(example['image_id'])\nquestion=example['question']\n\nprint(question)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe(image, question, top_k=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or manually replicate the results of the pipeline:\n\n1. Take an image and a question, prepare them for the model using the processor from the model\n2. Forward the result or preprocessing through the model\n3. From the logits, get the most likely answer's id, and find the actual answer in the `id2label`","metadata":{}},{"cell_type":"code","source":"processor=ViltProcessor.from_pretrained(os.getenv(\"WANDB_NAME\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=Image.open(example['image_id'])\nquestion=example['question']\n\ninputs=processor(image, question, return_tensors=\"pt\")\n\nmodel=ViltForQuestionAnswering.from_pretrained(os.getenv(\"WANDB_NAME\"))\n\n\n# forward pass\nwith torch.no_grad():\n    outputs=model(**inputs)\n\nlogits=outputs.logits\nidx=logits.argmax(-1).item()\nprint(\"Predicted answer\", model.config.id2label[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}