{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/zero-shot-object-detection?scriptVersionId=164771326\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nTraditionally, models used for [object detection](https://www.kaggle.com/code/aisuko/object-detection) require labeled image datasets for training, and are limited to detecting the set of classes from the training data. Zero-shot object detection is supported by the OWL-ViT model which uses a different approach. OWL-ViT is an open-vocabulary object detector. It means that it can detect objects in images based on free-text queries without the need to fine-tune the model on labeled datasets. OWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) with lightweight object classification and localization heads. **Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads;associate images and their corresponding textual descriptions, and ViT processes image patches as inputs.** The authors of OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using a bipartite matching loss. With this apporach, the model can detect objects based on textual descriptions without prior training on labeled datasets.\n\nHere we will use OWL-ViT:\n- Detection objects based on text prompts\n- For batch object detection\n- For image-guided object detection","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.35.2","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:56:03.966848Z","iopub.execute_input":"2023-12-11T10:56:03.96732Z","iopub.status.idle":"2023-12-11T10:56:22.557412Z","shell.execute_reply.started":"2023-12-11T10:56:03.967284Z","shell.execute_reply":"2023-12-11T10:56:22.556148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero-Shot Object Detection Pipeline","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nmodel_checkpoint=\"google/owlvit-base-patch32\"\ndetector=pipeline(model=model_checkpoint, task=\"zero-shot-object-detection\")\ndetector.enable_cpu_offloading()\ndetector.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:56:22.560365Z","iopub.execute_input":"2023-12-11T10:56:22.561375Z","iopub.status.idle":"2023-12-11T10:56:58.178916Z","shell.execute_reply.started":"2023-12-11T10:56:22.561317Z","shell.execute_reply":"2023-12-11T10:56:58.177627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Image","metadata":{}},{"cell_type":"code","source":"import skimage\nimport numpy as np\nfrom PIL import Image\n\nimage=skimage.data.astronaut()\nimage=Image.fromarray(np.uint8(image)).convert(\"RGB\")\nimage","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:56:58.18062Z","iopub.execute_input":"2023-12-11T10:56:58.18219Z","iopub.status.idle":"2023-12-11T10:56:58.901117Z","shell.execute_reply.started":"2023-12-11T10:56:58.182129Z","shell.execute_reply":"2023-12-11T10:56:58.899356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pass the image and the candidate object labels to look for the pipeline. Here we pass the image directly; other suitable options include a local path to an image or an image url. We also pass text descriptions for all items we want to query the image for.","metadata":{}},{"cell_type":"code","source":"predictions=detector(image, candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],)\npredictions","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:56:58.904725Z","iopub.execute_input":"2023-12-11T10:56:58.905298Z","iopub.status.idle":"2023-12-11T10:57:06.586459Z","shell.execute_reply.started":"2023-12-11T10:56:58.90525Z","shell.execute_reply":"2023-12-11T10:57:06.584818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageDraw\n\ndraw=ImageDraw.Draw(image)\n\nfor prediction in predictions:\n    box=prediction[\"box\"]\n    label=prediction[\"label\"]\n    score=prediction[\"score\"]\n    \n    xmin,ymin, xmax,ymax=box.values()\n    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n    draw.text((xmin,ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\n\nimage","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:57:06.588835Z","iopub.execute_input":"2023-12-11T10:57:06.58982Z","iopub.status.idle":"2023-12-11T10:57:06.789682Z","shell.execute_reply.started":"2023-12-11T10:57:06.589768Z","shell.execute_reply":"2023-12-11T10:57:06.788232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text-prompted zero-shot object detection by hand","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n\nmodel=AutoModelForZeroShotObjectDetection.from_pretrained(model_checkpoint)\nprocessor=AutoProcessor.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:57:06.791573Z","iopub.execute_input":"2023-12-11T10:57:06.793012Z","iopub.status.idle":"2023-12-11T10:57:10.395372Z","shell.execute_reply.started":"2023-12-11T10:57:06.792913Z","shell.execute_reply":"2023-12-11T10:57:10.394253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\nurl=\"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\nim=Image.open(requests.get(url, stream=True).raw)\nim","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:57:10.39729Z","iopub.execute_input":"2023-12-11T10:57:10.398271Z","iopub.status.idle":"2023-12-11T10:57:11.100901Z","shell.execute_reply.started":"2023-12-11T10:57:10.398223Z","shell.execute_reply":"2023-12-11T10:57:11.099372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_queries=[\"hat\", \"book\", \"sunglasses\", \"camera\"]\ninputs=processor(text=text_queries, images=im, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:57:11.102908Z","iopub.execute_input":"2023-12-11T10:57:11.103391Z","iopub.status.idle":"2023-12-11T10:57:11.163341Z","shell.execute_reply.started":"2023-12-11T10:57:11.103355Z","shell.execute_reply":"2023-12-11T10:57:11.161406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will need to resize the images before feeding them to the model by using the `post_process_object_detection()` to make sure the predicted bounding boxes have the correct coordinates relative to the orignal image:","metadata":{}},{"cell_type":"code","source":"import torch\n\nwith torch.no_grad():\n    outputs=model(**inputs)\n    target_sizes=torch.tensor([im.size[::-1]])\n    results=processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n\ndraw=ImageDraw.Draw(im)\n\nscores=results[\"scores\"].tolist()\nlabels=results[\"labels\"].tolist()\nboxes=results[\"boxes\"].tolist()\n\nfor box, score, label in zip(boxes, scores, labels):\n    xmin, ymin, xmax,ymax=box\n    draw.rectangle((xmin, ymin,xmax, ymax), outline=\"red\", width=1)\n    draw.text((xmin,ymin), f\"{text_queries[label]}:{round(score,2)}\", fill=\"white\")\n\nim","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:57:11.165345Z","iopub.execute_input":"2023-12-11T10:57:11.165978Z","iopub.status.idle":"2023-12-11T10:57:12.989803Z","shell.execute_reply.started":"2023-12-11T10:57:11.165905Z","shell.execute_reply":"2023-12-11T10:57:12.98826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Batch processing\n\nWe can pass multiple sets of images and text queries to search for different(or same) objects in several images. Let's use astronaut image and the beach image together. For batch processing, we should pass text queries as a nested list to the processor and images as list of PIL images, PyTorch tensors, or NumPy arrays.","metadata":{}},{"cell_type":"code","source":"im_batch=Image.open(requests.get(url, stream=True).raw)\n\nimages=[image, im_batch]\n\ntext_queries=[\n    [\"human face\", \"rocket\", \"nasa badge\",\"start-spangled banner\"],\n    [\"hat\",\"book\",\"sunglasses\", \"camera\"],\n]\n\ninputs=processor(text=text_queries, images=images, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:59:56.056371Z","iopub.execute_input":"2023-12-11T10:59:56.056848Z","iopub.status.idle":"2023-12-11T10:59:56.58172Z","shell.execute_reply.started":"2023-12-11T10:59:56.056803Z","shell.execute_reply":"2023-12-11T10:59:56.580245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With several images, we can pass a tuples. Here we are going to create predictions for the two examples, and visualize the second one(image_idx=1).","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    outputs=model(**inputs)\n    target_sizes=[x.size[::-1] for x in images]\n    results=processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\n    \nimage_idx=1\ndraw=ImageDraw.Draw(images[image_idx])\n\nscores=results[image_idx][\"scores\"].tolist()\nlabels=results[image_idx][\"labels\"].tolist()\nboxes=results[image_idx][\"boxes\"].tolist()\n\nfor box, score, label in zip(boxes, scores, labels):\n    xmin, ymin, xmax, ymax=box\n    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n    draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}:{round(score,2)}\", fill=\"white\")\n\nimages[image_idx]","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:59:59.448129Z","iopub.execute_input":"2023-12-11T10:59:59.448587Z","iopub.status.idle":"2023-12-11T11:00:02.206838Z","shell.execute_reply.started":"2023-12-11T10:59:59.448549Z","shell.execute_reply":"2023-12-11T11:00:02.205548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image-guided object detection\n\nIt means we can use an image query to find similar objects in the target image, and an image of a single cat as a query:","metadata":{}},{"cell_type":"code","source":"url=\"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage_target=Image.open(requests.get(url, stream=True).raw)\n\nquery_url=\"http://images.cocodataset.org/val2017/000000524280.jpg\"\nquery_image=Image.open(requests.get(query_url, stream=True).raw)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:00:10.858368Z","iopub.execute_input":"2023-12-11T11:00:10.858775Z","iopub.status.idle":"2023-12-11T11:00:11.965175Z","shell.execute_reply.started":"2023-12-11T11:00:10.858745Z","shell.execute_reply":"2023-12-11T11:00:11.963518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Take a quick look at the images\nfig, ax=plt.subplots(1,2)\nax[0].imshow(image_target)\nax[1].imshow(query_image)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:00:12.834697Z","iopub.execute_input":"2023-12-11T11:00:12.835226Z","iopub.status.idle":"2023-12-11T11:00:13.609292Z","shell.execute_reply.started":"2023-12-11T11:00:12.83518Z","shell.execute_reply":"2023-12-11T11:00:13.607672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the query_iamges","metadata":{}},{"cell_type":"code","source":"inputs=processor(images=image_target, query_images=query_image, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:00:33.396431Z","iopub.execute_input":"2023-12-11T11:00:33.397058Z","iopub.status.idle":"2023-12-11T11:00:33.518799Z","shell.execute_reply.started":"2023-12-11T11:00:33.397005Z","shell.execute_reply":"2023-12-11T11:00:33.517379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    outputs=model.image_guided_detection(**inputs)\n    target_sizes=torch.tensor([image_target.size[::-1]])\n    results=processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\n\ndraw=ImageDraw.Draw(image_target)\n\nscores=results[\"scores\"].tolist()\nboxes=results[\"boxes\"].tolist()\n\nfor box, score, label in zip(boxes, scores, labels):\n    xmin, ymin, xmax, ymax=box\n    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\n\nimage_target","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:00:36.117822Z","iopub.execute_input":"2023-12-11T11:00:36.118679Z","iopub.status.idle":"2023-12-11T11:00:39.233272Z","shell.execute_reply.started":"2023-12-11T11:00:36.118619Z","shell.execute_reply":"2023-12-11T11:00:39.231668Z"},"trusted":true},"execution_count":null,"outputs":[]}]}