{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nLet's see how to use custom models on Civit platforms with diffusers.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install diffusers==0.23.1\n!pip install omegaconf==2.3.0\n!pip install compel==2.0.2","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:59:46.882286Z","iopub.execute_input":"2024-02-05T09:59:46.883257Z","iopub.status.idle":"2024-02-05T10:00:29.457996Z","shell.execute_reply.started":"2024-02-05T09:59:46.883215Z","shell.execute_reply":"2024-02-05T10:00:29.456992Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting diffusers==0.23.1\n  Downloading diffusers-0.23.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (3.13.1)\nRequirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (0.20.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (6.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (1.24.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (2.31.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.23.1) (0.4.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.23.1) (2023.12.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.23.1) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.23.1) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.23.1) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.2->diffusers==0.23.1) (21.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.23.1) (3.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.23.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.23.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.23.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.23.1) (2023.11.17)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.13.2->diffusers==0.23.1) (3.1.1)\nDownloading diffusers-0.23.1-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.23.1\nCollecting omegaconf==2.3.0\n  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf==2.3.0)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf==2.3.0) (6.0.1)\nBuilding wheels for collected packages: antlr4-python3-runtime\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=ebe508583e074e75351d997a4e15512033fc076cf89fcd60a5f80f7749867938\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, omegaconf\nSuccessfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\nCollecting compel==2.0.2\n  Downloading compel-2.0.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: diffusers>=0.11 in /opt/conda/lib/python3.10/site-packages (from compel==2.0.2) (0.23.1)\nRequirement already satisfied: pyparsing~=3.0 in /opt/conda/lib/python3.10/site-packages (from compel==2.0.2) (3.1.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from compel==2.0.2) (2.1.2)\nRequirement already satisfied: transformers~=4.25 in /opt/conda/lib/python3.10/site-packages (from compel==2.0.2) (4.37.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (3.13.1)\nRequirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (0.20.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (6.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (1.24.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (2.31.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.11->compel==2.0.2) (0.4.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.25->compel==2.0.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.25->compel==2.0.2) (6.0.1)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.25->compel==2.0.2) (0.15.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers~=4.25->compel==2.0.2) (4.66.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->compel==2.0.2) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->compel==2.0.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->compel==2.0.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->compel==2.0.2) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->compel==2.0.2) (2023.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers>=0.11->compel==2.0.2) (3.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->compel==2.0.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers>=0.11->compel==2.0.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers>=0.11->compel==2.0.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers>=0.11->compel==2.0.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers>=0.11->compel==2.0.2) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->compel==2.0.2) (1.3.0)\nDownloading compel-2.0.2-py3-none-any.whl (30 kB)\nInstalling collected packages: compel\nSuccessfully installed compel-2.0.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here we will download the custom model is based on `StableDiffusion V1.5`.","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\ncivitai_key = user_secrets.get_secret(\"CIVITAI\")\nos.environ['CIVITAI_KEY']=civitai_key\nos.environ[\"CHECKPOINT_MERGED\"] = \"meichidark_mix_v3.5.safetensors\"\nos.environ[\"LORA_ADAPTER\"] = \"armor.safetensors\"","metadata":{"execution":{"iopub.status.busy":"2024-02-05T10:00:37.926330Z","iopub.execute_input":"2024-02-05T10:00:37.926988Z","iopub.status.idle":"2024-02-05T10:00:38.194585Z","shell.execute_reply.started":"2024-02-05T10:00:37.926952Z","shell.execute_reply":"2024-02-05T10:00:38.193606Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Downloading the custom checkpoint(merged)\n\nWe download the merged checkpoint first.","metadata":{}},{"cell_type":"code","source":"!wget --content-disposition -O ${CHECKPOINT_MERGED} https://civitai.com/api/download/models/90778&token=${CIVITAI_KEY}","metadata":{"execution":{"iopub.status.busy":"2024-02-05T10:00:56.181940Z","iopub.execute_input":"2024-02-05T10:00:56.182714Z","iopub.status.idle":"2024-02-05T10:00:57.204588Z","shell.execute_reply.started":"2024-02-05T10:00:56.182677Z","shell.execute_reply":"2024-02-05T10:00:57.203526Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!wget --content-disposition -O ${LORA_ADAPTER} https://civitai.com/api/download/models/224823&token=${CIVITAI_KEY}","metadata":{"execution":{"iopub.status.busy":"2024-02-05T10:01:22.282833Z","iopub.execute_input":"2024-02-05T10:01:22.283206Z","iopub.status.idle":"2024-02-05T10:01:23.224899Z","shell.execute_reply.started":"2024-02-05T10:01:22.283171Z","shell.execute_reply":"2024-02-05T10:01:23.223675Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Loading the custom checkpoint(merged)","metadata":{}},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\nimport torch\n\n# Loading the pretrained checkpoint(merged) from downloaded safetensors\npipe=StableDiffusionPipeline.from_single_file(\n    os.getenv('CHECKPOINT_MERGED'),\n    use_safetensors=True,\n    torch_dtype=torch.float16 # For CUDA\n)\npipe.enable_model_cpu_offload()\npipe","metadata":{"execution":{"iopub.status.busy":"2024-02-05T10:01:33.277932Z","iopub.execute_input":"2024-02-05T10:01:33.278828Z","iopub.status.idle":"2024-02-05T10:01:52.629817Z","shell.execute_reply.started":"2024-02-05T10:01:33.278792Z","shell.execute_reply":"2024-02-05T10:01:52.628560Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dfdf19630504b4e853addefb7e3e329"}},"metadata":{}},{"name":"stderr","text":"2024-02-05 10:01:41.235586: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-05 10:01:41.235695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-05 10:01:41.385034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Loading the pretrained checkpoint(merged) from downloaded safetensors\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pipe\u001b[38;5;241m=\u001b[39m\u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_single_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCHECKPOINT_MERGED\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# For CUDA\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m pipe\u001b[38;5;241m.\u001b[39menable_model_cpu_offload()\n\u001b[1;32m     11\u001b[0m pipe\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diffusers/loaders.py:2796\u001b[0m, in \u001b[0;36mFromSingleFileMixin.from_single_file\u001b[0;34m(cls, pretrained_model_link_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ckpt_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m   2795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_valid_url_prefix:\n\u001b[0;32m-> 2796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2797\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided path is either not a file or a valid huggingface URL was not provided. Valid URLs begin with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(valid_url_prefixes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2798\u001b[0m         )\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;66;03m# get repo_id and (potentially nested) file path of ckpt in repo\u001b[39;00m\n\u001b[1;32m   2801\u001b[0m     repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ckpt_path\u001b[38;5;241m.\u001b[39mparts[:\u001b[38;5;241m2\u001b[39m])\n","\u001b[0;31mValueError\u001b[0m: The provided path is either not a file or a valid huggingface URL was not provided. Valid URLs begin with https://huggingface.co/, huggingface.co/, hf.co/, https://hf.co/"],"ename":"ValueError","evalue":"The provided path is either not a file or a valid huggingface URL was not provided. Valid URLs begin with https://huggingface.co/, huggingface.co/, hf.co/, https://hf.co/","output_type":"error"}]},{"cell_type":"markdown","source":"# Loading the custom LoRA","metadata":{}},{"cell_type":"code","source":"# pipe.load_lora_weights(os.getenv('LORA_ADAPTER'))\n# pipe","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:56:51.520734Z","iopub.status.idle":"2024-02-05T09:56:51.521124Z","shell.execute_reply.started":"2024-02-05T09:56:51.520942Z","shell.execute_reply":"2024-02-05T09:56:51.520958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting Clip_skip","metadata":{}},{"cell_type":"code","source":"clip_skip=2\npipe.text_encoder.text_model.encoder.layers=pipe.text_encoder.text_model.encoder.layers[:-clip_skip]\npipe.safety_checker=None","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:56:51.522194Z","iopub.status.idle":"2024-02-05T09:56:51.522531Z","shell.execute_reply.started":"2024-02-05T09:56:51.522363Z","shell.execute_reply":"2024-02-05T09:56:51.522377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading scheduler","metadata":{}},{"cell_type":"code","source":"from diffusers import (EulerAncestralDiscreteScheduler,\n                       EulerDiscreteScheduler,\n                       DPMSolverMultistepScheduler)\nscheduler='none'\n\nif scheduler=='EDS':\n    pipe.scheduler= EulerDiscreteScheduler.from_config(pipe.scheduler.config)\nelif scheduler=='EADS':\n    pipe.scheduler=EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\nelse:\n    pipe.scheduler=DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.to('cuda')\npipe","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:56:51.524568Z","iopub.status.idle":"2024-02-05T09:56:51.525067Z","shell.execute_reply.started":"2024-02-05T09:56:51.524812Z","shell.execute_reply":"2024-02-05T09:56:51.524832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the prompt\n\nWe want to overcome the 77 tokens limit. So, we use [compel](https://github.com/damian0815/compel) to embedding the prompts to a PyTorch tensor.","metadata":{}},{"cell_type":"code","source":"from compel import Compel\n\n\nembeddings=True\n\nprompt='((a female wearing a red hood)), fantasy theme, medieval, fierce look, ((dynamic poses)), tattoo on her arms, white ruffled sleeve shirt, choker,  gloves, corset, leather pants, belt, waist sash, cape,  (((masterpiece))),  ((best quality)), ((intricate detailed)), ((Hyperrealistic)), a woman with perfect body figure wearing cyberpunk cloth, pale skin, ((huge breast)),  highly detailed, illustration, perfect hands, detailed fingers, beautiful detailed eyes, red hair, black hair, multiple color hair, long hair, (fantasy:1.2),  armor, detailed background, tavern , night, light by candle, lens flare, tempting look, looking at the viewer, from above,  <lora:adventurers_v1:1>  <lora:sxz-niji-v2:0.6>'\nnegative_prompt='easynegative, badhandv4, (low quality, worst quality:1.4), poorly drawn hands, bad anatomy, monochrome, { long body }, bad anatomy, liquid body, malformed, mutated, anatomical nonsense, bad proportions, uncoordinated body, unnatural body, disfigured, ugly, gross proportions, mutation, disfigured, deformed, { mutation}, {poorlydrawn}, bad hand, mutated hand, bad fingers, mutated fingers,   badhandv4, liquid tongue, long neck, fused ears, bad ears, poorly drawn ears, extra ears, liquid ears, heavy ears, missing ears, fused animal ears, bad animal ears, poorly drawn animal ears, extra animal ears, liquid animal ears, heavy animal ears, missing animal ears, bad hairs, poorly drawn hairs, fused hairs, bad face, fused face, poorly drawn face, cloned face, big face, long face, bad eyes, fused eyes poorly drawn eyes, extra eyes, bad mouth, fused mouth, poorly drawn mouth, bad tongue, big mouth, bad perspective, bad objects placement, NSFW, bad weapon, fused weapon, extra weapons, poorly weapon, bad sword, poor sword'\n\n\ncompel=Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\ncon_embeds=compel([prompt])\nneg_embeds=compel([negative_prompt])\n\nprint(con_embeds.size())\nprint(neg_embeds.size())\n\n\n# Warning\n# Below function will cause the different size of two prompts(con, neg_promt)\n\n# max_length=pipe.tokenizer.model_max_length\n# def get_prompt_embeddings(prompt, negative_prompt):\n#     count_prompt=len(prompt.split(' '))\n#     count_negative_prompt=len(prompt.split(' '))\n    \n#     if count_prompt>=count_negative_prompt:\n#         input_ids=pipe.tokenizer(prompt, \n#                                   truncation=False, \n#                                   return_tensors='pt').input_ids.to('cuda')\n#         shape_max_length=input_ids.shape[-1]\n        \n#         negative_ids=pipe.tokenizer(negative_prompt, truncation=False, \n#                                     padding='max_length', \n#                                     max_length=shape_max_length, \n#                                     return_tensors='pt').input_ids.to('cuda')\n#     else:\n#         negative_ids=pipe.tokenizer(negative_prompt, truncation=False,\n#                                    return_tensors='pt').input_ids.to('cuda')\n#         shape_max_length=negative_ids.shape[-1]\n#         input_ids=pipe.tokenizer(prompt, truncation=False,\n#                                 padding='max_length',\n#                                 max_length=shape_max_length,\n#                                 return_tensors='pt').input_ids.to('cuda')\n        \n#     concat_embeds=[]\n#     neg_embeds=[]\n#     for i in range(0,shape_max_length, max_length):\n#         concat_embeds.append(pipe.text_encoder(input_ids[:,i:i+max_length])[0])\n#         neg_embeds.append(pipe.text_encoder(negative_ids[:,i:i+max_length])[0])\n    \n#     prompt_embeddings=torch.cat(concat_embeds, dim=1)\n#     negative_prompt_embeddings=torch.cat(neg_embeds, dim=1)\n    \n#     return prompt_embeddings, negative_prompt_embeddings\n\n# if embeddings:\n#     prompt_embed, negative_embed=get_prompt_embeddings(prompt, negative_prompt)\n# print(prompt_embed.size())\n# print(negative_embed.size())","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:56:51.526318Z","iopub.status.idle":"2024-02-05T09:56:51.526839Z","shell.execute_reply.started":"2024-02-05T09:56:51.526554Z","shell.execute_reply":"2024-02-05T09:56:51.526575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if embeddings:\n    imgs=pipe(prompt_embeds=con_embeds,\n             negative_prompt_embeds=neg_embeds,\n             width=512,\n             height=832,\n             guidance_scale=7.0,\n             num_inference_steps=35,\n             num_images_per_prompt=1,\n             generator=torch.manual_seed(93421173)).images\nelse:\n    imgs=pipe(prompt=prompt,\n              negative_prompt=negative_prompt,\n              width=512,height=832,\n              guidance_scale=12.0,num_inference_steps=50,\n              num_images_per_prompt=1,\n              generator=torch.manual_seed(0)).images\nimgs[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:56:51.527861Z","iopub.status.idle":"2024-02-05T09:56:51.528348Z","shell.execute_reply.started":"2024-02-05T09:56:51.528098Z","shell.execute_reply":"2024-02-05T09:56:51.528133Z"},"trusted":true},"execution_count":null,"outputs":[]}]}