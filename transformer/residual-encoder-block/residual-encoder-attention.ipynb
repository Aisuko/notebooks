{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/residual-encoder-attention?scriptVersionId=164087786\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"cb79059b","metadata":{"papermill":{"duration":0.002466,"end_time":"2024-02-24T07:13:13.570939","exception":false,"start_time":"2024-02-24T07:13:13.568473","status":"completed"},"tags":[]},"source":["# Overview\n","\n","The **Residual Encoder Attention block** is a key component of Transformer models, which are widely used in natural language processing tasks. This block is designed to process a sequence of input data by applying **self-sttention** and **position-wise feed-forward networks**, with **residual connections**. See more detail in notebook [Transformer From Scratch](https://www.kaggle.com/code/aisuko/transformer-from-scratch#Residual-Connection)\n","\n","The **residual connections** help to migrate the problem of vanishing gradients during training, making it easier to train deep models. The **layer normalization** helps to stabilize the learning process.\n","\n","This block is typically used as a building block for larger models, with several such blocks stacked togather.\n","\n","\n","# Implementation\n","\n","It is a simple implementation of multi-head self-attention using PyTorch's built in functions."]},{"cell_type":"code","execution_count":1,"id":"d9a268a1","metadata":{"execution":{"iopub.execute_input":"2024-02-24T07:13:13.576806Z","iopub.status.busy":"2024-02-24T07:13:13.576372Z","iopub.status.idle":"2024-02-24T07:13:16.998979Z","shell.execute_reply":"2024-02-24T07:13:16.997751Z"},"papermill":{"duration":3.428542,"end_time":"2024-02-24T07:13:17.001563","exception":false,"start_time":"2024-02-24T07:13:13.573021","status":"completed"},"tags":[]},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, n_state, n_head):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.n_head=n_head\n","        self.attention=nn.MultiheadAttention(n_state, n_head)\n","    \n","    def forward(self, x):\n","        return self.attention(x,x,x)[0]\n","\n","\n","class MLP(nn.Module):\n","    \"\"\"\n","    MLP is a simple implementaion of a feed-forward neural network(also known as a multi-layer perceptron)\n","    with two linear layers and a ReLU acivation function.\n","    \"\"\"\n","    def __init__(self, n_state):\n","        super(MLP, self).__init__()\n","        self.fc1=nn.Linear(n_state, n_state)\n","        self.fc2=nn.Linear(n_state, n_state)\n","    \n","    def forward(self, x):\n","        return self.fc2(F.relu(self.fc1(x)))\n","\n","\n","class ResidualEncoderAttentionBlock(nn.Module):\n","    def __init__(self, n_state, n_head):\n","        super(ResidualEncoderAttentionBlock, self).__init__()\n","        self.attn=MultiHeadSelfAttention(n_state, n_head)\n","        self.attn_ln=nn.LayerNorm(n_state)\n","        self.mlp=MLP(n_state)\n","        self.mlp_ln=nn.LayerNorm(n_state)\n","\n","    def forward(self, x):\n","        x=x+self.attn(self.attn_ln(x))\n","        x=x+self.mlp(self.mlp_ln(x))\n","        return x"]},{"cell_type":"markdown","id":"d1867d24","metadata":{"papermill":{"duration":0.001629,"end_time":"2024-02-24T07:13:17.005327","exception":false,"start_time":"2024-02-24T07:13:17.003698","status":"completed"},"tags":[]},"source":["# Testing\n","\n","This test case creates a `ResidualEncoderAttentionBlock` with **4 hidden units** and **2 attention heads**, and **a batch of 1 sequences**, **each of length 2**, **with 4 features**. It then processes the batch through the block and checks that the output has the same shape as the input.\n","\n","If the block is implement correctly, this test should pass."]},{"cell_type":"code","execution_count":2,"id":"c560661c","metadata":{"execution":{"iopub.execute_input":"2024-02-24T07:13:17.013308Z","iopub.status.busy":"2024-02-24T07:13:17.012481Z","iopub.status.idle":"2024-02-24T07:13:17.155949Z","shell.execute_reply":"2024-02-24T07:13:17.154377Z"},"papermill":{"duration":0.151907,"end_time":"2024-02-24T07:13:17.159003","exception":false,"start_time":"2024-02-24T07:13:17.007096","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["ResidualEncoderAttentionBlock(\n","  (attn): MultiHeadSelfAttention(\n","    (attention): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n","    )\n","  )\n","  (attn_ln): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n","  (mlp): MLP(\n","    (fc1): Linear(in_features=4, out_features=4, bias=True)\n","    (fc2): Linear(in_features=4, out_features=4, bias=True)\n","  )\n","  (mlp_ln): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",")\n","Tensor Size torch.Size([3, 2, 4])\n"]}],"source":["import torch\n","\n","torch.manual_seed(123)\n","\n","def test_res_en_atten_block():\n","    # Initialize a block with 4 hidden units and 2 attention heads\n","    block=ResidualEncoderAttentionBlock(n_state=4, n_head=2)\n","    print(block)\n","    \n","    # Create a batch of 1 sequences, each of length 2, with 4 features\n","    x=torch.randn(3, 2, 4)\n","    print(f'Tensor Size {x.size()}')\n","    \n","    # Process the batch through the block\n","    y=block(x)\n","    \n","    # Check that the output has the same shape as the input\n","    assert y.shape==x.shape\n","\n","test_res_en_atten_block()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":8.319278,"end_time":"2024-02-24T07:13:18.089624","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-24T07:13:09.770346","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}