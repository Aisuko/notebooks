{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3687455",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003958,
     "end_time": "2024-03-20T00:35:12.924024",
     "exception": false,
     "start_time": "2024-03-20T00:35:12.920066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "The Attention mechanism generates scores, determining how much focus to place on each data part. These are used to create a weighted sum of the inputs, which feeds intot he next network layer. This allows the model to capture context and relationships within the data that might be missed with traditional, fixed approaches to processing sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da0fbf",
   "metadata": {
    "papermill": {
     "duration": 0.00263,
     "end_time": "2024-03-20T00:35:12.929955",
     "exception": false,
     "start_time": "2024-03-20T00:35:12.927325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# String to Numberic arrays\n",
    "\n",
    "We wil simulate the process of setences -> tokens -> embeddings(vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765b6d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T00:35:12.937994Z",
     "iopub.status.busy": "2024-03-20T00:35:12.937258Z",
     "iopub.status.idle": "2024-03-20T00:35:12.957851Z",
     "shell.execute_reply": "2024-03-20T00:35:12.956631Z"
    },
    "papermill": {
     "duration": 0.027576,
     "end_time": "2024-03-20T00:35:12.960472",
     "exception": false,
     "start_time": "2024-03-20T00:35:12.932896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings:\n",
      "The:[20, 8, 5]\n",
      "weather:[23, 5, 1, 20, 8, 5, 18]\n",
      "is:[9, 19]\n",
      "beautiful:[2, 5, 1, 21, 20, 9, 6, 21, 12]\n",
      "today:[20, 15, 4, 1, 25]\n",
      "in:[9, 14]\n",
      "Melbourne:[13, 5, 12, 2, 15, 21, 18, 14, 5]\n",
      "\n",
      "Attention Weights\n",
      "The:0.081\n",
      "weather:0.189\n",
      "is:0.054\n",
      "beautiful:0.243\n",
      "today:0.135\n",
      "in:0.054\n",
      "Melbourne:0.243\n",
      "\n",
      "Final Vector Before Transformation:\n",
      "[13.297297297297298, 7.837837837837839]\n",
      "\n",
      "Processed Output: 21.135135135135137\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "sentence=\"The weather is beautiful today in Melbourne\"\n",
    "words=sentence.split()\n",
    "\n",
    "# simulate embeddings\n",
    "embed={word: [ord(char) -96 for char in word.lower()] for word in words}\n",
    "print(\"Word embeddings:\")\n",
    "for word, embedding in embed.items():\n",
    "    print(f\"{word}:{embedding}\")\n",
    "    \n",
    "# generate attention weights\n",
    "total_characters=sum(len(word) for word in words)\n",
    "attention_weights={word: len(word)/total_characters for word in words}\n",
    "print(\"\\nAttention Weights\")\n",
    "for word, weight in attention_weights.items():\n",
    "    print(f\"{word}:{weight:.3f}\")\n",
    "    \n",
    "# compute weighted sum of embeddings\n",
    "weighted_embeddings={word: [weight*val for val in embedding] \n",
    "                     for word, embedding in embed.items() \n",
    "                     for word_weight, weight in attention_weights.items() if word==word_weight}\n",
    "final_vector= [0]*len(max(embed.values(), key=len))\n",
    "for embedding in weighted_embeddings.values():\n",
    "    final_vector=[sum(x) for x in zip(final_vector, embedding)]\n",
    "    \n",
    "print(\"\\nFinal Vector Before Transformation:\")\n",
    "print(final_vector)\n",
    "\n",
    "processed_output=sum(final_vector)\n",
    "print(f\"\\nProcessed Output: {processed_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293d104",
   "metadata": {
    "papermill": {
     "duration": 0.002796,
     "end_time": "2024-03-20T00:35:12.966475",
     "exception": false,
     "start_time": "2024-03-20T00:35:12.963679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Scaled Dot Product Attention\n",
    "\n",
    "Attention can be done is many different ways. Dot product attention where the alignment funciton is a dot product. [Paper](https://arxiv.org/pdf/1508.04025v5.pdf). And scaled dot product attention was introduced by [Paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/125/036/271/570/911/small/b4d359206e83008b.webp)\n",
    "\n",
    "Here is a step-by-step approach to scaled dot product attention\n",
    "\n",
    "\n",
    "## Step1: Deriving Q, K and V from Input Embeddings\n",
    "\n",
    "### Q\n",
    "Matrix contianing the query vectors. **These represent the set of items you want to draw attntion to**. In the context of processing a sentence, **a query is typically associated with the current word**. The model uses the query to seek out **relevant information acorss the sequence**.\n",
    "\n",
    "### K\n",
    "Matrix containing the key vectors. **Keys are paired with the values and are used to retrieve information**. Each key is associated with a value in a way that the model can use the similarity between a quary and a key to **determine how much attention to pay to the corresponding value**.\n",
    "\n",
    "### V\n",
    "Matrix containing the value vectors. **Values hold the actual information the model wants to retrieve**. **Once the model determines which key(and thereby values) are most relevant to a given query**, it aggregates these values, weighted by their relevance, to produce the output.\n",
    "\n",
    "\n",
    "## Step2: Obtain Q, K and V\n",
    "\n",
    "**1. Tokenizing and Embeddings**\n",
    "\n",
    "Tokenizing the input and embedding the token. These embeddings capture the semantic meaning of the input elements.\n",
    "\n",
    "**2. Learned Linear Transformations**\n",
    "\n",
    "The model learns separate lienar transformations(weight matrices) to project the input embeddings into query, key and value spaces. These transformations are part of the model's parameters and are optimized during training. In short, we apply these **weight matrices** to the input embeddings to get Q, K and V.\n",
    "\n",
    "**3. Purpose of Different Spaces**\n",
    "\n",
    "BY projecting the input into three different spaces, the model can independently manipulate the aspects of the input that are used to calculate attention weights (via Q and K) and the aspects that are used to compute the output of the attention mechnism (via V).\n",
    "\n",
    "\n",
    "## Step3: Calculate Dot Products of Q and K Transpose\n",
    "\n",
    "We use numpy's built in function to do this.\n",
    "\n",
    "## Step4: Get the Scaled Dot Product\n",
    "\n",
    "We will divide dot products by the square root of the dimensions of the keys to prevent large values of dot products.\n",
    "\n",
    "## Step5: Apply Softmax to the Scaled Dot Product\n",
    "\n",
    "Softmax is basically a math function that converts numbers in the vectos into probabilities. Here it transforms a given array of numbers(the vector) into an array of new numbers that add to 1.\n",
    "\n",
    "## Step6: Multiply by V\n",
    "\n",
    "Multiply the attention weights with the value matrix V. This step aggregates the values based on the weights, **essentially selecting which values to focus on**. It's basically connecting the probabilities back tot he input matrix via V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46f88c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T00:35:12.974601Z",
     "iopub.status.busy": "2024-03-20T00:35:12.973895Z",
     "iopub.status.idle": "2024-03-20T00:35:12.994300Z",
     "shell.execute_reply": "2024-03-20T00:35:12.993159Z"
    },
    "papermill": {
     "duration": 0.027685,
     "end_time": "2024-03-20T00:35:12.997104",
     "exception": false,
     "start_time": "2024-03-20T00:35:12.969419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test softmax: [0.65900114 0.24243297 0.09856589]\n",
      "\n",
      "Softmax probabilities:[[4.32998049e-26 3.61905363e-18 7.68690348e-18 1.93320955e-19\n",
      "  1.05466928e-20 1.36151497e-20 7.26962744e-19 2.84585507e-19\n",
      "  5.59467740e-17 2.95566807e-20]\n",
      " [5.88669680e-19 2.80923880e-05 1.21716717e-04 1.58252314e-07\n",
      "  1.49977318e-09 1.37981948e-09 2.25480403e-06 2.80652025e-07\n",
      "  3.81273701e-03 6.25136086e-09]\n",
      " [1.60347517e-18 2.53910646e-04 1.18440729e-03 1.59045369e-06\n",
      "  8.06628380e-09 1.03642422e-08 1.85415675e-05 2.92487764e-06\n",
      "  4.89879897e-02 4.87276624e-08]\n",
      " [6.19644087e-20 6.39497291e-07 2.10552398e-06 5.33383647e-09\n",
      "  5.06620913e-11 5.98807319e-11 4.99802902e-08 1.01120592e-08\n",
      "  5.05837443e-05 2.24678588e-10]\n",
      " [6.33971370e-21 9.04927416e-09 3.36648201e-08 1.12829171e-10\n",
      "  1.46662877e-12 1.99890269e-12 8.25447245e-10 1.53658576e-10\n",
      "  6.93568740e-07 5.24842012e-12]\n",
      " [1.87789995e-21 7.06756408e-10 2.02619597e-09 7.97124319e-12\n",
      "  1.78268766e-13 1.70331937e-13 6.02065534e-11 1.47646523e-11\n",
      "  3.80461087e-08 4.96671682e-13]\n",
      " [4.27559255e-19 1.45818261e-05 6.64412564e-05 1.22615371e-07\n",
      "  8.45307087e-10 1.08338826e-09 1.10175911e-06 2.06300442e-07\n",
      "  2.52044969e-03 3.92150582e-09]\n",
      " [4.78241557e-20 3.34577273e-07 1.41962085e-06 3.53744069e-09\n",
      "  3.23800390e-11 3.89229905e-11 3.25669313e-08 4.97529027e-09\n",
      "  3.51253056e-05 1.42722274e-10]\n",
      " [9.79808164e-18 6.97538658e-03 2.37724739e-02 2.15665107e-05\n",
      "  1.53246490e-07 1.84836776e-07 3.54555494e-04 5.28596598e-05\n",
      "  9.11712895e-01 6.01497654e-07]\n",
      " [4.98082553e-21 6.67426914e-09 2.61809065e-08 8.53375724e-11\n",
      "  1.04615188e-12 1.07293199e-12 9.16921770e-10 1.39209381e-10\n",
      "  6.05253423e-07 5.13471656e-12]]\n",
      "\n",
      "Output:[[3.55897188e-16 3.52147627e-16 2.89969655e-16 2.54630340e-16\n",
      "  3.11836300e-16 2.95727792e-16 3.29925797e-16 3.60422285e-16\n",
      "  3.26704974e-16 2.09688926e-16 2.66582193e-16 3.08104115e-16\n",
      "  2.76072889e-16 2.75745563e-16 2.91092151e-16 3.04153545e-16]\n",
      " [2.06444837e-02 2.03599843e-02 1.70294961e-02 1.50971270e-02\n",
      "  1.84402423e-02 1.70512158e-02 1.92027643e-02 2.12522990e-02\n",
      "  1.92581096e-02 1.17612432e-02 1.58623320e-02 1.82271639e-02\n",
      "  1.60994975e-02 1.63179879e-02 1.68105338e-02 1.76534991e-02]\n",
      " [2.62675421e-01 2.58965272e-01 2.16872077e-01 1.92440836e-01\n",
      "  2.34888743e-01 2.16909323e-01 2.44381615e-01 2.70623480e-01\n",
      "  2.45245235e-01 1.49333338e-01 2.02167436e-01 2.32249408e-01\n",
      "  2.04920637e-01 2.07850836e-01 2.13802704e-01 2.24595946e-01]\n",
      " [2.77950940e-04 2.74225069e-04 2.29012046e-04 2.02818218e-04\n",
      "  2.47814806e-04 2.29693274e-04 2.58454338e-04 2.85677130e-04\n",
      "  2.58872559e-04 1.58883908e-04 2.13045773e-04 2.44909780e-04\n",
      "  2.16651475e-04 2.19261207e-04 2.26430195e-04 2.37681124e-04]\n",
      " [3.83838222e-06 3.78814683e-06 3.16022975e-06 2.79635865e-06\n",
      "  3.41988800e-06 3.17206272e-06 3.56864989e-06 3.94382919e-06\n",
      "  3.57340142e-06 2.19705749e-06 2.93841057e-06 3.37848106e-06\n",
      "  2.99098962e-06 3.02627876e-06 3.12823992e-06 3.28275931e-06]\n",
      " [2.12679228e-07 2.09921414e-07 1.74992527e-07 1.54790974e-07\n",
      "  1.89220691e-07 1.75858295e-07 1.97686598e-07 2.18200541e-07\n",
      "  1.97727410e-07 1.22038314e-07 1.62556357e-07 1.86963092e-07\n",
      "  1.65678813e-07 1.67395678e-07 1.73347514e-07 1.81867562e-07]\n",
      " [1.35523052e-02 1.33622104e-02 1.11863118e-02 9.92353386e-03\n",
      "  1.21148645e-02 1.11917257e-02 1.26076960e-02 1.39592356e-02\n",
      "  1.26499146e-02 7.70921167e-03 1.04255169e-02 1.19776675e-02\n",
      "  1.05714335e-02 1.07204251e-02 1.10321790e-02 1.15879645e-02]\n",
      " [1.92206851e-04 1.89628822e-04 1.58396041e-04 1.40281918e-04\n",
      "  1.71483192e-04 1.58787472e-04 1.78744669e-04 1.97704720e-04\n",
      "  1.79140130e-04 1.09744080e-04 1.47416546e-04 1.69443149e-04\n",
      "  1.49831968e-04 1.51755952e-04 1.56585253e-04 1.64377656e-04]\n",
      " [4.90912790e+00 4.84007359e+00 4.05208401e+00 3.59506737e+00\n",
      "  4.38709964e+00 4.05477292e+00 4.56675306e+00 5.05434854e+00\n",
      "  4.58058204e+00 2.79394764e+00 3.77579795e+00 4.33809906e+00\n",
      "  3.82931977e+00 3.88152627e+00 3.99586773e+00 4.19722564e+00]\n",
      " [3.32766741e-06 3.28332813e-06 2.74119323e-06 2.42704309e-06\n",
      "  2.96711993e-06 2.74972208e-06 3.09433996e-06 3.42111594e-06\n",
      "  3.09999807e-06 1.90203227e-06 2.55012348e-06 2.93181293e-06\n",
      "  2.59359272e-06 2.62577319e-06 2.71124562e-06 2.84582566e-06]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    exp_scores=np.exp(z) # this is e^z where e is a math constant\n",
    "    probabilities=exp_scores/np.sum(exp_scores)\n",
    "    return probabilities\n",
    "\n",
    "vector_array=[2.0,1.0, 0.1]\n",
    "print(f\"Test softmax: {softmax(vector_array)}\")\n",
    "\n",
    "# simulate input embeddings\n",
    "X=np.random.rand(10,16) # 10 elements, each is a 16-dimensional vector\n",
    "\n",
    "# initialize weight matrices for queries, keys, and values\n",
    "W_Q=np.random.rand(16,16) # dimension chosen for example purposes\n",
    "W_K=np.random.rand(16,16)\n",
    "W_V=np.random.rand(16,16)\n",
    "\n",
    "# compute queries, keys and values\n",
    "Q=np.dot(X, W_Q)\n",
    "K=np.dot(X, W_K)\n",
    "V=np.dot(X, W_V)\n",
    "\n",
    "# Calulate dot products of Q and K Transpose\n",
    "dot_product=np.dot(Q, K.T)\n",
    "\n",
    "# Get the Scaled Dot Product\n",
    "d_k=K.shape[-1]\n",
    "scaled_dot_product =dot_product/(np.sqrt(d_k))\n",
    "\n",
    "# Apply Softmax to the Scaled Dot Product\n",
    "attention_weights=softmax(scaled_dot_product)\n",
    "print(f\"\\nSoftmax probabilities:{attention_weights}\")\n",
    "\n",
    "# Multiply by V\n",
    "output=np.dot(attention_weights, V)\n",
    "print(f\"\\nOutput:{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507bd26b",
   "metadata": {
    "papermill": {
     "duration": 0.003385,
     "end_time": "2024-03-20T00:35:13.003803",
     "exception": false,
     "start_time": "2024-03-20T00:35:13.000418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Acknowledge\n",
    "\n",
    "* https://medium.com/the-research-nest/explained-attention-mechanism-in-ai-e9bb6f0b0b4d"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.528467,
   "end_time": "2024-03-20T00:35:13.433284",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-20T00:35:09.904817",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
