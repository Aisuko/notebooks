{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71c869c",
   "metadata": {
    "papermill": {
     "duration": 0.006744,
     "end_time": "2024-07-25T05:41:51.365523",
     "exception": false,
     "start_time": "2024-07-25T05:41:51.358779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "We are going to build a Generativelt Pretrained Transformer(GPT), following the paper \"Attention is All You Need\".\n",
    "\n",
    "# Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07bc558",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:51.380225Z",
     "iopub.status.busy": "2024-07-25T05:41:51.379445Z",
     "iopub.status.idle": "2024-07-25T05:41:52.891638Z",
     "shell.execute_reply": "2024-07-25T05:41:52.890388Z"
    },
    "papermill": {
     "duration": 1.522419,
     "end_time": "2024-07-25T05:41:52.894309",
     "exception": false,
     "start_time": "2024-07-25T05:41:51.371890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-25 05:41:52--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: 'names.txt'\r\n",
      "\r\n",
      "names.txt           100%[===================>]   1.06M  5.14MB/s    in 0.2s    \r\n",
      "\r\n",
      "2024-07-25 05:41:52 (5.14 MB/s) - 'names.txt' saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -O names.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e02b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:52.909617Z",
     "iopub.status.busy": "2024-07-25T05:41:52.908775Z",
     "iopub.status.idle": "2024-07-25T05:41:52.916560Z",
     "shell.execute_reply": "2024-07-25T05:41:52.915440Z"
    },
    "papermill": {
     "duration": 0.017697,
     "end_time": "2024-07-25T05:41:52.918581",
     "exception": false,
     "start_time": "2024-07-25T05:41:52.900884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in chracters: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('/kaggle/working/names.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "print(\"length of dataset in chracters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b193ad",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:52.933607Z",
     "iopub.status.busy": "2024-07-25T05:41:52.932807Z",
     "iopub.status.idle": "2024-07-25T05:41:52.938483Z",
     "shell.execute_reply": "2024-07-25T05:41:52.937416Z"
    },
    "papermill": {
     "duration": 0.015882,
     "end_time": "2024-07-25T05:41:52.940964",
     "exception": false,
     "start_time": "2024-07-25T05:41:52.925082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45318040",
   "metadata": {
    "papermill": {
     "duration": 0.006402,
     "end_time": "2024-07-25T05:41:52.953986",
     "exception": false,
     "start_time": "2024-07-25T05:41:52.947584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Unique characters that occur in this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4227f4d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:52.968970Z",
     "iopub.status.busy": "2024-07-25T05:41:52.968098Z",
     "iopub.status.idle": "2024-07-25T05:41:52.992595Z",
     "shell.execute_reply": "2024-07-25T05:41:52.991441Z"
    },
    "papermill": {
     "duration": 0.034213,
     "end_time": "2024-07-25T05:41:52.994704",
     "exception": false,
     "start_time": "2024-07-25T05:41:52.960491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce0b4c",
   "metadata": {
    "papermill": {
     "duration": 0.006756,
     "end_time": "2024-07-25T05:41:53.008268",
     "exception": false,
     "start_time": "2024-07-25T05:41:53.001512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenize the text\n",
    "\n",
    "We want to convert the raw text as a string to some sequence of integers according to some vocabulary of possible elements.\n",
    "\n",
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8bd332d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:53.023080Z",
     "iopub.status.busy": "2024-07-25T05:41:53.022713Z",
     "iopub.status.idle": "2024-07-25T05:41:53.029631Z",
     "shell.execute_reply": "2024-07-25T05:41:53.028620Z"
    },
    "papermill": {
     "duration": 0.017252,
     "end_time": "2024-07-25T05:41:53.032092",
     "exception": false,
     "start_time": "2024-07-25T05:41:53.014840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create two lookup tables in character level\n",
    "stoi={ch:i for i, ch in enumerate(chars)}\n",
    "itos={i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "encode=lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integer\n",
    "decode=lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "    \n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a3ec15",
   "metadata": {
    "papermill": {
     "duration": 0.007039,
     "end_time": "2024-07-25T05:41:53.045969",
     "exception": false,
     "start_time": "2024-07-25T05:41:53.038930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Encode the dataset and store it into a torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b00f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:53.061241Z",
     "iopub.status.busy": "2024-07-25T05:41:53.060486Z",
     "iopub.status.idle": "2024-07-25T05:41:56.412261Z",
     "shell.execute_reply": "2024-07-25T05:41:56.410988Z"
    },
    "papermill": {
     "duration": 3.362186,
     "end_time": "2024-07-25T05:41:56.414848",
     "exception": false,
     "start_time": "2024-07-25T05:41:53.052662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data=torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e16c09",
   "metadata": {
    "papermill": {
     "duration": 0.007042,
     "end_time": "2024-07-25T05:41:56.429221",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.422179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a5d3f4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.444221Z",
     "iopub.status.busy": "2024-07-25T05:41:56.443708Z",
     "iopub.status.idle": "2024-07-25T05:41:56.448832Z",
     "shell.execute_reply": "2024-07-25T05:41:56.447932Z"
    },
    "papermill": {
     "duration": 0.014873,
     "end_time": "2024-07-25T05:41:56.450772",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.435899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n=int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49748340",
   "metadata": {
    "papermill": {
     "duration": 0.006474,
     "end_time": "2024-07-25T05:41:56.463903",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.457429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the chunk(context) size\n",
    "\n",
    "Feed the entire text into model all at once that would be computationally very expensive and prohibitive. So, we only work with chunks of the dataset, like `tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])`.\n",
    "\n",
    "When we plug it into a Transformer is we're going to actually simultaneously train it to make predict at every one of these positions. In the example below, these positions now in the chunk of nine characters there's actually eight individual examples packed in there. So there's the example that one 18 when in the context of 18 47 when in the context of 18 47 likely comes next. And in the context of 18 and 47 56 comes next in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df9a526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.478607Z",
     "iopub.status.busy": "2024-07-25T05:41:56.478083Z",
     "iopub.status.idle": "2024-07-25T05:41:56.486208Z",
     "shell.execute_reply": "2024-07-25T05:41:56.485276Z"
    },
    "papermill": {
     "duration": 0.017715,
     "end_time": "2024-07-25T05:41:56.488126",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.470411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformer will never receive more than block size inputs when it's predicting the \n",
    "# next chracter\n",
    "block_size=8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce8eb91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.503021Z",
     "iopub.status.busy": "2024-07-25T05:41:56.502682Z",
     "iopub.status.idle": "2024-07-25T05:41:56.510604Z",
     "shell.execute_reply": "2024-07-25T05:41:56.509619Z"
    },
    "papermill": {
     "duration": 0.017804,
     "end_time": "2024-07-25T05:41:56.512663",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.494859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) thr target: 47\n",
      "when input is tensor([18, 47]) thr target: 56\n",
      "when input is tensor([18, 47, 56]) thr target: 57\n",
      "when input is tensor([18, 47, 56, 57]) thr target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) thr target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) thr target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) thr target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) thr target: 58\n"
     ]
    }
   ],
   "source": [
    "x=train_data[:block_size]\n",
    "y=train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context=x[:t+1]\n",
    "    target=y[t]\n",
    "    print(f\"when input is {context} thr target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838357a3",
   "metadata": {
    "papermill": {
     "duration": 0.006555,
     "end_time": "2024-07-25T05:41:56.526145",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.519590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define batch dimension\n",
    "\n",
    "We're going to feed these chunks of text into a Transformer so we're going to have many batches of multiple chunks of text that are all like stacked up in a single tensor and that's just done for efficiency(We can keep the GPUs busy, because they are good at parallel processing of data.)\n",
    "\n",
    "We want to process multiple chunks all at the same time, but those chunks are procesed completely independently they don't talk to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "133e47e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.541594Z",
     "iopub.status.busy": "2024-07-25T05:41:56.540948Z",
     "iopub.status.idle": "2024-07-25T05:41:56.575309Z",
     "shell.execute_reply": "2024-07-25T05:41:56.574339Z"
    },
    "papermill": {
     "duration": 0.044736,
     "end_time": "2024-07-25T05:41:56.577585",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.532849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# Sampling random location in the dataset to pull chunks from\n",
    "# setting seed\n",
    "torch.manual_seed(1337)\n",
    "batch_size=4 # how many independent sequences will we proces in parallel?\n",
    "block_size=8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a smal;l batch of data of inputs x and targets y\n",
    "    data=train_data if split=='train' else val_data\n",
    "    ix=torch.randint(len(data)-block_size, (batch_size,)) # we are randomy generate batch_size(4) numbers between len(data)-block_size\n",
    "    x=torch.stack([data[i:i+block_size] for i in ix]) # stack one dimension tensor up at rows, they become a row in four by eight tensor\n",
    "    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb,yb=get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context=xb[b, :t+1]\n",
    "        target=yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86fbab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.593186Z",
     "iopub.status.busy": "2024-07-25T05:41:56.592835Z",
     "iopub.status.idle": "2024-07-25T05:41:56.598975Z",
     "shell.execute_reply": "2024-07-25T05:41:56.597899Z"
    },
    "papermill": {
     "duration": 0.016365,
     "end_time": "2024-07-25T05:41:56.601070",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.584705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4380ef",
   "metadata": {
    "papermill": {
     "duration": 0.007116,
     "end_time": "2024-07-25T05:41:56.615413",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.608297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define a biggram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d934aae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.631230Z",
     "iopub.status.busy": "2024-07-25T05:41:56.630858Z",
     "iopub.status.idle": "2024-07-25T05:41:56.724593Z",
     "shell.execute_reply": "2024-07-25T05:41:56.723429Z"
    },
    "papermill": {
     "duration": 0.104332,
     "end_time": "2024-07-25T05:41:56.726790",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.622458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the token from a lookup table\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits=self.token_embedding_table(idx) # (B,T,C)->(batch size=4, Time=8, C: channel(vocabulary size):65 )\n",
    "        \n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            #F.cross_entropy wants B,C,T sequence, so, we need to reshape our logits.\n",
    "            B,T,C=logits.shape\n",
    "            logits=logits.view(B*T, C) # We stretching out the array to a two-dimensional array\n",
    "\n",
    "\n",
    "            targets=targets.view(B*T)\n",
    "\n",
    "            # the negative log likelihood loss\n",
    "            loss=F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss # scores for the next character in the sequence\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Take (B,T) and predict (B,T+1)\n",
    "        \"\"\"\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss=self(idx) # end up going to the forward function\n",
    "            # focus only on the last time step\n",
    "            logits=logits[:,-1,:] # becomes (B,C) # pluck out the last dimension, it is the prediction for what comes next\n",
    "            # apply softmax to get probabilities\n",
    "            probs=F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next=torch.multinomial(probs, num_samples=1) # (B,1) # get one sample using torch.multinomial\n",
    "            # append sampled index to the running sequence\n",
    "            idx=torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m=BigramLanguageModel(vocab_size)\n",
    "logits, loss=m(xb,yb)\n",
    "print(logits.shape) # we got the scores for every 4 by 8 positions\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e19f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.743594Z",
     "iopub.status.busy": "2024-07-25T05:41:56.743201Z",
     "iopub.status.idle": "2024-07-25T05:41:56.749533Z",
     "shell.execute_reply": "2024-07-25T05:41:56.748419Z"
    },
    "papermill": {
     "duration": 0.01718,
     "end_time": "2024-07-25T05:41:56.751704",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.734524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 65)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d233d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.768361Z",
     "iopub.status.busy": "2024-07-25T05:41:56.767656Z",
     "iopub.status.idle": "2024-07-25T05:41:56.775429Z",
     "shell.execute_reply": "2024-07-25T05:41:56.774469Z"
    },
    "papermill": {
     "duration": 0.018235,
     "end_time": "2024-07-25T05:41:56.777388",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.759153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('token_embedding_table.weight',\n",
       "              tensor([[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
       "                      [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
       "                      [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
       "                      ...,\n",
       "                      [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
       "                      [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
       "                      [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c8d9a49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.794158Z",
     "iopub.status.busy": "2024-07-25T05:41:56.793805Z",
     "iopub.status.idle": "2024-07-25T05:41:56.799058Z",
     "shell.execute_reply": "2024-07-25T05:41:56.798064Z"
    },
    "papermill": {
     "duration": 0.016418,
     "end_time": "2024-07-25T05:41:56.801736",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.785318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 65)\n",
      ")\n",
      "1 -> Embedding(65, 65)\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.modules\n",
    "for idx, n in enumerate(m.modules()):\n",
    "    print(idx, '->', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081bd97d",
   "metadata": {
    "papermill": {
     "duration": 0.007421,
     "end_time": "2024-07-25T05:41:56.816823",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.809402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8077214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:56.833805Z",
     "iopub.status.busy": "2024-07-25T05:41:56.833034Z",
     "iopub.status.idle": "2024-07-25T05:41:58.133675Z",
     "shell.execute_reply": "2024-07-25T05:41:58.132759Z"
    },
    "papermill": {
     "duration": 1.311684,
     "end_time": "2024-07-25T05:41:58.136095",
     "exception": false,
     "start_time": "2024-07-25T05:41:56.824411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# Adam-> the simplest possible optimizer, we can use SGD instead\n",
    "\n",
    "# Adam is typically good with the learning rate 3E negative four\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d8332",
   "metadata": {
    "papermill": {
     "duration": 0.007479,
     "end_time": "2024-07-25T05:41:58.152236",
     "exception": false,
     "start_time": "2024-07-25T05:41:58.144757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c4aeb5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:41:58.169446Z",
     "iopub.status.busy": "2024-07-25T05:41:58.168670Z",
     "iopub.status.idle": "2024-07-25T05:42:13.790228Z",
     "shell.execute_reply": "2024-07-25T05:42:13.789055Z"
    },
    "papermill": {
     "duration": 15.632914,
     "end_time": "2024-07-25T05:42:13.792706",
     "exception": false,
     "start_time": "2024-07-25T05:41:58.159792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.572469472885132\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss=m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # zeroing out all the gradients from the previous step\n",
    "    loss.backward() # getting the gradients for all the parameters\n",
    "    optimizer.step() # using these gradients update out parameters\n",
    "    \n",
    "#     print(loss.item())\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "072066eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T05:42:13.810599Z",
     "iopub.status.busy": "2024-07-25T05:42:13.809752Z",
     "iopub.status.idle": "2024-07-25T05:42:13.872442Z",
     "shell.execute_reply": "2024-07-25T05:42:13.871376Z"
    },
    "papermill": {
     "duration": 0.074036,
     "end_time": "2024-07-25T05:42:13.874830",
     "exception": false,
     "start_time": "2024-07-25T05:42:13.800794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercckehathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThineent.\n",
      "\n",
      "Lavinde.\n",
      "athave l.\n",
      "KEONGBUCHandspo be y,-hedarwnoddy scace, tridesar, wne'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers w dere! ABer wotouciullle's\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1172f4b1",
   "metadata": {
    "papermill": {
     "duration": 0.007841,
     "end_time": "2024-07-25T05:42:13.890607",
     "exception": false,
     "start_time": "2024-07-25T05:42:13.882766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "* https://youtu.be/kCc8FmEb1nY?si=6rdQ4iGCj-icyrtT\n",
    "* https://www.kaggle.com/code/aisuko/character-lm-without-framework"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.984599,
   "end_time": "2024-07-25T05:42:14.819505",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-25T05:41:48.834906",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
