{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will go through implement the different decoding strategies by using the Huggingface Transformers library.\n\n\n# Auto-regressive Language generation\n\nIt is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions:\n\n$$P(w_{1:T}|W_0)=\\prod_{t=1}^{T}P(w_{t}|w_{1:t-1},W_{0}), w_{1:0}=\\phi$$\n\nand $W_{0}$ being the intial context word sequence. The length T of the word sequnce is usually determined on-the-fly and corresponds to the timestep t=T the EOS token is generated from $P(w_{t}|w_{1}:t-1, W_{0})$. Let's see the currently most prominent decoding methods:\n\n* Greedy search\n* Beam search\n* Sampling","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"pip install transformers==4.38.2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://huggingface.co/blog/how-to-generate\n* https://huggingface.co/blog/ray-rag","metadata":{}}]}