{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will go through implement the different decoding strategies by using the Huggingface Transformers library.\n\n\n# Auto-regressive Language generation\n\nIt is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions:\n\n$$P(w_{1:T}|W_0)=\\prod_{t=1}^{T}P(w_{t}|w_{1:t-1},W_{0}), w_{1:0}=\\phi$$\n\nand $W_{0}$ being the intial context word sequence. The length T of the word sequnce is usually determined on-the-fly and corresponds to the timestep t=T the EOS token is generated from $P(w_{t}|w_{1}:t-1, W_{0})$. Let's see the currently most prominent decoding methods:\n\n* Greedy search\n* Beam search\n* Sampling","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.38.2","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:24:07.182953Z","iopub.execute_input":"2024-03-14T11:24:07.183631Z","iopub.status.idle":"2024-03-14T11:24:30.944164Z","shell.execute_reply.started":"2024-03-14T11:24:07.183595Z","shell.execute_reply":"2024-03-14T11:24:30.942931Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers==4.38.2\n  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.2) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.38.2) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.2) (2024.2.2)\nDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\nSuccessfully installed transformers-4.38.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport warnings\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic=True\n    # https://github.com/huggingface/transformers/issues/28731\n    torch.backends.cuda.enable_mem_efficient_sdp(False)\n    device='cuda'\nelse:\n    device='cpu'\n\nwarnings.filterwarnings('ignore')\n\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:24:30.946285Z","iopub.execute_input":"2024-03-14T11:24:30.946591Z","iopub.status.idle":"2024-03-14T11:24:34.276991Z","shell.execute_reply.started":"2024-03-14T11:24:30.946565Z","shell.execute_reply":"2024-03-14T11:24:34.276048Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:25:37.894457Z","iopub.execute_input":"2024-03-14T11:25:37.894907Z","iopub.status.idle":"2024-03-14T11:25:39.065437Z","shell.execute_reply.started":"2024-03-14T11:25:37.894874Z","shell.execute_reply":"2024-03-14T11:25:39.064471Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02787f42cc314746a91bc07141202598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68d15cae30ad45a7ac836b26600954b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827a18784a5c4ca087f61a184addb26d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2512f25ab44e3983acedd40717c55c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"071fae51e97b4afd8dba20210e96f7f9"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel=AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\nmodel.config","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:26:05.350015Z","iopub.execute_input":"2024-03-14T11:26:05.350928Z","iopub.status.idle":"2024-03-14T11:26:06.112140Z","shell.execute_reply.started":"2024-03-14T11:26:05.350890Z","shell.execute_reply":"2024-03-14T11:26:06.111262Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"811c390273b34d47a9cbebf42ac20a41"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"GPT2Config {\n  \"_name_or_path\": \"gpt2\",\n  \"activation_function\": \"gelu_new\",\n  \"architectures\": [\n    \"GPT2LMHeadModel\"\n  ],\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"initializer_range\": 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 1024,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 12,\n  \"n_positions\": 1024,\n  \"pad_token_id\": 50256,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"task_specific_params\": {\n    \"text-generation\": {\n      \"do_sample\": true,\n      \"max_length\": 50\n    }\n  },\n  \"transformers_version\": \"4.38.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 50257\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Greedy Search\n\nGreedy search is the simplest decoding method. It selectes the word with the highest probability as its next word: $w_{t}=argmax_{w}*P(w|w_{1:t-1})$ at each timestep t. The following sketch shows greedy search.\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/093/437/219/901/721/original/a65abc1406cb4c72.png)\n\nStarting from the word \"The\", the algorithm greedily chooses the next word of highest probability \"nice\" and so on, so that the final generated word sequence is (\"The\", \"nice\", \"woman\") having an overall probability of 0.5x0.4=0.2. In the following we will generate sequences using GPT2 on the context (\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\"). Let's see how gready search can be used in `transformers`:","metadata":{}},{"cell_type":"code","source":"# encode context the generation is conditioned on\nmodel_inputs=tokenizer(\"I enjoy walking with my cute dog\", return_tensors=\"pt\").to(device)\n\n# generate 40 new tokens\ngreedy_output=model.generate(**model_inputs, max_new_tokens=40)\n\nprint(\"Output:\\n\"+100*'-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:26:11.855602Z","iopub.execute_input":"2024-03-14T11:26:11.856570Z","iopub.status.idle":"2024-03-14T11:26:14.164020Z","shell.execute_reply.started":"2024-03-14T11:26:11.856534Z","shell.execute_reply":"2024-03-14T11:26:14.163043Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n\nI'm not sure\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The generated words following the context are reasonable, but the model quickly starts repeating itself! This is very common problem in language generation and seems to be even more so in greedy and beam search. More detail see [Vijayakumar., 2016](https://arxiv.org/abs/1610.02424) and [Shao et al. 2017](https://arxiv.org/abs/1610.02424).\n\nThe major drawback of greedy search though is that it misses high probability words hidden behind a low probability words as can be seen in out sketch above: The word \"has\" with its high conditional probability of 0.9 is hidden behind the word \"dog\", which has only the second-highest conditional probability, so that greedy search misses the word sequence \"The\", \"dog\", \"has\".","metadata":{}},{"cell_type":"markdown","source":"# Beam Search\n\nBeam search reduces the risk of missing hidden high probability word sequences by keeping the most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. For example, num_beams=2:\n\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/093/566/654/879/861/original/e9b86a2f188c635c.png)\n\nAt time step 1, besides the most likely hypothesis (\"The\", \"nice\"), beam search also keeps track of the second most likely one(\"The\", \"Dog\"). At time step 2, beam search finds that the word sequnece (\"The\", \"dog\",\"has\"), has with 0.36 a higher probability than (\"The\", \"nice\",\"woman\"), which has 0.2. Great, it has found the most likely word sequence in our toy example! Beam search will always find an output sequence with higher probability than greedy search, but is not guranted to find the most likely output.\n\n\n## Beam Search in Transformers\n\nHere we set `num_beams>1` and `early_stopping=True` so that generation is finished when all beam hypotheses reached the EOS token.","metadata":{}},{"cell_type":"code","source":"beam_output=model.generate(**model_inputs, max_new_tokens=40, num_beams=5, early_stopping=True)\n\nprint(\"Output:\\n\"+100*'-')\nprint(tokenizer.decode(beam_output[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:26:31.792102Z","iopub.execute_input":"2024-03-14T11:26:31.792911Z","iopub.status.idle":"2024-03-14T11:26:32.302881Z","shell.execute_reply.started":"2024-03-14T11:26:31.792879Z","shell.execute_reply":"2024-03-14T11:26:32.301696Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI'm not sure if I'll ever be able to walk with him again. I'm not sure\n","output_type":"stream"}]},{"cell_type":"markdown","source":"While the result is arguably more fluent, the output still includes repetitions of the same word sequences. One of the available remedies is to introduce n-grams(a.k.a word sequences of n words) penalties as introduced by [Paulus et al, 2017](https://arxiv.org/abs/1705.04304) and [Klein et al, 2017](https://arxiv.org/abs/1701.02810). The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0.","metadata":{}},{"cell_type":"code","source":"# set no-repeat_ngram_size to 2\nbeam_output=model.generate(**model_inputs,max_new_tokens=40,num_beams=5,no_repeat_ngram_size=2,early_stopping=True)\n\nprint(\"Output:\\n\"+100*'-')\nprint(tokenizer.decode(beam_output[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:26:35.945370Z","iopub.execute_input":"2024-03-14T11:26:35.945729Z","iopub.status.idle":"2024-03-14T11:26:36.512690Z","shell.execute_reply.started":"2024-03-14T11:26:35.945699Z","shell.execute_reply":"2024-03-14T11:26:36.511739Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that the repetition does not appear anymore. Nevertheless, n-gram penalties have to be used with care. An article generated about the city Melbourne should not use a 2-gram penalty or otherwise, the name of the city would only appear once in the whole text.\n\nAnother important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best. We can simply set the parameter `num_return_sequences` to the number of highest scoring beams that should be returned. Make sure though that `num_return_sequences <= num_beams`.","metadata":{}},{"cell_type":"code","source":"# set return_num_sequences>1\nbeam_outputs=model.generate(**model_inputs, max_new_tokens=40, num_beams=5, no_repeat_ngram_size=2, num_return_sequences=5, early_stopping=True)\n\nprint(\"Output:\\n\"+100*\"-\")\nfor i, beam_output in enumerate(beam_outputs):\n    print(\"{}:{}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T11:28:56.012898Z","iopub.execute_input":"2024-03-14T11:28:56.013766Z","iopub.status.idle":"2024-03-14T11:28:56.539865Z","shell.execute_reply.started":"2024-03-14T11:28:56.013731Z","shell.execute_reply":"2024-03-14T11:28:56.538912Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Output:\n----------------------------------------------------------------------------------------------------\n0:I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n1:I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n2:I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's a good idea to\n3:I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time to take a\n4:I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's a good idea.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see, the five beam hypotheses are only marginally different to each other-which should not be too surprising when using only 5 beams.\n\n\n# Beam Search is not Best Possible Option\n\nBeam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization - see [Murray et al. 2018](https://arxiv.org/abs/1808.10006) and [Yang et al, 2018](https://arxiv.org/abs/1808.09582). But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. diglog and story generation. We have seen that beam search heavily suffers from repetitive generation. This is especially hard to control with n-gram or other penalties in story generation since finding a good trade-off between inhibiting repetition and repeating cycles of identical n-grams requires a lot of finetuning. As argued in [Ari Holtzman et al(2019)](https://arxiv.org/abs/1904.09751), high quality human language does not follow a distribution of high probability next words. In other words, as human, we want generated text to surprise us and not to be boring/predictable. The authors show this nicely by plotting the probability, a model would give to human text vs. what beam search does.\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/096/804/301/777/915/small/a2ba7b70e0fa191c.png)\n\n\n# Sampling\n\nIn its most basic form, sampling means randomly picking the next word $w_{t}$  according to its conditional probability distribution:\n\n$$w_{t}~P(w|w_{1:t-1})$$\n\nTaking the example from above, the following graphic visualizes language generation when sampling.\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/096/815/799/840/566/original/0cf20071a21e631c.png)\n\nIt becomes obvious that languaeg generation using sampling is not deterministic anymore. The word \"car\" is sampled from teh conditioned probability distribution P(w|\"The\"), followed by sampling (\"drives\") from P(w|\"The\",\"car\").\n\nIn Huggingface Transformers, we set `do_sample=True` and deactivate Top-K sampling via `top_k=0`. In the following, we will fix the random seed for illustration purpose. Feel free to chaneg the `set_seed` argument to obtain different results, or to remove it for non-determinism.","metadata":{}},{"cell_type":"code","source":"# set seeed to reproduce results, Feel free to change the seed though to get different results\nfrom transformers import set_seed\n\nset_seed(42)\n\n# activate sampling and deactivate top_k by settting top_k sampling to 0\nsample_output=model.generate(\n    **model_inputs,\n    max_new_tokens=40,\n    do_sample=True,\n    top_k=0\n)\n\nprint(\"Output:\\n\"+100*'-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a closer look, it is not very coherent and doesn't sound like it was written by a human. That is the big problem when sampling word sequences: **The model often generate incoherent gibberish, cf Ari Holtzman et al(2019).**\n\n\n# Sampling with Temperature\n\nA trick is to make the distribution P(w|w_{1:t-1}) sharper(increasing the likelihood of high probability words and decreasing the likelihood of low probabiliry words) by lowering the so-called `temperature` of the softmax.\n\nAn illustration of applying temperature to our example from above could look as follows\n\n![](https://cdn.masto.host/sigmoidsocial/media_attachments/files/112/096/871/548/228/841/small/695491daf7135f33.png)\n\nThe conditional next word distribution of step t=1 becomes much sharper leaving almost no chance for word \"car\" to be selected. We can cool down the distribution in the library by setting `temperature=0.6`:","metadata":{}},{"cell_type":"code","source":"set_seed(42)\n\nsample_output=model.generate(\n    **model_inputs,\n    max_new_tokens=40,\n    do_sample=True,\n    top_k=0,\n    temperature=0.6\n)\n\nprint(\"Output:\\n\"+100*'-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are less weried n-grams and the output is a bit more coherent now! while applying temperature can make a distribution less random, in its limit, when setting `temperature` ->0, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.\n\n\n# Top-k Sampling\n\n[Fan et.al 2018](https://arxiv.org/pdf/1805.04833.pdf) introduced a simple, but very powerful sampling scheme, called **Top-K** sampling. In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. GPT-2 adopted this sampling scheme, which was one of the reasons for its success in story generation.","metadata":{}},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://huggingface.co/blog/how-to-generate\n* https://huggingface.co/blog/ray-rag","metadata":{}}]}