{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n We have several notebooks to introduce Transformer like:\n \n * [Encoder in Transformer](https://www.kaggle.com/code/aisuko/encoder-in-transformers-architecture)\n * [Decoder in Transformer](https://www.kaggle.com/code/aisuko/decoder-in-transformers-architecture)\n * [Multiple Head Attention](https://www.kaggle.com/code/aisuko/mask-multi-multi-head-attention)\n \n\n## Let's give a short review of these components.\n\n\n**Encoder**\n\nIt has a `Multi-Head Attention` mechanism and a fully connected `Feed-Forward network`. There are also residual connections around two sub-layers, plus layer normalization for the output of each sub-layer. All sub-layers in the model and the embedding layers produce outputs of dimension $d_{model}=512$.\n\n**Decoder**\n\nThe decoder follows a similar structure, but it inserts a third sub-layer taht performs multi-head attention over the output of the encoder block. There is also a modification of the self-attention sub-layer in the decoder block to avoid positions from attending to subsequent positions. This masking ensures that the predictions for position `i` depend solely on the known outputs at positions less than i.\n\nBoth the encoder and decoder blocks are repeated N times. In the original paper, it is N=6, and we will define a similar value in this notebook.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Input Embeddings\n\nThe `InputEmbeddings` class below is responsible for converting the input text into numerical vectors of `d_model` dimensions. To prevent that our input embeddings become extremely small, we normalize them by multiplying them by the $\\sqrt{d_{model}}$","metadata":{}},{"cell_type":"code","source":"import math\nimport torch.nn as nn\n\nclass InputEmbeddings(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int):\n        super().__init__()\n        self.d_model=d_model # Dimension of vectors (512)\n        self.vocab_size=vocab_size # Size of the vocabulary\n        self.embedding=nn.Embedding(vocab_size, d_model)\n    \n    def forward(self, x):\n        return self.embedding(x)*math.sqrt(self.d_model) # normalizing the variance of the embeddings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Positional Encoding\n\nIn the original paper, the authors add the positional encodings to the input embeddings at the bottom of both the encoder and decoder block so the model can have some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two vectors can be summed and we can combine the semantic content from the word embeddings and positional information from the positional encodings.\n\nIn the `PositionalEncoding` class below, we will create a matrix of positional encodings `pe` with dimensions `(seq_len, d_model)`. We will start by filling it with 0s. We will then apply the sine function to even indices of the positional encoding matrix while the cosine function is applied to the odd ones.\n\n$$Even Indices(2i): PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n\n$$Odd Indices(2i+1): PE(pos, 2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n\nWe apply the sine and cosine functions because it allows the model to determine the position of a word based on the position of other word in the sequence, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. This happens due to the properties of sine and cosine functions, where a shift in the input results in a predictable change in the output.","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model:int, seq_len:int, dropout:float) -> None:\n        super().__init__()\n        self.d_model=d_model # Dimensionality of the model\n        self.seq_len=seq_len # Maximum sequence length\n        self.dropout=nn.Dropout(dropout) # dropout layer to prevent overfitting\n        \n        # creating a positional ecoding matrix of shape (seq_len, d_model) filled with zeros\n        pe=torch.zeros(seq_len, d_model)\n        \n        # creating a tensor representing positions (0 to seq_len -1)\n        position=torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # transforming `position` into a 2D tensor[seq_len,1]\n        \n        # creating te division term for the positional encoding formula\n        div_term=torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model))\n        \n        # apply sine to even indices in pe\n        pe[:,0::2]=torch.sin(position*div_term)\n        \n        # apply cosine to odd indices in pe\n        pe[:,1::2]=torch.cos(position*div_term)\n        \n        # adding an extra dimension at the beginning of pe matrix for batch handling\n        pe=pe.unsqueeze('pe', pe)\n        \n        # registering 'pe' as buffer, buffer is a tensor not considered as a model parameter\n        self.register_buffer('pe',pe)\n    \n    def forward(self, x):\n        # adding positional encoding to the input tensor X\n        x=x+(self.pe[:,:x.shape[1],:].requires_grad_(False))\n        return self.dropout(x) # dropout for regularization","metadata":{},"execution_count":null,"outputs":[]}]}