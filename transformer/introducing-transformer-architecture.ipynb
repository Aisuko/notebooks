{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/introducing-transformer-architecture?scriptVersionId=188300194\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ecaa41a0","metadata":{"papermill":{"duration":0.002794,"end_time":"2024-07-15T01:21:13.738267","exception":false,"start_time":"2024-07-15T01:21:13.735473","status":"completed"},"tags":[]},"source":["# Historical Context(Neural Network)\n","\n","* Before 2012\n","  * Nightmare\n","* 2012:ImageNet Classification with Deep Convolutional Neural Networks\n","  * Pattern was found-> Neural Networks->we don't need to worry about the compute and data\n","* 2014:Neural Nework expends to other areas\n","  * Sequence to Sequence Learning with Neural Networks-> (encoder-decoder architecture, attention mechanism, RNN)\n","  * like machine translation\n","* Dec 2017 Attenion is all you need\n","  * Everything only Attention, delete all RNN components\n","  * Positional encodings\n","  * Residual network (ReSNet) structure\n","  * Interspersing of Attention and MLP\n","  * LayerNorms\n","  * Multiple heads of attention in parallel\n","  * Great hyperparameters\n"," \n"," \n"," \n","# Attention = \"communication\" phase of tranformer\n","\n","**Self-attention** in Transformers\n","\n","$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","\n","In the communicate phase of the transformer\n","* Every head applies this, in parallel\n","* and then every layer, in series with different weights each time\n","\n","**Multiple-head attention**\n","\n","Self-attetnion applied multiple times in parallel. Heads is really copy-paste in parallel.\n","\n","**Cross-attention**\n","\n","It is different from Self-attention in piece and values come from."]},{"cell_type":"code","execution_count":1,"id":"b3f9c53c","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-15T01:21:13.745768Z","iopub.status.busy":"2024-07-15T01:21:13.745318Z","iopub.status.idle":"2024-07-15T01:21:13.765088Z","shell.execute_reply":"2024-07-15T01:21:13.763494Z"},"papermill":{"duration":0.027183,"end_time":"2024-07-15T01:21:13.768099","exception":false,"start_time":"2024-07-15T01:21:13.740916","status":"completed"},"tags":[]},"outputs":[],"source":["# Attention basically\n","import numpy as np\n","\n","class Node:\n","    def __init__(self):\n","        # the vector stored at this node\n","        self.data = np.random.randn(20)\n","        \n","        # weights governing how this node interacts with other nodes\n","        self.w_key=np.random.randn(20, 20)\n","        self.w_query=np.random.randn(20, 20)\n","        self.w_value=np.random.randn(20,20)\n","        \n","    def key(self):\n","        # What do I have\n","        return self.w_key@self.data\n","    \n","    def query(self):\n","        # What am I looking for?\n","        return self.w_query@ self.data\n","    \n","    def value(self):\n","        # What do I publicly reveal/broadcast to others? communicate\n","        return self.w_value @ self.data\n","    \n","class DIG:\n","    \"\"\"\n","    Mirrow message passing schema at the heart of transformer\n","    \"\"\"\n","    def __init__(self):\n","        # 10 nodes\n","        self.nodes=[Node() for _ in range(10)]\n","        randi=lambda: np.random.randint(len(self.nodes))\n","        # 40 edges\n","        self.edges=[[randi(), randi()] for _ in range(40)]\n","        \n","    def run(self):\n","        updates = []\n","        for i,n in enumerate(self.nodes):\n","            \n","            # what is this node looking for?\n","            q=n.query()\n","            \n","            # find all edges that are input to this node\n","            inputs=[self.nodes[ifrom] for (ifrom,ito) in self.edges if ito==i]\n","            if len(inputs)==0:\n","                continue # ignore\n","            \n","            # gather their keys, i.e, what they hold\n","            keys= [m.key() for m in inputs]\n","            \n","            # calculate the compatibilities\n","            scores=[k.dot(q) for k in keys]\n","            \n","            # softmax them so they sum to 1- normalization\n","            scores=np.exp(scores)\n","            \n","            scores=scores/np.sum(scores)\n","            # gather the appropriate values with a weighted sum\n","            values=[m.value() for m in inputs]\n","            update=sum([s*v for s,v in zip(scores, values)])\n","            updates.append(update)\n","            \n","        for n,u in zip(self.nodes, updates):\n","            n.data=n.data+u # residual connection"]},{"cell_type":"markdown","id":"ada2aaa0","metadata":{"papermill":{"duration":0.002274,"end_time":"2024-07-15T01:21:13.773051","exception":false,"start_time":"2024-07-15T01:21:13.770777","status":"completed"},"tags":[]},"source":["In Encoder-Decoder models,\n","* Encoder is a fully-connected cluster, which means all tokens see each other, and know the relationship between each other. In order words, they know the specific tokens can be an answer for other token.\n","* Decoder is fully connected to Encoder positions, and left-to-right connected in decoder positions. Decoder get tokens from the top of encoders, and also the previous output tokens from itself."]},{"cell_type":"markdown","id":"62a766ba","metadata":{"papermill":{"duration":0.002064,"end_time":"2024-07-15T01:21:13.777432","exception":false,"start_time":"2024-07-15T01:21:13.775368","status":"completed"},"tags":[]},"source":["# Decoder only Neural Network\n","\n","Only as a demonstrate for the deocder only architecture."]},{"cell_type":"code","execution_count":2,"id":"e26684c8","metadata":{"execution":{"iopub.execute_input":"2024-07-15T01:21:13.784108Z","iopub.status.busy":"2024-07-15T01:21:13.783645Z","iopub.status.idle":"2024-07-15T01:21:17.672459Z","shell.execute_reply":"2024-07-15T01:21:17.671218Z"},"papermill":{"duration":3.895454,"end_time":"2024-07-15T01:21:17.675169","exception":false,"start_time":"2024-07-15T01:21:13.779715","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","\n","class MLP(nn.Module):\n","    \"\"\"\n","    m\n","    Multi-layer perceptron. It is indivisual processing on each node,\n","    transforming the feature representation at that node.\n","    \n","    2-layer neural network\n","    \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc=nn.Linear(config.n_embd, 4*config.n_embd)\n","        self.c_proj=nn.Linear(4*config.n_embd, config.n_embd)\n","        self.dropout=nn.Dropout(config.dropout)\n","        \n","    def forward(self, x):\n","        x=self.c_fc(x)\n","        x=new_gelu(x) # nonlinearity\n","        x=self.c_proj(x)\n","        x=self.dropout(x)\n","        return x\n","    \n","class CausualSelfAttention(nn.Module):\n","    \"\"\"\n","    How we mask the connectivity in the graph\n","    \n","    Note: We can't obtain any information from the future when we are predicting the tokens\n","    \n","    For example, like what we did in https://www.kaggle.com/code/aisuko/single-character-nn-prediction-with-pytorch\n","    \n","    We use 14th token to predict 15th token. 14th token as input of NN\n","    \n","    \n","    \"\"\"\n","    def __init__(self, config):\n","        assert config.n_embd % config.n_head ==0\n","        # key, query value projections for all heads, but in a batch\n","        self.c_attn=nn.Linear(config.n_embd, 3*config.n_embd)\n","        # output projection\n","        self.c_proj=nn.Linear(config.n_embd, config.n_embd)\n","        # regularization\n","        self.attn_dropout=nn.Dropout(config.dropout)\n","        self.resid_dropout=nn.Dropout(config.dropout)\n","        # causal mask to ensure that attention is only applied to the left in the input sequence\n","        self.register_buffer(\"bias\", torch.trill(torch.ones(config.block_size, config.block_size)).view(1,1, config.block_size, config.block_size))\n","        self.n_head=config.n_head\n","        self.n_embd=config.n_embd\n","        \n","    def forward(self,x):\n","        B,T,C=x.size() # batch size, sequence length, embedding dimensionality(n_embd)\n","        \n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        q,k,v=self.c_attn(x).split(self.n_embd, dim=2)\n","        k=k.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)-> batch dimension, the head dimension, time dimension, we have features at them\n","        q=q.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n","        v=v.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B, nh, T, hs)\n","        \n","        # causual self-attention; self-attention: (B, bh, T, hs)x(B, nh, hs, T) -> (B, nh, T, T)\n","        att=(q@k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n","        \n","        # Note: remove this sinle line that we will get encoder\n","        # if we don't masked the attetnion, all the note will communicate with each other.\n","        # And the information flows between all the nodes\n","        \n","        # masks the attention-> clamping the attention between the nodes that are not supposed to communicate to be negative infinity\n","        att=att.masked_fill(self.bias[:,:,:T,:T]==0, float('-inf'))\n","        \n","        # we use softmax make negative infitinity attetnion be zero\n","        att=F.softmax(att, dim=-1)\n","        \n","        att=self.attn_dropout(att) # optional dropout\n","        \n","        # gathering of the information according to the affinities we calcualted\n","        # It is a weighted sum of the values of all these nodes.\n","        y=att@v # (B, bh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n","        \n","        # \n","        y=y.transpose(1,2).contiguous().view(B,T,C) # re-assemnle all head outputs side by side\n","        \n","        # a linear projection back to the residual pathway4\n","        y=self.resid_dropout(self.c_proj(y))\n","        return y\n","        \n","\n","class Block(nn.Module):\n","    \"\"\"\n","    Decoder only module\n","    * We don't have encoder\n","    * We don't have corss(multi-head) attention\n","    \"\"\"\n","    \n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1=nn.LayerNorm(config.n_embd)\n","        self.attn=CausualSelfAttention(config)\n","        self.ln_2=nn.LayerNorm(config.n_embd)\n","        self.mlp=MLP(config)\n","        \n","    def forward(self, x):\n","        x=x+self.attn(self.ln_1(x)) # communicate phase(with each other) ->Masked-Multi-Head Attention\n","        # if we want multi-head attetnion, add new line in here\n","        x=x+self.mlp(self.ln_2(x)) # compute phase -> Feed Forward\n","        return x\n","\n","class GPT(nn.Module):\n","    \n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.vocab_size is not None\n","        assert config.block_size is not None\n","        self.config=config\n","        \n","        self.transformer=nn.ModuleDict(\n","            dict(\n","                wte=nn.Embedding(config.vocab_size, config.n_embd),\n","                wpe=nn.Embedding(config.block_size, config.n_embd),\n","                drop=nn.Dropout(config.dropout),\n","                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","                ln_f=nn.LayerNorm(config.n_embd),\n","            )\n","        )\n","        self.lm_head=nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","    \n","    def forward(self, idx, targets=None):\n","        device=idx.device\n","        b,t=idx.size()\n","        assert t<=self.config.block_size, f\"Can't forward sequence of length {t}, block size is only\"\n","        pos=torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape(1,t)\n","        \n","        # forward the GPT model itself\n","        tok_emb=self.transformer.wte(idx) # token embeddings of shape (b,t, n_embd)\n","        pos_emb=self.transformer.wpe(pos) # position embeddings of shape (1,t,n_embd)\n","        x=self.transformer.drop(tok_emb+pos_emb)\n","        for block in self.transformer.h: \n","            x=block(x)\n","        x=self.transformer.ln_f(x) # layerform\n","        logits=self.lm_head(x) # linear layer\n","        \n","        # if we are given some desired targets also calcualte the loss\n","        loss=None\n","        if targets is not None:\n","            loss=F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","        return logits, loss\n","    "]},{"cell_type":"markdown","id":"d242ccb8","metadata":{"papermill":{"duration":0.00246,"end_time":"2024-07-15T01:21:17.680751","exception":false,"start_time":"2024-07-15T01:21:17.678291","status":"completed"},"tags":[]},"source":["# Credit\n","\n","* https://youtu.be/XfpMkf4rD6E?si=Gak8OBHLUMzHVXC6"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":7.814449,"end_time":"2024-07-15T01:21:18.505284","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-15T01:21:10.690835","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}