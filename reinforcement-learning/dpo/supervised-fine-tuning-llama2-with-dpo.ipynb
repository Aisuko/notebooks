{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/supervised-fine-tuning-llama2-with-dpo?scriptVersionId=165210041\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nRLHF(Reinforcement Learning from Human Feedback) can help to ensure that the language model's output are aligned with human expectations such as chattiness or safety features. However,it also brings some of the complexity of RL into NLP. Some like, we need to build a good reward function, train the model to estimate the values of a state, and at the same time be careful not to strive too far from the original model and produce gibberish instead of sensibel text. Such a process is quite involved requiring a number of complex moving parts where it is not always easy to get things right.\n\nIn [Direct Preference Optimization paper](https://arxiv.org/abs/2305.18290) peoposes to cast the RL-based objective used by existing methods to an objective which can be directly optimized via a simple binary cross-entropy loss which simplifies this process of refining LLMs greatly.\n\nWe are going to use [DPO](https://www.kaggle.com/code/aisuko/fine-tune-llm-with-direct-preference-optimization#Direct-Preference-Optimization) fine-tune Llama v2 7B on the one of the [**Preference datasets**](https://www.kaggle.com/code/aisuko/fine-tune-llm-with-direct-preference-optimization#Preparing-datasets). Here we use [stack_exchaneg preference](https://huggingface.co/datasets/lvwerra/stack-exchange-paired) dataset.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.36.2\n!pip install accelerate==0.25.0\n!pip install datasets==2.15.0\n!pip install peft==0.7.1\n!pip install bitsandbytes==0.41.3\n!pip install trl==0.7.7\n!pip install tqdm==4.66.1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Llama2-with-stack-exchange\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune model distilbert base uncased\"\nos.environ[\"WANDB_NAME\"] = \"ft-Llama2-with-stack-exchange-paired\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_name=\"meta-llama/Llama-2-7b-hf\"\ntokenizer=AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\ntokenizer.pad_token=tokenizer.eos_token\ntokenizer.padding_side=\"right\" # this fixed the weird overflow issue with fp16 training","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Datasets","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n\ndef prepare_sample_text(example):\n    text=f\"Question:{example['question']}\\n\\nAnswer:{example['response_j']}\"\n    return text\n\ndef chars_token_ratio(dataset, tokenizer, nb_examples=400):\n    \"\"\"\n    Estimate the average number of characters per token in the dataset.\n    \"\"\"\n    total_characters, total_tokens=0,0\n    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n        text=prepare_sample_text(example)\n        total_characters+=len(text)\n        if tokenizer.is_fast:\n            total_tokens+=len(tokenizer(text).tokens())\n        else:\n            total_tokens+=len(tokenizer.tokenize(text))\n    return total_characters/total_tokens\n\n\nstreaming=True\n\ndataset=load_dataset(\n    path=\"lvwerra/stack-exchange-paired\",\n    data_dir=\"data/finetune\", # the subset to use\n    split=\"train\",\n    num_proc=4,\n    streaming=streaming,\n)\n\ndataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if streaming:\n    valid_data=dataset.take(4000)\n    train_data=dataset.skip(4000)\n    train_data=train_data.shuffle(buffer_size=5000, seed=None)\nelse:\n    dataset=dataset.train_test_split(test_size=0.005, seed=None)\n    train_data=dataset[\"train\"]\n    valid_data=dataset[\"test\"]\n\nchars_per_token=chars_token_ratio(train_data, tokenizer)\nchars_per_token","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl.trainer import ConstantLengthDataset\n\ntrain_dataset=ConstantLengthDataset(\n    tokenizer,\n    train_data,\n    formatting_func=prepare_sample_text,\n    infinite=True,\n    seq_length=1024,\n    chars_per_token=chars_per_token,\n)\n\nvalid_dataset=ConstantLengthDataset(\n    tokenizer,\n    valid_data,\n    formatting_func=prepare_sample_text,\n    infinite=False,\n    seq_length=1024,\n    chars_per_token=chars_per_token,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quantize the model in FP4\n\nReducing the GPU memory usage in loading models and inference processes.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\n\n\nbnb_config=BitsAndBytesConfig(\n    # It is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from bitsandbytes.\n    load_in_4bit=True,\n    # This sets the quantization data type in the bnb.nn.Linear4Bit layers.\n    bnb_4bit_quant_type=\"nf4\",\n    # Support nested quantization\n    bnb_4bit_use_double_quant=True,\n    # This sets the computational type which might be different than the input time\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model=AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=False,\n)\n\nbase_model.config.use_cache=False\nprint(base_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft LoraConfig, TaskType\n\npeft_config=LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\",\"v_proj\"],\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\npeft_confg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\n\ntraining_args=TrainingArguments(\n    output_dir=\"./sft\",\n    max_steps=100,\n    logging_steps=10,\n    save_steps=10,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    group_by_length=False,\n    learning_rate=1e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=50,\n    weight_decay=0.05,\n    optim=\"paged_adamw_32bit\",\n    fp16=True,\n    remove_unused_columns=False,\n    run_name=os.getenv(\"WANDB_NAME\"),\n    report_to=\"wandb\"\n)\n\nsft_trainer=SFTTrainer(\n    model=base_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    packing=True,\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_args,\n)\n\nsft_trainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the merge model","metadata":{}},{"cell_type":"code","source":"kwargs={\n    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n    'finetuned_from': 'meta-llama/Llama-2-7b-hf',\n#     'tasks': '',\n#     'dataset_tags':'',\n    'dataset':'lvwerra/stack-exchange-paired'\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\nsft_trainer.push_to_hub(**kwargs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndel sft_trainer, base_model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=AutoPeftModelForcausalLM.from_pretrained(\"./sft/final_checkpoint\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel=model.merge_and_unload()\n\nmodel.save_pretrained(\"./sft/final_merged_checkpoint\", safe_serialization=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Direct Preference Optimization","metadata":{}},{"cell_type":"code","source":"def return_prompt_and_responses(samples)-> Dict[str,str]:\n    return {\n        \"prompt\":[\n            \"Question:\"+question+\"\\n\\nAnswer:\" for question in samples[\"question\"]\n        ],\n        \"chosen\": samples[\"response_j\"],\n        \"rejected\": samples[\"response_k\"],\n    }\n\n\ndef get_stack_exchange_paired(data_dir=\"data/rl\",sanity_check=False,cache_dir=None,num_proc=24):\n    dataset=load_dataset(\n        \"lvwerra/stack-exchange-paired\",\n        split=\"train\",\n        data_dir=\"data/rl\",\n        cache_dir=cache_dir,\n    )\n    original_columns=dataset.column_names\n    \n    if sanity_check:\n        dataset=dataset.select(range(min(len(dataset), 1000)))\n    \n    return dataset.map(\n        return_prompt_and_responses,\n        batched=True,\n        num_proc=num_proc,\n        remove_columns=original_columns,\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check [Direct Preference Optimization](https://www.kaggle.com/code/aisuko/fine-tuning-mistral-7b-with-dpo?scriptVersionId=158111896#Direct-Preference-Optimization) to get more information about model and reference model","metadata":{}},{"cell_type":"code","source":"model=AutoModelForCausalLM.from_pretrained(\n    \"./sft/final_merged_checkpoint\",\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n)\n\nmodel.config.use_cache=False\n\nmodel_ref=AutoModelForCausalLM.from_pretrained(\n    \"./sft/final_merged_checkpoint\",\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_dpo=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ntokenizer_dpo.pad_token=tokenizer.eos_token","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load the Stack-exchange paired dataset\ntrain_dataset=get_stack_exchange_paired(\n    data_dir=\"data/rl\",\n    sanity_check=False\n)\n\ntrain_dataset=train_dataset.filter(\n    lambda x: len(x[\"prompt\"])+len(x[\"chosen\"])<=1024 and len(x[\"prompt\"])+len(x[\"rejected\"])<=1024\n)\n\neval_dataset=get_stack_exchange_paired(data_dir=\"data/evaluation\", sanity_check=True)\neval_dataset=eval_dataset.filter(\n    lambda x: len(x[\"prompt\"])+len(x[\"chosen\"])<=1024 and len(x[\"prompt\"])+len(x[\"rejected\"])<=1024\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize training arguments","metadata":{}},{"cell_type":"code","source":"training_args=TrainingArguments(\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=1,\n    max_steps=1000,\n    logging_steps=10,\n    save_steps=100,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    learning_rate=5e-4,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    report_to=\"wandb\",\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=100,\n    optim=\"paged_adamw_32bit\",\n    bf16=True,\n    remove_unused_columns=False,\n    run_name=os.getenv(\"WANDB_NAME\"),\n)\n\npeft_config=LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=['q_proj','v_proj','k_proj','out_proj','fc_in','fc_out','wte',],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\ndpo_trainer=DPOTrainer(\n    model,\n    model_ref,\n    args=training_args,\n    beta=0.1,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    peft_config=peft_config,\n    max_prompt_length=512,\n    max_length=1024,\n)\n\ndpo_trainer.train()\ndpo_trainer.save_model(os.getenv(\"WANDB_NAME\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\ndpo_trainer.model.save_pretrained(os.getenv(\"WANDB_NAME\")+\"/final_checkpoint\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference list\n\n* https://huggingface.co/blog/dpo-trl","metadata":{}}]}