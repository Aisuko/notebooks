{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/multiple-gpus-ft-llama3-1-with-fsdp-and-qlora?scriptVersionId=193245457\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nLet's fine-tune LLms using two GPUs and many of CPUs by using FSDP and QLoRA. Thanks for article [Multi_GPu Fine-tuning for Llama 3.1 70B with FSDP and QLoRA](https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6).\n\n\n## Fully Shared Data Parallel(FSDP)\n\nIt split everything(model itsefl, gradients,the activations, and the optimizer states) over all the GPUs adn offload some of it yo yhe CPU RAM if we don't have enough GPU memory. In other words, FSDP distrubutes optimizer sattes, gradients, and parameters across multiple devices(GPUs and CPUs).\n\nDuring yhe forward pass, teach FADP unit gathers the necessary weights shared from other deveices to form the complete set of weights, performs the computation, and then discards the non-local shards.\n\nAfter computing the loss, during the backward pass, each FSDP unit again gathers the complete set of weights and performs computations to determine local gradients, which are then averaged.\n\nThese averaged gradients are redistributed across the devices thorugh a reduce-scatter operation. After this, each device updates its own shard of the parameters.\n\nHere are are going to user Hugging Face's Accelerate.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -U -q transformers==4.39.3\n!pip install -U -q accelerate==0.28.0\n!pip install -U -q datasets==2.18.0\n!pip install -U -q peft==0.10.0\n!pip install -U -q bitsandbytes==0.43.1\n!pip install -U -q trl==0.8.6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Llama3.1 with SFT on openassistant-guanaco\"\nos.environ[\"WANDB_NAME\"] = \"ft-sft-Llama3-1-on-openassistant-guanaco\"\nos.environ[\"MODEL_NAME\"] = \"meta-llama/Meta-Llama-3.1-70B\"\nos.environ[\"TOKENIZER_NAME\"] = \"meta-llama/Meta-Llama-3.1-70B\"\nos.environ[\"DATASET\"] = \"timdettmers/openassistant-guanaco\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch, multiprocessing\nfrom datasets import load_dataset\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    set_seed\n)\nfrom trl import SFTTrainer, SFTConfig\nfrom peft.utils.other import fsdp_auto_wrap_policy\nfrom accelerate import Accelerator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accelerator = Accelerator()\nset_seed(1234)\n#use bf16 and FlashAttention if supported\nif torch.cuda.is_bf16_supported():\n    os.system('pip install flash_attn')\n    compute_dtype = torch.bfloat16\n    attn_implementation = 'flash_attention_2'\nelse:\n    compute_dtype = torch.float16\n    attn_implementation = 'sdpa'\nmodel_name = \"meta-llama/Meta-Llama-3.1-70B\"\n#Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\ntokenizer.pad_token = \"<|finetune_right_pad_id|>\"\ntokenizer.pad_token_id = 128004\ntokenizer.padding_side = 'right'\nds = load_dataset(\"timdettmers/openassistant-guanaco\")\n#Add the EOS token\ndef process(row):\n    row[\"text\"] = row[\"text\"]+\"<|end_of_text|>\"\n    return row\nds = ds.map(\n    process,\n    num_proc= multiprocessing.cpu_count(),\n    load_from_cache_file=False,\n)\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_storage=compute_dtype,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n          model_name, quantization_config=bnb_config, torch_dtype=torch.bfloat16, attn_implementation=attn_implementation\n)\nfor name, param in model.named_parameters():\n    # freeze base model's layers\n    param.requires_grad = False\ndef make_inputs_require_grad(module, input, output):\n    output.requires_grad_(True)\nmodel.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\nmodel.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':True})\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.05,\n        r=16,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n)\n\ntraining_arguments = SFTConfig(\n        output_dir=os.getenv(\"WANDB_NAME\") ,\n        eval_strategy=\"steps\",\n        do_eval=True,\n        optim=\"adamw_torch\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=16,\n        per_device_eval_batch_size=1,\n        log_level=\"debug\",\n        logging_steps=10,\n        learning_rate=1e-4,\n        bf16 = True,\n        eval_steps=10,\n        max_steps=50,\n        warmup_ratio=0.1,\n        lr_scheduler_type=\"linear\",\n        dataset_text_field=\"text\",\n        max_seq_length=512,\n        report_to=\"tensorboard\",\n        run_name=os.getenv('WANDB_NAME')\n)\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=ds['train'],\n        eval_dataset=ds['test'],\n        peft_config=peft_config,\n        tokenizer=tokenizer,\n        args=training_arguments,\n)\nfsdp_plugin = trainer.accelerator.state.fsdp_plugin\nfsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\ntrainer.train()\nif trainer.is_fsdp_enabled:\n    trainer.accelerator.state.\n    fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.save_model(output_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs={\n    'model_name': os.getenv(\"WANDB_NAME\"),\n    'finetuned_from': os.getenv('MODEL_NAME'),\n#     'tasks': 'Text-Generation',\n#     'dataset_tags':'',\n    'dataset': os.getenv(\"DATASET\")\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledgements\n\n* https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6","metadata":{}}]}