{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/producing-adapter-with-vera?scriptVersionId=185168250\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nFine-tuning Vectors on top of Random Matrices(VeRA).VeRA has two matrices same to LoRA, but they are frozen, random and shread across layers. The trinable parameters are in two vectors d and b that are placed after A and b, respectively. d and b are not shared across layers. It means that VeRA uses random matrices in the context of parameter-efficient fine-tuning. Since VeRA only trains 2 vectors, VeRA has significantly fewer trainable parameters than LoRA.\n\nIt is implemented in Hugging Face PEFT. This implementation has two significant limitations(as of June 8th, 2024):\n* It doen't support VeRA over a quantized model, it can only target modules using `nn.Linear`\n* The targeted modules must have the same shape\n* VeRA produce a larger adapter than LoRA. This is because, PEFT also saves the random matrices in addition to the fine-tuned vectors. It guarantees the portability of the fine-tuned adapter to other hardware/software configurations.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -U -q transformers==4.39.3\n!pip install -U -q accelerate==0.28.0\n!pip install -U -q datasets==2.18.0\n!pip install -U -q peft==0.10.0\n!pip install -U -q bitsandbytes==0.43.1\n!pip install -U -q trl==0.8.6","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Llama 3 8B with vera\"\nos.environ[\"WANDB_NAME\"] = \"ft-Llama3-8b-vera\"\nos.environ[\"MODEL_NAME\"] = \"meta-llama/Meta-Llama-3-8B\"\nos.environ[\"DATASET\"] = \"timdettmers/openassistant-guanaco\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_bf16_supported():\n    compute_dtype = torch.bfloat16","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv(\"MODEL_NAME\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token=\"<|eot_id|>\"\ntokenizer.pad_token_id=128009\ntokenizer.padding_side=\"left\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# loading datesets\nds=load_dataset(os.getenv(\"DATASET\"))\nds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\n\n# add EOS token\ndef pre_process(x):\n    x[\"text\"]=x[\"test\"]+\"<|end_of_text|>\"\n    return x\n\nds=ds.map(pre_process, num_proc=multiprocessing.cpu_count(), load_from_cache_file=False)\nds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"), \n    device_map=\"auto\",\n    torch_dtype=compute_dtype\n    # attn_implementation\n)\nmodel","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.device","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params=0\n    all_params=0\n    for _, param in model.named_parameters():\n        all_params+=param.numel()\n        if param.requires_grad:\n            trainable_params+=param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params/all_params:.2f}\")\n\nprint_trainable_parameters(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom peft import VeraConfig\n\npeft_config=VeraConfig(\n    vera_dropout=0.05,\n    r=512,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"gate_proj\",\"up_proj\"]\n)\n\ntraining_arguments=SFTConfig(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    evaluation_strategy=\"steps\",\n    do_eval=False,\n    optim=\"paged_adamw_8bit\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=16,\n    per_device_eval_batch_size=2,\n    log_level=\"debug\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    learning_rate=1e-4,\n    fp16=True,\n    bf16=torch.cuda.is_bf16_supported(),\n    eval_steps=100,\n    num_train_epochs=3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n    report_to=\"wandb\",\n    run_name=os.getenv('WANDB_NAME'),\n    output_dir=os.getenv('WANDB_NAME')\n)\n\ntrainer=SFTrainer(\n    model=model,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"test\"],\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=\"512\",\n    tokenizer=tokenizer,\n    arges=training_arguments\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Credit\n* https://towardsdatascience.com/fine-tune-tiny-adapters-for-llama-3-with-vera-7c48f4391d84\n* https://arxiv.org/abs/2310.11454\n* https://www.kaggle.com/code/aisuko/fine-tune-llama3-with-orpo","metadata":{}}]}