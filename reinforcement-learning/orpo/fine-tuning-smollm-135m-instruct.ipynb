{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q transformers==4.39.3\n!pip install -U -q accelerate==0.28.0\n!pip install -U -q datasets==2.18.0\n# !pip install -U -q peft==0.10.0\n!pip install -U -q bitsandbytes==0.43.1\n!pip install -U -q trl==0.8.6","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T23:29:08.547357Z","iopub.execute_input":"2024-08-15T23:29:08.547835Z","iopub.status.idle":"2024-08-15T23:30:34.591335Z","shell.execute_reply.started":"2024-08-15T23:29:08.547793Z","shell.execute_reply":"2024-08-15T23:30:34.590150Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2024.2.0 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:30:34.593369Z","iopub.execute_input":"2024-08-15T23:30:34.593697Z","iopub.status.idle":"2024-08-15T23:30:34.599273Z","shell.execute_reply.started":"2024-08-15T23:30:34.593670Z","shell.execute_reply":"2024-08-15T23:30:34.598403Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning HuggingFace SmolLM-135M-Instruct with ultrafeedback\"\nos.environ[\"WANDB_NAME\"] = \"ft-smollm-135M-instruct\"\nos.environ[\"MODEL_NAME\"] = \"HuggingFaceTB/SmolLM-135M-Instruct\"\nos.environ[\"TOKENIZER_NAME\"] = \"HuggingFaceTB/SmolLM-135M-Instruct\"\nos.environ[\"DATASET\"] = \"HuggingFaceH4/ultrafeedback_binarized\"","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:30:34.600593Z","iopub.execute_input":"2024-08-15T23:30:34.600918Z","iopub.status.idle":"2024-08-15T23:30:35.598097Z","shell.execute_reply.started":"2024-08-15T23:30:34.600889Z","shell.execute_reply":"2024-08-15T23:30:35.597114Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nds=load_dataset(os.getenv(\"DATASET\"), split=[\"train_prefs\",\"test_prefs\"])\nds","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:30:35.600797Z","iopub.execute_input":"2024-08-15T23:30:35.601161Z","iopub.status.idle":"2024-08-15T23:30:53.302062Z","shell.execute_reply.started":"2024-08-15T23:30:35.601126Z","shell.execute_reply":"2024-08-15T23:30:53.301011Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c651f7976546fb99def2a8446776f5"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 226M/226M [00:01<00:00, 190MB/s]  \nDownloading data: 100%|██████████| 226M/226M [00:01<00:00, 175MB/s]  \nDownloading data: 100%|██████████| 7.29M/7.29M [00:00<00:00, 28.9MB/s]\nDownloading data: 100%|██████████| 3.72M/3.72M [00:00<00:00, 16.3MB/s]\nDownloading data: 100%|██████████| 184M/184M [00:01<00:00, 176MB/s]  \nDownloading data: 100%|██████████| 3.02M/3.02M [00:00<00:00, 12.9MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train_prefs split:   0%|          | 0/61135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36bb292ff0074472b4d597dad5b58b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train_sft split:   0%|          | 0/61135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c6fd24fb294111ac92c04debdf2e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_prefs split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0050f015f55b4b79be8c186c59eca38a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_sft split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"740849e79dbc4e59bc47d79af638827c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train_gen split:   0%|          | 0/61135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"403c7e2dc5d34f07bb1b42585093677b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_gen split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"911ab7f616144b6ca49fcfcb4d2925b1"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"[Dataset({\n     features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n     num_rows: 61135\n }),\n Dataset({\n     features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n     num_rows: 2000\n })]"},"metadata":{}}]},{"cell_type":"code","source":"train_ds=ds[0].shuffle(seed=42).select(range(3000))\neval_ds=ds[1].shuffle(seed=42).select(range(1000))\n\nprint(train_ds)\nprint(eval_ds)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:30:53.303381Z","iopub.execute_input":"2024-08-15T23:30:53.303952Z","iopub.status.idle":"2024-08-15T23:30:54.676712Z","shell.execute_reply.started":"2024-08-15T23:30:53.303917Z","shell.execute_reply":"2024-08-15T23:30:54.675789Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n    num_rows: 3000\n})\nDataset({\n    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n    num_rows: 1000\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv(\"TOKENIZER_NAME\"), add_eos_token=True)\n\n# TODO, need to invesgate the model architecture\ntokenizer.pad_token=tokenizer.eos_token\ntokenizer.padding_side=\"left\"","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:32:37.790163Z","iopub.execute_input":"2024-08-15T23:32:37.790626Z","iopub.status.idle":"2024-08-15T23:32:37.961379Z","shell.execute_reply.started":"2024-08-15T23:32:37.790594Z","shell.execute_reply":"2024-08-15T23:32:37.960582Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch, multiprocessing\n\ndef preprocess(x):\n    x[\"chosen\"]=tokenizer.apply_chat_template(x[\"chosen\"], tokenize=False)\n    x[\"rejected\"]=tokenizer.apply_chat_template(x[\"rejected\"], tokenize=False)\n    return x\n\ntrain_ds=train_ds.map(preprocess, num_proc=multiprocessing.cpu_count(), load_from_cache_file=False)\neval_ds=eval_ds.map(preprocess, num_proc=multiprocessing.cpu_count(), load_from_cache_file=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:32:39.846535Z","iopub.execute_input":"2024-08-15T23:32:39.846893Z","iopub.status.idle":"2024-08-15T23:32:41.223979Z","shell.execute_reply.started":"2024-08-15T23:32:39.846867Z","shell.execute_reply":"2024-08-15T23:32:41.222868Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b461b191df4e469e8e71c06b2f0e68d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fee7f8aec964e1d9c147a18eafe8a54"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    torch_dtype=torch.float16,\n#     device_map={\"\": 0},\n    device_map=\"cuda\",\n    trust_remote_code=True\n)\n\nmodel.gradient_checkpointing_enable()\nmodel.device","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:32:47.194811Z","iopub.execute_input":"2024-08-15T23:32:47.195203Z","iopub.status.idle":"2024-08-15T23:32:51.145639Z","shell.execute_reply.started":"2024-08-15T23:32:47.195158Z","shell.execute_reply":"2024-08-15T23:32:51.144749Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82ee74c32b1f46ce868d1bf85eac8f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb156583ec4848dcb340ae44934884f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6fa27d3bfaa4d2d8630808092bdc7e5"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"from trl import ORPOTrainer, ORPOConfig\n\norpo_config=ORPOConfig(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    evaluation_strategy=\"steps\",\n    do_eval=True,\n    optim=\"adamw_8bit\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=8,\n    log_level=\"debug\",\n    logging_steps=100,\n    learning_rate=3e-4,\n    eval_steps=100,\n    save_steps=100,\n    save_strategy=\"epoch\",\n    num_train_epochs=1,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n    beta=0.1, # beta is ORPO's lambda\n    max_length=1024,\n    report_to=\"wandb\",\n    run_name=os.getenv('WANDB_NAME')\n)\n\ntrainer = ORPOTrainer(\n        model=model,\n        train_dataset=train_ds,\n        eval_dataset=eval_ds,\n        args=orpo_config,\n        tokenizer=tokenizer,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T23:32:53.660183Z","iopub.execute_input":"2024-08-15T23:32:53.661053Z","iopub.status.idle":"2024-08-16T00:02:20.432293Z","shell.execute_reply.started":"2024-08-15T23:32:53.661016Z","shell.execute_reply":"2024-08-16T00:02:20.431185Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-08-15 23:32:55.828478: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 23:32:55.828595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 23:32:55.959839: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a662dd318ad4d04ae81e63ca6ef5f86"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3109 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fdacb39a3084f81b8ec991df6a60c58"}},"metadata":{}},{"name":"stderr","text":"You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\nCurrently training with a batch size of: 8\n***** Running training *****\n  Num examples = 3,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 2\n  Total optimization steps = 187\n  Number of trainable parameters = 134,515,008\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33murakiny\u001b[0m (\u001b[33mcausal_language_trainer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240815_233332-nhqlgcv2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20HuggingFace%20SmolLM-135M-Instruct%20with%20ultrafeedback/runs/nhqlgcv2' target=\"_blank\">ft-smollm-135M-instruct</a></strong> to <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20HuggingFace%20SmolLM-135M-Instruct%20with%20ultrafeedback' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20HuggingFace%20SmolLM-135M-Instruct%20with%20ultrafeedback' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20HuggingFace%20SmolLM-135M-Instruct%20with%20ultrafeedback</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20HuggingFace%20SmolLM-135M-Instruct%20with%20ultrafeedback/runs/nhqlgcv2' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20HuggingFace%20SmolLM-135M-Instruct%20with%20ultrafeedback/runs/nhqlgcv2</a>"},"metadata":{}},{"name":"stderr","text":"Could not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='187' max='187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [187/187 28:21, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/rejected</th>\n      <th>Logps/chosen</th>\n      <th>Logits/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Nll Loss</th>\n      <th>Log Odds Ratio</th>\n      <th>Log Odds Chosen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.317400</td>\n      <td>1.097916</td>\n      <td>134.384000</td>\n      <td>7.441000</td>\n      <td>0.930000</td>\n      <td>-0.136836</td>\n      <td>-0.136334</td>\n      <td>0.468000</td>\n      <td>-0.000502</td>\n      <td>-1.363338</td>\n      <td>-1.368362</td>\n      <td>33.970997</td>\n      <td>33.777000</td>\n      <td>1.022179</td>\n      <td>-0.757369</td>\n      <td>0.005010</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ft-smollm-135M-instruct/checkpoint-187\nConfiguration saved in ft-smollm-135M-instruct/checkpoint-187/config.json\nConfiguration saved in ft-smollm-135M-instruct/checkpoint-187/generation_config.json\nModel weights saved in ft-smollm-135M-instruct/checkpoint-187/model.safetensors\ntokenizer config file saved in ft-smollm-135M-instruct/checkpoint-187/tokenizer_config.json\nSpecial tokens file saved in ft-smollm-135M-instruct/checkpoint-187/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=187, training_loss=1.221203645920371, metrics={'train_runtime': 1729.8709, 'train_samples_per_second': 1.734, 'train_steps_per_second': 0.108, 'total_flos': 0.0, 'train_loss': 1.221203645920371, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"kwargs={\n    'model_name': os.getenv(\"WANDB_NAME\"),\n    'finetuned_from': os.getenv('MODEL_NAME'),\n#     'tasks': 'Text-Generation',\n#     'dataset_tags':'',\n    'dataset': os.getenv(\"DATASET\")\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T00:02:44.149673Z","iopub.execute_input":"2024-08-16T00:02:44.150515Z","iopub.status.idle":"2024-08-16T00:02:58.699973Z","shell.execute_reply.started":"2024-08-16T00:02:44.150480Z","shell.execute_reply":"2024-08-16T00:02:58.698794Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"tokenizer config file saved in ft-smollm-135M-instruct/tokenizer_config.json\nSpecial tokens file saved in ft-smollm-135M-instruct/special_tokens_map.json\nUploading the following files to aisuko/ft-smollm-135M-instruct: merges.txt,README.md,tokenizer_config.json,tokenizer.json,vocab.json,special_tokens_map.json\nSaving model checkpoint to ft-smollm-135M-instruct\nConfiguration saved in ft-smollm-135M-instruct/config.json\nConfiguration saved in ft-smollm-135M-instruct/generation_config.json\nModel weights saved in ft-smollm-135M-instruct/model.safetensors\ntokenizer config file saved in ft-smollm-135M-instruct/tokenizer_config.json\nSpecial tokens file saved in ft-smollm-135M-instruct/special_tokens_map.json\nDropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a264876a06594102a69b83a96e6fa112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510a510656714fe6ac528065f0b2e7b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"892467fc181c40718366b06af603557f"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/aisuko/ft-smollm-135M-instruct/commit/67b3b95d48e7d860816bf2e33054f4c39478c2dd', commit_message='End of training', commit_description='', oid='67b3b95d48e7d860816bf2e33054f4c39478c2dd', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"model=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"WANDB_NAME\"), \n    torch_dtype=torch.float16, \n    device_map=\"cuda\", \n    trust_remote_code=True\n)\n\nchat=[\n    [{\"role\":\"user\",\"content\":\"How is vanilla cultivated?\"}],\n    [{\"role\": \"user\", \"content\": \"How much money do I have if I have one dollar?\"}],\n    [{\"role\": \"user\", \"content\": \"Where is Berlin?\"}],\n    [{\"role\": \"user\", \"content\": \"Give me a list of 5 European countries.\"}],\n    [{\"role\": \"user\", \"content\": \"What is AI?\"}],\n    [{\"role\": \"user\", \"content\": \"What can you do right? Exactly?\"}]\n]\n\n\nfor c in chat:\n    p=tokenizer.apply_chat_template(c, tokenize=False)\n    inputs = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, do_sample=True, pad_token_id=tokenizer.eos_token_id, top_p=0.9, max_new_tokens=150)\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(result)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T00:03:41.804291Z","iopub.execute_input":"2024-08-16T00:03:41.805290Z","iopub.status.idle":"2024-08-16T00:04:11.048827Z","shell.execute_reply.started":"2024-08-16T00:03:41.805245Z","shell.execute_reply":"2024-08-16T00:04:11.047825Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"loading configuration file ft-smollm-135M-instruct/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"ft-smollm-135M-instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 576,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 1536,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 9,\n  \"num_hidden_layers\": 30,\n  \"num_key_value_heads\": 3,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.39.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nloading weights file ft-smollm-135M-instruct/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at ft-smollm-135M-instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file ft-smollm-135M-instruct/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\n","output_type":"stream"},{"name":"stdout","text":"user\nHow is vanilla cultivated?\nasedngassistauser\nHow is vanilla cultivated?\nuser\nHow much money do I have if I have one dollar?\n،حیده محتوٹی سلاطونی، آرنموسیم، سنیردوت از آیرا و این آسازی، در سطر عند سازیید مظالاجیم اوند برسان.\nتعالیاگم سازی، سنیرداد تنابق ایرخست مهاد زبان بیان\nuser\nWhere is Berlin?\nassistant\nBerlin is a city located in Germany's German-speaking region, bordering Poland and Switzerland. It is the capital of Germany and serves as the main city of the German-speakingspeaking region. Berlin is home to thehof derolkärchen, which is the largest city in Germany and consists of the German cityscape of Berlin. Here is a brief summary of Berlin's history and location:\n\nometown: Germany\n\nHistory:\n\n* Established as a city during the German Empire in 1871, Berlin was the first city to be built by GermanGERMANNY in 1872.\n* Since then, Berlin has served as the capital of Germany since the turn of the 2\nuser\nGive me a list of 5 European countries.\nassistant\nOne way to get a list of 5 European countries is to use the provided list. Here is the list:\n\n* Europe\n* Germany\n* Germany\n* Switzerland\n* France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*France\n*\nuser\nWhat is AI?\nassistant\nAI is a subfield of computer science that combines science, technology, and artificial intelligence. AI is the focus of a recent research project titled \"AI and AI Systems\" by a team at the University of California, Los Angeles (UCLA).\n\nThe project's goal is to create new artificial intelligence and robotic systems that are better at tasks that require cognitive intelligence, such as recognizing objects, understanding environmental context, and navigating environments. It also aims to be more AI-efficient and adaptive, meaning it can learn and adapt to new situations as needed.\n\nThe project is funded by grants from the United States National Science Foundation (NSF), the National Science Foundation of the United States, and the National Science Foundation. It is also\nuser\nWhat can you do right? Exactly?\nassistant\nTo provide an answer, you can use simple and effective approaches to answer this question. Here are some examples of how you can do this:\n\n1.\n","output_type":"stream"}]}]}