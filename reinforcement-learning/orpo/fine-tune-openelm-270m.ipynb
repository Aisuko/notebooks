{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nHere are some backgound information, see from [GitHub discussion](https://github.com/orgs/SkywardAI/discussions/14).\n\n\n# Architecture of OpenELM\n\n> See the architecture from [OpenELM-270M's config.json](https://huggingface.co/apple/OpenELM-270M/blob/main/config.json)\n\n* RoPE to encode positional information\n* Group-query attention(GQA) for more efficient inference\n* FlashAttention\n* RMSNorm\n\n\n# OPenELM's Training\n\n> Besides DPO, there are many others RLHF techniques,like ORPO. Here we use DPO.\n\nThey ran 350k training steps with a batch size of 4M tokens, yielding a total of 1.4T tokens used for pre-training. For reference, Llama2 was trained on 2T tokens, Gemma on 6T tokens, and Llama3 on 15T tokens.\n\nAnd also, Apple trained them on a cleaned version of Ultrafeedback using DPO(set to 0.1) and statistical rehection sampling method with these hyperparameters.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -U -q transformers==4.39.3\n!pip install -U -q accelerate==0.28.0\n!pip install -U -q datasets==2.18.0\n# !pip install -U -q peft==0.10.0\n!pip install -U -q bitsandbytes==0.43.1\n!pip install -U -q trl==0.8.6","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:27:02.401272Z","iopub.execute_input":"2024-06-24T06:27:02.401699Z","iopub.status.idle":"2024-06-24T06:28:26.522236Z","shell.execute_reply.started":"2024-06-24T06:27:02.401663Z","shell.execute_reply":"2024-06-24T06:28:26.521116Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2024.2.0 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:28:26.524554Z","iopub.execute_input":"2024-06-24T06:28:26.524936Z","iopub.status.idle":"2024-06-24T06:28:26.531380Z","shell.execute_reply.started":"2024-06-24T06:28:26.524898Z","shell.execute_reply":"2024-06-24T06:28:26.530544Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning openELM-270m with ultrafeedback\"\nos.environ[\"WANDB_NAME\"] = \"ft-openelm-270m-ultrafeedback\"\nos.environ[\"MODEL_NAME\"] = \"apple/OpenELM-270M\"\nos.environ[\"TOKENIZER_NAME\"] = \"meta-llama/Llama-2-7b-hf\"\nos.environ[\"DATASET\"] = \"HuggingFaceH4/ultrafeedback_binarized\"","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:28:26.532511Z","iopub.execute_input":"2024-06-24T06:28:26.532782Z","iopub.status.idle":"2024-06-24T06:28:27.734881Z","shell.execute_reply.started":"2024-06-24T06:28:26.532759Z","shell.execute_reply":"2024-06-24T06:28:27.733866Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(\n    os.getenv(\"TOKENIZER_NAME\"), \n    add_eos_token=True, \n    use_fast=True)\n\ntokenizer.pad_token=tokenizer.eos_token\ntokenizer.padding_side=\"left\"","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:28:27.737768Z","iopub.execute_input":"2024-06-24T06:28:27.738516Z","iopub.status.idle":"2024-06-24T06:28:35.162991Z","shell.execute_reply.started":"2024-06-24T06:28:27.738468Z","shell.execute_reply":"2024-06-24T06:28:35.162202Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7a9103bdbe54ad3a8b7deeda2631a50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e6b977d907d41948d1f1fe833e85aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e60c6f50d64996bc62ceed4adb96a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d1f921b0ba149ba990063868212634b"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds=load_dataset(os.getenv(\"DATASET\"), split=[\"train_prefs\",\"test_prefs\"])\nds","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:28:35.164137Z","iopub.execute_input":"2024-06-24T06:28:35.164620Z","iopub.status.idle":"2024-06-24T06:29:22.201145Z","shell.execute_reply.started":"2024-06-24T06:28:35.164587Z","shell.execute_reply":"2024-06-24T06:29:22.200199Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a419a97e48c4bceac6a01e3fd732832"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 226M/226M [00:09<00:00, 24.5MB/s] \nDownloading data: 100%|██████████| 226M/226M [00:08<00:00, 27.5MB/s] \nDownloading data: 100%|██████████| 7.29M/7.29M [00:01<00:00, 5.10MB/s]\nDownloading data: 100%|██████████| 3.72M/3.72M [00:00<00:00, 6.57MB/s]\nDownloading data: 100%|██████████| 184M/184M [00:07<00:00, 24.8MB/s] \nDownloading data: 100%|██████████| 3.02M/3.02M [00:01<00:00, 2.38MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train_prefs split:   0%|          | 0/61135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7639099957684148ba88e6531197e226"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train_sft split:   0%|          | 0/61135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38202800cf546068189ae26f015dd87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_prefs split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd19fd775e884f4db6dd5d1f11f8cb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_sft split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d7ea15aaec40f3829495c3c2ddfbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train_gen split:   0%|          | 0/61135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0820220be1c842afae19794f7a9170c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_gen split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2be5bf2ae624bee80696105741d4b2b"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[Dataset({\n     features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n     num_rows: 61135\n }),\n Dataset({\n     features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n     num_rows: 2000\n })]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fit low GPU","metadata":{}},{"cell_type":"code","source":"train_ds=ds[0].shuffle(seed=42).select(range(300))\neval_ds=ds[1].shuffle(seed=42).select(range(100))\n\nprint(train_ds)\nprint(eval_ds)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:29:22.202145Z","iopub.execute_input":"2024-06-24T06:29:22.202571Z","iopub.status.idle":"2024-06-24T06:29:22.255546Z","shell.execute_reply.started":"2024-06-24T06:29:22.202546Z","shell.execute_reply":"2024-06-24T06:29:22.254609Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n    num_rows: 300\n})\nDataset({\n    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n    num_rows: 100\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch, multiprocessing\n\ndef preprocess(x):\n    x[\"chosen\"]=tokenizer.apply_chat_template(x[\"chosen\"], tokenize=False)\n    x[\"rejected\"]=tokenizer.apply_chat_template(x[\"rejected\"], tokenize=False)\n    return x\n\ntrain_ds=train_ds.map(preprocess, num_proc=multiprocessing.cpu_count(), load_from_cache_file=False)\neval_ds=eval_ds.map(preprocess, num_proc=multiprocessing.cpu_count(), load_from_cache_file=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:29:22.256657Z","iopub.execute_input":"2024-06-24T06:29:22.256935Z","iopub.status.idle":"2024-06-24T06:29:23.118525Z","shell.execute_reply.started":"2024-06-24T06:29:22.256910Z","shell.execute_reply":"2024-06-24T06:29:23.117587Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746599b0720b4ad681b722ea02350744"}},"metadata":{}},{"name":"stderr","text":"\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b1c26c3af1848f09a4026cbfeae3d0b"}},"metadata":{}},{"name":"stderr","text":"\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    torch_dtype=torch.float16,\n#     device_map={\"\": 0},\n    device_map=\"cuda\",\n    trust_remote_code=True\n)\n\nmodel.gradient_checkpointing_enable()\nmodel.device","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:29:23.120085Z","iopub.execute_input":"2024-06-24T06:29:23.120433Z","iopub.status.idle":"2024-06-24T06:29:40.444942Z","shell.execute_reply.started":"2024-06-24T06:29:23.120399Z","shell.execute_reply":"2024-06-24T06:29:40.443898Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c44a81c40eb4cf187335307ddc667b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_openelm.py:   0%|          | 0.00/14.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8545a26d2d4bdca43dcabad80b3cbf"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-270M:\n- configuration_openelm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_openelm.py:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a61d67c605fd474e977c8594f373cc11"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-270M:\n- modeling_openelm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd85a956664d4c8cb084e4af3e7c1450"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b76876151d9c42649abbafdc7c280012"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"from trl import ORPOTrainer, ORPOConfig\n\norpo_config=ORPOConfig(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    evaluation_strategy=\"steps\",\n    do_eval=True,\n    optim=\"adamw_8bit\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=8,\n    log_level=\"debug\",\n    logging_steps=100,\n    learning_rate=8e-6,\n    eval_steps=100,\n    save_steps=100,\n    save_strategy=\"epoch\",\n    num_train_epochs=1,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n    beta=0.1, # beta is ORPO's lambda\n    max_length=1024,\n    report_to=\"wandb\",\n    run_name=os.getenv('WANDB_NAME')\n)\n\ntrainer = ORPOTrainer(\n        model=model,\n        train_dataset=train_ds,\n        eval_dataset=eval_ds,\n        args=orpo_config,\n        tokenizer=tokenizer,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:29:40.446509Z","iopub.execute_input":"2024-06-24T06:29:40.447118Z","iopub.status.idle":"2024-06-24T06:34:09.977115Z","shell.execute_reply.started":"2024-06-24T06:29:40.447083Z","shell.execute_reply":"2024-06-24T06:34:09.976092Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-06-24 06:29:42.662166: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-24 06:29:42.662273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-24 06:29:42.794953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c05eabb77d449199bfcaa38e9b85078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ad93e3915246c9a1d68625f2319fd2"}},"metadata":{}},{"name":"stderr","text":"You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\nCurrently training with a batch size of: 8\n***** Running training *****\n  Num examples = 300\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 2\n  Total optimization steps = 19\n  Number of trainable parameters = 271,527,168\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33murakiny\u001b[0m (\u001b[33mcausal_language_trainer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240624_062957-dsjkfrrq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20openELM-270m%20with%20ultrafeedback/runs/dsjkfrrq' target=\"_blank\">ft-openelm-270m-ultrafeedback</a></strong> to <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20openELM-270m%20with%20ultrafeedback' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20openELM-270m%20with%20ultrafeedback' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20openELM-270m%20with%20ultrafeedback</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20openELM-270m%20with%20ultrafeedback/runs/dsjkfrrq' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20openELM-270m%20with%20ultrafeedback/runs/dsjkfrrq</a>"},"metadata":{}},{"name":"stderr","text":"Could not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 03:43, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ft-openelm-270m-ultrafeedback/checkpoint-19\nConfiguration saved in ft-openelm-270m-ultrafeedback/checkpoint-19/config.json\nConfiguration saved in ft-openelm-270m-ultrafeedback/checkpoint-19/generation_config.json\nModel weights saved in ft-openelm-270m-ultrafeedback/checkpoint-19/model.safetensors\ntokenizer config file saved in ft-openelm-270m-ultrafeedback/checkpoint-19/tokenizer_config.json\nSpecial tokens file saved in ft-openelm-270m-ultrafeedback/checkpoint-19/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=19, training_loss=2.1115851151315788, metrics={'train_runtime': 255.1737, 'train_samples_per_second': 1.176, 'train_steps_per_second': 0.074, 'total_flos': 0.0, 'train_loss': 2.1115851151315788, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"kwargs={\n    'model_name': os.getenv(\"WANDB_NAME\"),\n    'finetuned_from': os.getenv('MODEL_NAME'),\n#     'tasks': 'Text-Generation',\n#     'dataset_tags':'',\n    'dataset': os.getenv(\"DATASET\")\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:34:09.981445Z","iopub.execute_input":"2024-06-24T06:34:09.981725Z","iopub.status.idle":"2024-06-24T06:34:47.171172Z","shell.execute_reply.started":"2024-06-24T06:34:09.981700Z","shell.execute_reply":"2024-06-24T06:34:47.169913Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"tokenizer config file saved in ft-openelm-270m-ultrafeedback/tokenizer_config.json\nSpecial tokens file saved in ft-openelm-270m-ultrafeedback/special_tokens_map.json\nUploading the following files to aisuko/ft-openelm-270m-ultrafeedback: README.md,tokenizer.model,tokenizer.json,special_tokens_map.json,tokenizer_config.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45fa2ff8f03c47a8ace51f2f6a5f3d04"}},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ft-openelm-270m-ultrafeedback\nConfiguration saved in ft-openelm-270m-ultrafeedback/config.json\nConfiguration saved in ft-openelm-270m-ultrafeedback/generation_config.json\nModel weights saved in ft-openelm-270m-ultrafeedback/model.safetensors\ntokenizer config file saved in ft-openelm-270m-ultrafeedback/tokenizer_config.json\nSpecial tokens file saved in ft-openelm-270m-ultrafeedback/special_tokens_map.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87243325d2284b5185c0929e74a29e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d2e5f6b3574aa191932890159b3056"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f17c156607df47c9922a41bf7d12c889"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/aisuko/ft-openelm-270m-ultrafeedback/commit/209431fc805b493e67d0159a751a464f09c49b65', commit_message='End of training', commit_description='', oid='209431fc805b493e67d0159a751a464f09c49b65', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"model=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"WANDB_NAME\"), \n    torch_dtype=torch.float16, \n    device_map=\"cuda\", \n    trust_remote_code=True\n)\n\nchat=[\n    [{\"role\":\"user\",\"content\":\"How is vanilla cultivated?\"}],\n    [{\"role\": \"user\", \"content\": \"How much money do I have if I have one dollar?\"}],\n    [{\"role\": \"user\", \"content\": \"Where is Berlin?\"}],\n    [{\"role\": \"user\", \"content\": \"Give me a list of 5 European countries.\"}],\n    [{\"role\": \"user\", \"content\": \"What is AI?\"}],\n    [{\"role\": \"user\", \"content\": \"What can you do right? Exactly?\"}]\n]\n\n\nfor c in chat:\n    p=tokenizer.apply_chat_template(c, tokenize=False)\n    inputs = tokenizer(p, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, do_sample=True, pad_token_id=tokenizer.eos_token_id, top_p=0.9, max_new_tokens=150)\n    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(result)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T06:41:51.449390Z","iopub.execute_input":"2024-06-24T06:41:51.449791Z","iopub.status.idle":"2024-06-24T06:42:15.045028Z","shell.execute_reply.started":"2024-06-24T06:41:51.449760Z","shell.execute_reply":"2024-06-24T06:42:15.044085Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"loading configuration file ft-openelm-270m-ultrafeedback/config.json\nloading configuration file ft-openelm-270m-ultrafeedback/config.json\nModel config OpenELMConfig {\n  \"_name_or_path\": \"ft-openelm-270m-ultrafeedback\",\n  \"activation_fn_name\": \"swish\",\n  \"architectures\": [\n    \"OpenELMForCausalLM\"\n  ],\n  \"auto_map\": {\n    \"AutoConfig\": \"apple/OpenELM-270M--configuration_openelm.OpenELMConfig\",\n    \"AutoModelForCausalLM\": \"apple/OpenELM-270M--modeling_openelm.OpenELMForCausalLM\"\n  },\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"ffn_dim_divisor\": 256,\n  \"ffn_multipliers\": [\n    0.5,\n    0.73,\n    0.97,\n    1.2,\n    1.43,\n    1.67,\n    1.9,\n    2.13,\n    2.37,\n    2.6,\n    2.83,\n    3.07,\n    3.3,\n    3.53,\n    3.77,\n    4.0\n  ],\n  \"ffn_with_glu\": true,\n  \"head_dim\": 64,\n  \"initializer_range\": 0.02,\n  \"max_context_length\": 2048,\n  \"model_dim\": 1280,\n  \"model_type\": \"openelm\",\n  \"normalization_layer_name\": \"rms_norm\",\n  \"normalize_qk_projections\": true,\n  \"num_gqa_groups\": 4,\n  \"num_kv_heads\": [\n    3,\n    3,\n    3,\n    3,\n    3,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    5,\n    5,\n    5,\n    5\n  ],\n  \"num_query_heads\": [\n    12,\n    12,\n    12,\n    12,\n    12,\n    16,\n    16,\n    16,\n    16,\n    16,\n    16,\n    16,\n    20,\n    20,\n    20,\n    20\n  ],\n  \"num_transformer_layers\": 16,\n  \"qkv_multipliers\": [\n    0.5,\n    1.0\n  ],\n  \"rope_freq_constant\": 10000,\n  \"rope_max_length\": 4096,\n  \"share_input_output_layers\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.39.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file ft-openelm-270m-ultrafeedback/model.safetensors\nInstantiating OpenELMForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2\n}\n\nAll model checkpoint weights were used when initializing OpenELMForCausalLM.\n\nAll the weights of OpenELMForCausalLM were initialized from the model checkpoint at ft-openelm-270m-ultrafeedback.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use OpenELMForCausalLM for predictions without further training.\nloading configuration file ft-openelm-270m-ultrafeedback/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2\n}\n\n\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"[INST] How is vanilla cultivated? [/INST].\n[INST] Why doesn't it have an \"Evacuate the Hole\" function? It seems like it's the most used item. What is it used for? [/INST] We're going to be focusing on the basics so far, and then as a guide we'll go through a little bit more about what we're doing. And, I'm going to make it clear that this isn't about an official guide, I'm just here to provide you guys with a few general tips and tricks. And, if you want to learn a little more, you can read my latest post: How to Get A Procedural Vanilla Map\nSo, the\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"[INST] How much money do I have if I have one dollar? [/INST] [INST] How much money do I have if I have one dollar? [/INST]\n\nI have been saving and earning since I was 20. I have no debt, so that's not a concern. I have no savings and I am 23. I'm living paycheck to paycheck, so my savings is pretty minimal and I have had no money coming in so far. \n\nI've had a few days now where I have been feeling like I don't have money and it's affecting my life in a negative way. I don't know if it is my job or my bad mood or my own self doubt, but my de\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"[INST] Where is Berlin? [/INST] [/INST]\nI was very lucky to be invited to the second International Conference on \"Five Reasons to Improve Your Life as a Young Professional in Berlin\". This event is about how to use one's time to make a difference in the world.\nI came from a very good background. I've been an active professional since I was 20 years old and had a good job for 12 years. I had the chance to work as a researcher for some startups and had a great impact on the world.\nFor many years I had the good fortune to be involved with the UN Millennium Development Goals (UNDGs) which I worked as a project leader on an\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"[INST] Give me a list of 5 European countries. [/INST]'\n\nThe following is a list of 5 countries, as per the UNESCO website:\n\n\n1) Ireland\n\n2) Iceland\n\n3) Portugal\n\n4) Luxembourg\n\n5) Cyprus\n\n6) Malta\n\n7) Greece\n\n8) Latvia\n\n9) The Netherlands\n\n10) Malta\n\n11) Portugal\n\n12) Denmark\n\n13) Spain\n\n14) Belgium\n\n15) The Netherlands\n\n16) Luxembourg\n\n17) Spain\n\n18) Turkey\n\n19) Luxembourg\n\n20) Bulgaria\n\n\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"[INST] What is AI? [/INST] the internet as a whole is a lot more complex. As far as the US is concerned, a great deal of the time, it's been really easy for everyone to be aware of things. And the main question here is whether or not it's something we want, or whether it's something that we should be doing. But the problem is, what do people expect to find and get out of it? That's a lot of information. When it's really hard to find anything at all. And, the first part of the question that we've all been asking is whether the government's role is just as important as other governments'. When there's so much information out there, what's going\n[INST] What can you do right? Exactly? [/INST]\n\nFirst thing that comes to mind: Don't eat junk food. Even if you're a vegetarian, I've noticed this to be true on this sub as well (and you don't even have to starve yourself to lose weight). As for my opinion, my mom has a different opinion. She doesn't even say \"eat junk food\" but has a different opinion about food itself. We eat the same way, and that's just the way we were raised. We don't eat fast food but we definitely aren't a vegetarian. So she says that you should eat a more nutritious diet and that we are not vegetarians because we aren't\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We will compare OpenELM-270M to the other 1B parameters models, because we want to check the if we can better results from the larger models.","metadata":{}},{"cell_type":"markdown","source":"# Credit\n\n* https://medium.com/@bnjmn_marie/fine-tune-tiny-chat-models-with-apple-openelm-and-orpo-f7be4fc137cd\n* https://huggingface.co/apple/OpenELM","metadata":{}}]}