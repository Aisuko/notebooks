{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nORPO is a new exciting fine-tuning technique that combines that traditional supervised fine-tuning and preference alignement stagaes into a single process. This reduces the computational resources and time required for training. Moreover, empirical results demonstrate that ORPO outperforms other alignment methods on various model size and benchmarks.\n\nWe will fine-tune the Llama 3 model 8B model using ORPO with the TRL library.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -U -q transformers==4.38.2\n!pip install -U -q accelerate==0.27.2\n!pip install -U -q datasets==2.18.0\n!pip install -U -q peft==0.9.0\n!pip install -U -q bitsandbytes==0.42.0\n!pip install -U -q trl==0.8.6","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:49:53.129679Z","iopub.execute_input":"2024-04-25T12:49:53.130529Z","iopub.status.idle":"2024-04-25T12:51:26.303433Z","shell.execute_reply.started":"2024-04-25T12:49:53.130485Z","shell.execute_reply":"2024-04-25T12:51:26.302349Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Note: If your env suports flash attention, be sure installed it.","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    attn_implementation = \"flash_attention_2\"\n    torch_dtype = torch.bfloat16\nelse:\n    attn_implementation = \"eager\"\n    torch_dtype = torch.float16","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:51:26.305417Z","iopub.execute_input":"2024-04-25T12:51:26.305728Z","iopub.status.idle":"2024-04-25T12:51:29.475973Z","shell.execute_reply.started":"2024-04-25T12:51:26.305701Z","shell.execute_reply":"2024-04-25T12:51:29.475161Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Llama 3 8B\"\nos.environ[\"WANDB_NAME\"] = \"ft-Llama3-8b-orpo\"\nos.environ[\"MODEL_NAME\"] = \"meta-llama/Meta-Llama-3-8B\"\nos.environ[\"DATASET\"] = \"mlabonne/orpo-dpo-mix-40k\"\n\ntorch.backends.cudnn.deterministic=True\n# https://github.com/huggingface/transformers/issues/28731\ntorch.backends.cuda.enable_mem_efficient_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:51:29.477205Z","iopub.execute_input":"2024-04-25T12:51:29.477681Z","iopub.status.idle":"2024-04-25T12:51:30.448171Z","shell.execute_reply.started":"2024-04-25T12:51:29.477648Z","shell.execute_reply":"2024-04-25T12:51:30.447062Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Quantization with QLoRA","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nfrom peft import LoraConfig\n\nbnb_config=BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True\n#     llm_int8_enable_fp32_cpu_offload=True\n)\n\npeft_config=LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:51:30.449418Z","iopub.execute_input":"2024-04-25T12:51:30.449726Z","iopub.status.idle":"2024-04-25T12:51:32.840568Z","shell.execute_reply.started":"2024-04-25T12:51:30.449700Z","shell.execute_reply":"2024-04-25T12:51:32.839648Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv('MODEL_NAME'))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:51:32.842974Z","iopub.execute_input":"2024-04-25T12:51:32.843365Z","iopub.status.idle":"2024-04-25T12:51:34.830839Z","shell.execute_reply.started":"2024-04-25T12:51:32.843340Z","shell.execute_reply":"2024-04-25T12:51:34.830032Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdf1cd09135f4d3d9c9bf630135f0a6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23046bc923c4c168482c764b965d368"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae8a99d0ddee42d692e4a32a7647acdc"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv('MODEL_NAME'),\n    quantization_config=bnb_config,\n    device_map='cuda',\n    attn_implementation=attn_implementation\n)\n\nmodel.device","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:51:34.831862Z","iopub.execute_input":"2024-04-25T12:51:34.832109Z","iopub.status.idle":"2024-04-25T12:53:40.559635Z","shell.execute_reply.started":"2024-04-25T12:51:34.832087Z","shell.execute_reply":"2024-04-25T12:53:40.558676Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a746737dca124d3ba1423371c6816b0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7e2b459d57493c926087ed77f1a864"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d124a486ffbe4986b7f21528790591d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91aea770afd74f99be6ca4af6b16597b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc28b768e14b41a483abd23a4fd8ccc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2360ec9f116f4551aca1912b197b12e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88948ef77fe249b9b0fabe910c689c8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20dad4779a9f48ab9c0a68b37b056bc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cab40fd4b1b42b5b5bb3e9555691c45"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Set chat format and feeding pretrained weights","metadata":{}},{"cell_type":"code","source":"from trl import setup_chat_format\nfrom peft import prepare_model_for_kbit_training\n\nmodel, tokenizer=setup_chat_format(model, tokenizer)\n\nmodel=prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:53:40.560775Z","iopub.execute_input":"2024-04-25T12:53:40.561058Z","iopub.status.idle":"2024-04-25T12:53:40.662060Z","shell.execute_reply.started":"2024-04-25T12:53:40.561034Z","shell.execute_reply":"2024-04-25T12:53:40.661227Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.device","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:53:40.663299Z","iopub.execute_input":"2024-04-25T12:53:40.663686Z","iopub.status.idle":"2024-04-25T12:53:40.670405Z","shell.execute_reply.started":"2024-04-25T12:53:40.663649Z","shell.execute_reply":"2024-04-25T12:53:40.669403Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Note: if you have enough computing resource, please considering use all data for your training.\n# ds=load_dataset(os.getenv('DATASET'), split='all')\n# ds=ds.shuffle(seed=42).select(range(1000))\n\n\nds=load_dataset(os.getenv('DATASET'), split='train[:200]')\nds","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:53:40.671699Z","iopub.execute_input":"2024-04-25T12:53:40.672007Z","iopub.status.idle":"2024-04-25T12:53:47.169795Z","shell.execute_reply.started":"2024-04-25T12:53:40.671982Z","shell.execute_reply":"2024-04-25T12:53:47.168813Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a04d6b424ea74029ac7ff06e617ba3f3"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 115M/115M [00:00<00:00, 155MB/s]  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/44245 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef45cbe4778345c5bd24f0914a919deb"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['source', 'chosen', 'rejected', 'prompt'],\n    num_rows: 200\n})"},"metadata":{}}]},{"cell_type":"code","source":"ds=ds.shuffle(seed=42)\n\ndef format_chat_template(row):\n    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n    return row\n\nds=ds.map(format_chat_template, num_proc=os.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:53:47.171136Z","iopub.execute_input":"2024-04-25T12:53:47.172006Z","iopub.status.idle":"2024-04-25T12:53:47.916057Z","shell.execute_reply.started":"2024-04-25T12:53:47.171970Z","shell.execute_reply":"2024-04-25T12:53:47.914263Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a49e8144cc487cbb02cd9a19a4fc0f"}},"metadata":{}}]},{"cell_type":"code","source":"ds=ds.train_test_split(test_size=0.01)\nds","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:53:47.918044Z","iopub.execute_input":"2024-04-25T12:53:47.918530Z","iopub.status.idle":"2024-04-25T12:53:47.943544Z","shell.execute_reply.started":"2024-04-25T12:53:47.918483Z","shell.execute_reply":"2024-04-25T12:53:47.942384Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['source', 'chosen', 'rejected', 'prompt'],\n        num_rows: 198\n    })\n    test: Dataset({\n        features: ['source', 'chosen', 'rejected', 'prompt'],\n        num_rows: 2\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine-tuning\n\nWe need to set a few hyperparameter for ORPO configuration.\n\n### learning_rate\n\nORPO uses very low learning rates compared to traditinal SFT or even DPO. This value of 8e-6 comes from the original paper. SFT is 1e-5, DPO is 5e-6.\n\n### beta\n\nIt is the $\\lambda\\$ parameter in the paper, with the default value of 0.1\n\n### max_lengthm batch_size\n\nOther parameters, like `max_length` and batch size are set to use as much VRAM as avaliable(~20 GB).","metadata":{}},{"cell_type":"code","source":"from trl import ORPOConfig, ORPOTrainer\n\n# https://github.com/huggingface/trl/blob/v0.8.6/trl/trainer/orpo_config.py\norpo_args=ORPOConfig(\n    learning_rate=8e-6,\n    beta=0.1,\n    lr_scheduler_type=\"linear\",\n    max_length=1024,\n    max_prompt_length=512,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_8bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    report_to=\"wandb\",\n    run_name=os.getenv('WANDB_NAME'),\n    output_dir=os.getenv('WANDB_NAME')\n)\n\ntrainer=ORPOTrainer(\n    model=model,\n    args=orpo_args,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"test\"],\n    peft_config=peft_config,\n    tokenizer=tokenizer\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:53:47.944700Z","iopub.execute_input":"2024-04-25T12:53:47.945002Z","iopub.status.idle":"2024-04-25T12:54:58.111440Z","shell.execute_reply.started":"2024-04-25T12:53:47.944976Z","shell.execute_reply":"2024-04-25T12:54:58.109530Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-04-25 12:53:50.478436: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-25 12:53:50.478534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-25 12:53:50.594004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/198 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"516328b92d20432ab7099a1aa3a98af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030c72ddb4e04bcea795a4541f58af0d"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33murakiny\u001b[0m (\u001b[33mcausal_language_trainer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240425_125405-alh1977k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20Llama%203%208B/runs/alh1977k' target=\"_blank\">ft-Llama3-8b-orpo</a></strong> to <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20Llama%203%208B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20Llama%203%208B' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20Llama%203%208B</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/causal_language_trainer/Fine-tuning%20Llama%203%208B/runs/alh1977k' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tuning%20Llama%203%208B/runs/alh1977k</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 34\u001b[0m\n\u001b[1;32m      4\u001b[0m orpo_args\u001b[38;5;241m=\u001b[39mORPOConfig(\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8e-6\u001b[39m,\n\u001b[1;32m      6\u001b[0m     beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWANDB_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m trainer\u001b[38;5;241m=\u001b[39mORPOTrainer(\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     27\u001b[0m     args\u001b[38;5;241m=\u001b[39morpo_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1971\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1971\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss_step\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_flos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_point_ops(inputs))\n\u001b[1;32m   1975\u001b[0m is_last_step_and_steps_less_than_grad_acc \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1976\u001b[0m     steps_in_epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;129;01mand\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m steps_in_epoch\n\u001b[1;32m   1977\u001b[0m )\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!","output_type":"error"}]},{"cell_type":"code","source":"kwargs={\n    'model_name': os.getenv(\"WANDB_NAME\"),\n    'finetuned_from': os.getenv('MODEL_NAME'),\n#     'tasks': '',\n#     'dataset_tags':'',\n    'dataset': os.getenv(\"DATASET\")\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:54:58.112521Z","iopub.status.idle":"2024-04-25T12:54:58.112978Z","shell.execute_reply.started":"2024-04-25T12:54:58.112790Z","shell.execute_reply":"2024-04-25T12:54:58.112805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merge and push merged model","metadata":{}},{"cell_type":"code","source":"import gc\n\ndel trainer, model\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:54:58.114848Z","iopub.status.idle":"2024-04-25T12:54:58.115186Z","shell.execute_reply.started":"2024-04-25T12:54:58.115026Z","shell.execute_reply":"2024-04-25T12:54:58.115040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained(os.getenv('MODEL_NAME'))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:54:58.116397Z","iopub.status.idle":"2024-04-25T12:54:58.116767Z","shell.execute_reply.started":"2024-04-25T12:54:58.116575Z","shell.execute_reply":"2024-04-25T12:54:58.116589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    os.getenv('MODEL_NAME'),\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nmodel.device()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:54:58.118157Z","iopub.status.idle":"2024-04-25T12:54:58.118562Z","shell.execute_reply.started":"2024-04-25T12:54:58.118385Z","shell.execute_reply":"2024-04-25T12:54:58.118402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = setup_chat_format(model, tokenizer)\n\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(model, os.getenv(\"WANDB_NAME\"))\nmodel = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:54:58.120278Z","iopub.status.idle":"2024-04-25T12:54:58.120645Z","shell.execute_reply.started":"2024-04-25T12:54:58.120453Z","shell.execute_reply":"2024-04-25T12:54:58.120467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.push_to_hub(os.getenv(\"WANDB_NAME\"), use_temp_dir=False)\n# tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"), use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T12:54:58.122111Z","iopub.status.idle":"2024-04-25T12:54:58.122439Z","shell.execute_reply.started":"2024-04-25T12:54:58.122282Z","shell.execute_reply":"2024-04-25T12:54:58.122296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://www.kaggle.com/code/aisuko/fine-tuning-phi-2-with-qlora\n* https://medium.com/towards-data-science/fine-tune-llama-3-with-orpo-56cfab2f9ada\n* https://www.kaggle.com/code/aisuko/llm-prompt-recovery-with-gemma","metadata":{}}]}