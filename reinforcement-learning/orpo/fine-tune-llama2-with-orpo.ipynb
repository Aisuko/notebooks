{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/fine-tune-llama3-with-orpo?scriptVersionId=174050973\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nORPO is a new exciting fine-tuning technique that combines that traditional supervised fine-tuning and preference alignement stagaes into a single process. This reduces the computational resources and time required for training. Moreover, empirical results demonstrate that ORPO outperforms other alignment methods on various model size and benchmarks.\n\nWe will fine-tune the Llama 3 model 8B model using ORPO with the TRL library.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -U -q transformers==4.39.3\n!pip install -U -q accelerate==0.28.0\n!pip install -U -q datasets==2.18.0\n!pip install -U -q peft==0.10.0\n!pip install -U -q bitsandbytes==0.43.1\n!pip install -U -q trl==0.8.6","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:00:17.592965Z","iopub.execute_input":"2024-04-26T02:00:17.593807Z","iopub.status.idle":"2024-04-26T02:01:36.074479Z","shell.execute_reply.started":"2024-04-26T02:00:17.593762Z","shell.execute_reply":"2024-04-26T02:01:36.073034Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Note: If your env suports flash attention, be sure installed it.","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    attn_implementation = \"flash_attention_2\"\n    torch_dtype = torch.bfloat16\nelse:\n    attn_implementation = \"eager\"\n    torch_dtype = torch.float16","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:36.077726Z","iopub.execute_input":"2024-04-26T02:01:36.078692Z","iopub.status.idle":"2024-04-26T02:01:36.086464Z","shell.execute_reply.started":"2024-04-26T02:01:36.078658Z","shell.execute_reply":"2024-04-26T02:01:36.085279Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Llama 3 8B\"\nos.environ[\"WANDB_NAME\"] = \"ft-Llama3-8b-orpo\"\nos.environ[\"MODEL_NAME\"] = \"meta-llama/Meta-Llama-3-8B\"\nos.environ[\"DATASET\"] = \"mlabonne/orpo-dpo-mix-40k\"\n\ntorch.backends.cudnn.deterministic=True\n# https://github.com/huggingface/transformers/issues/28731\ntorch.backends.cuda.enable_mem_efficient_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:36.090321Z","iopub.execute_input":"2024-04-26T02:01:36.091002Z","iopub.status.idle":"2024-04-26T02:01:36.493239Z","shell.execute_reply.started":"2024-04-26T02:01:36.090962Z","shell.execute_reply":"2024-04-26T02:01:36.492255Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:36.494699Z","iopub.execute_input":"2024-04-26T02:01:36.495118Z","iopub.status.idle":"2024-04-26T02:01:44.82598Z","shell.execute_reply.started":"2024-04-26T02:01:36.495078Z","shell.execute_reply":"2024-04-26T02:01:44.824652Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Loading pretrained config for `meta-llama/Meta-Llama-3-8B` from `transformers`...\n┌──────────────────────────────────────────────────────┐\n│Memory Usage for loading `meta-llama/Meta-Llama-3-8B` │\n├───────┬─────────────┬──────────┬─────────────────────┤\n│ dtype │Largest Layer│Total Size│ Training using Adam │\n├───────┼─────────────┼──────────┼─────────────────────┤\n│float32│   1.96 GB   │ 28.21 GB │      112.83 GB      │\n│float16│  1002.0 MB  │ 14.1 GB  │       56.42 GB      │\n│  int8 │   501.0 MB  │ 7.05 GB  │       28.21 GB      │\n│  int4 │   250.5 MB  │ 3.53 GB  │       14.1 GB       │\n└───────┴─────────────┴──────────┴─────────────────────┘\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Quantization with QLoRA","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nfrom peft import LoraConfig\n\nbnb_config=BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True\n#     llm_int8_enable_fp32_cpu_offload=True\n)\n\npeft_config=LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:44.82924Z","iopub.execute_input":"2024-04-26T02:01:44.829609Z","iopub.status.idle":"2024-04-26T02:01:44.84144Z","shell.execute_reply.started":"2024-04-26T02:01:44.829579Z","shell.execute_reply":"2024-04-26T02:01:44.840193Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv('MODEL_NAME'))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:44.843693Z","iopub.execute_input":"2024-04-26T02:01:44.843994Z","iopub.status.idle":"2024-04-26T02:01:45.486195Z","shell.execute_reply.started":"2024-04-26T02:01:44.84397Z","shell.execute_reply":"2024-04-26T02:01:45.484978Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv('MODEL_NAME'),\n    quantization_config=bnb_config,\n    # https://github.com/huggingface/trl/issues/1571#issuecomment-2075404536\n#     device_map='auto',\n    torch_dtype=torch_dtype\n#     attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:01:45.487765Z","iopub.execute_input":"2024-04-26T02:01:45.488171Z","iopub.status.idle":"2024-04-26T02:02:52.057935Z","shell.execute_reply.started":"2024-04-26T02:01:45.488132Z","shell.execute_reply":"2024-04-26T02:02:52.056789Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f1613f3ca548f2bebbc75d7aec4f68"}},"metadata":{}}]},{"cell_type":"code","source":"model.device","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.059605Z","iopub.execute_input":"2024-04-26T02:02:52.059904Z","iopub.status.idle":"2024-04-26T02:02:52.066991Z","shell.execute_reply.started":"2024-04-26T02:02:52.059878Z","shell.execute_reply":"2024-04-26T02:02:52.065993Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params=0\n    all_params=0\n    for _, param in model.named_parameters():\n        all_params+=param.numel()\n        if param.requires_grad:\n            trainable_params+=param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params/all_params:.2f}\")\n\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.068598Z","iopub.execute_input":"2024-04-26T02:02:52.068876Z","iopub.status.idle":"2024-04-26T02:02:52.080095Z","shell.execute_reply.started":"2024-04-26T02:02:52.068852Z","shell.execute_reply":"2024-04-26T02:02:52.078301Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"trainable params: 1050939392 || all params: 4540600320 || trainable%: 23.15\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Set chat format and feeze pretrained weights","metadata":{}},{"cell_type":"code","source":"from trl import setup_chat_format\nfrom peft import prepare_model_for_kbit_training\n\nmodel, tokenizer=setup_chat_format(model, tokenizer)\n\nmodel=prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.081493Z","iopub.execute_input":"2024-04-26T02:02:52.081777Z","iopub.status.idle":"2024-04-26T02:02:52.739038Z","shell.execute_reply.started":"2024-04-26T02:02:52.081753Z","shell.execute_reply":"2024-04-26T02:02:52.736876Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_chat_format\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_model_for_kbit_training\n\u001b[0;32m----> 4\u001b[0m model, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43msetup_chat_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m=\u001b[39mprepare_model_for_kbit_training(model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/models/utils.py:97\u001b[0m, in \u001b[0;36msetup_chat_format\u001b[0;34m(model, tokenizer, format, resize_to_multiple_of)\u001b[0m\n\u001b[1;32m     94\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mchat_template \u001b[38;5;241m=\u001b[39m chat_format\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# resize embedding layer to a multiple of 64, https://x.com/karpathy/status/1621578354024677377\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize_token_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_to_multiple_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresize_to_multiple_of\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Update the model config to use the new eos & bos tokens\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1803\u001b[0m, in \u001b[0;36mPreTrainedModel.resize_token_embeddings\u001b[0;34m(self, new_num_tokens, pad_to_multiple_of)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize_token_embeddings\u001b[39m(\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m, new_num_tokens: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, pad_to_multiple_of: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding:\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;124;03m    Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;124;03m        `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1803\u001b[0m     model_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resize_token_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_num_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_num_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pad_to_multiple_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1805\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model_embeds\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1840\u001b[0m, in \u001b[0;36mPreTrainedModel._resize_token_embeddings\u001b[0;34m(self, new_num_tokens, pad_to_multiple_of)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output_embeddings() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[1;32m   1839\u001b[0m     old_lm_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output_embeddings()\n\u001b[0;32m-> 1840\u001b[0m     new_lm_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_resized_lm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_lm_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_num_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(old_lm_head, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1842\u001b[0m         hook \u001b[38;5;241m=\u001b[39m old_lm_head\u001b[38;5;241m.\u001b[39m_hf_hook\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2011\u001b[0m, in \u001b[0;36mPreTrainedModel._get_resized_lm_head\u001b[0;34m(self, old_lm_head, new_num_tokens, transposed)\u001b[0m\n\u001b[1;32m   2005\u001b[0m has_new_lm_head_bias \u001b[38;5;241m=\u001b[39m old_lm_head\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;66;03m# When using DeepSpeed ZeRO-3, we shouldn't create new embeddings with DeepSpeed init\u001b[39;00m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;66;03m# because the shape of the new embedding layer is used across various modeling files\u001b[39;00m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;66;03m# as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;66;03m# to errors when training.\u001b[39;00m\n\u001b[0;32m-> 2011\u001b[0m new_lm_head \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_lm_head_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_new_lm_head_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mold_lm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mold_lm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;66;03m# initialize new lm head (in particular added tokens)\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_weights(new_lm_head)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1004.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 543.06 MiB is free. Process 2299 has 14.21 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 32.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1004.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 543.06 MiB is free. Process 2299 has 14.21 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 32.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"print_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.740006Z","iopub.status.idle":"2024-04-26T02:02:52.740411Z","shell.execute_reply.started":"2024-04-26T02:02:52.74022Z","shell.execute_reply":"2024-04-26T02:02:52.740237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Note: if you have enough computing resource, please considering use all data for your training.\n# ds=load_dataset(os.getenv('DATASET'), split='all')\n# ds=ds.shuffle(seed=42).select(range(1000))\n\n\nds=load_dataset(os.getenv('DATASET'), split='train[:200]')\nds","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.741475Z","iopub.status.idle":"2024-04-26T02:02:52.741819Z","shell.execute_reply.started":"2024-04-26T02:02:52.741651Z","shell.execute_reply":"2024-04-26T02:02:52.741665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds=ds.shuffle(seed=42)\n\ndef format_chat_template(row):\n    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n    return row\n\nds=ds.map(format_chat_template, num_proc=os.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.743127Z","iopub.status.idle":"2024-04-26T02:02:52.743504Z","shell.execute_reply.started":"2024-04-26T02:02:52.743299Z","shell.execute_reply":"2024-04-26T02:02:52.743313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds=ds.train_test_split(test_size=0.01)\nds","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.744619Z","iopub.status.idle":"2024-04-26T02:02:52.744989Z","shell.execute_reply.started":"2024-04-26T02:02:52.744802Z","shell.execute_reply":"2024-04-26T02:02:52.744817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning\n\nWe need to set a few hyperparameter for ORPO configuration.\n\n### learning_rate\n\nORPO uses very low learning rates compared to traditinal SFT or even DPO. This value of 8e-6 comes from the original paper. SFT is 1e-5, DPO is 5e-6.\n\n### beta\n\nIt is the $\\lambda\\$ parameter in the paper, with the default value of 0.1\n\n### max_lengthm batch_size\n\nOther parameters, like `max_length` and batch size are set to use as much VRAM as avaliable(~20 GB).","metadata":{}},{"cell_type":"code","source":"from trl import ORPOConfig, ORPOTrainer\n\n# https://github.com/huggingface/trl/blob/v0.8.6/trl/trainer/orpo_config.py\norpo_args=ORPOConfig(\n    learning_rate=8e-6,\n    beta=0.1,\n    lr_scheduler_type=\"linear\",\n    max_length=1024,\n    max_prompt_length=512,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_8bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    report_to=\"wandb\",\n    run_name=os.getenv('WANDB_NAME'),\n    output_dir=os.getenv('WANDB_NAME')\n)\n\ntrainer=ORPOTrainer(\n    model=model,\n    args=orpo_args,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"test\"],\n    peft_config=peft_config,\n    tokenizer=tokenizer\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.747949Z","iopub.status.idle":"2024-04-26T02:02:52.748282Z","shell.execute_reply.started":"2024-04-26T02:02:52.748123Z","shell.execute_reply":"2024-04-26T02:02:52.748136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs={\n    'model_name': os.getenv(\"WANDB_NAME\"),\n    'finetuned_from': os.getenv('MODEL_NAME'),\n#     'tasks': '',\n#     'dataset_tags':'',\n    'dataset': os.getenv(\"DATASET\")\n}\n\ntokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.749678Z","iopub.status.idle":"2024-04-26T02:02:52.750028Z","shell.execute_reply.started":"2024-04-26T02:02:52.749852Z","shell.execute_reply":"2024-04-26T02:02:52.749866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merge and push merged model","metadata":{}},{"cell_type":"code","source":"import gc\n\ndel trainer, model\ngc.collect()\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.751186Z","iopub.status.idle":"2024-04-26T02:02:52.75155Z","shell.execute_reply.started":"2024-04-26T02:02:52.751354Z","shell.execute_reply":"2024-04-26T02:02:52.751369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained(os.getenv('MODEL_NAME'))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.754951Z","iopub.status.idle":"2024-04-26T02:02:52.755303Z","shell.execute_reply.started":"2024-04-26T02:02:52.755135Z","shell.execute_reply":"2024-04-26T02:02:52.75515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    os.getenv('MODEL_NAME'),\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nmodel.device()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.756769Z","iopub.status.idle":"2024-04-26T02:02:52.757088Z","shell.execute_reply.started":"2024-04-26T02:02:52.756928Z","shell.execute_reply":"2024-04-26T02:02:52.756941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = setup_chat_format(model, tokenizer)\n\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(model, os.getenv(\"WANDB_NAME\"))\nmodel = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.758728Z","iopub.status.idle":"2024-04-26T02:02:52.759073Z","shell.execute_reply.started":"2024-04-26T02:02:52.758905Z","shell.execute_reply":"2024-04-26T02:02:52.75892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.push_to_hub(os.getenv(\"WANDB_NAME\"), use_temp_dir=False)\n# tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"), use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T02:02:52.76038Z","iopub.status.idle":"2024-04-26T02:02:52.760858Z","shell.execute_reply.started":"2024-04-26T02:02:52.760623Z","shell.execute_reply":"2024-04-26T02:02:52.760643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledge\n\n* https://www.kaggle.com/code/aisuko/fine-tuning-phi-2-with-qlora\n* https://medium.com/towards-data-science/fine-tune-llama-3-with-orpo-56cfab2f9ada\n* https://www.kaggle.com/code/aisuko/llm-prompt-recovery-with-gemma","metadata":{}}]}