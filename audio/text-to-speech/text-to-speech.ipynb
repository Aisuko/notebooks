{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/text-to-speech?scriptVersionId=164955787\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nText-to-speech (TTS) is the task of creating natural-sounding speech from text, where the speech can be generated in multiple languages and for multiple speakers. Several text-to-speech models are currently avaliable in Transformers, such as Bark, MMS, VITS and SpeechT5. We can easily generate audio using the `text-to-audio` pipeline (or its alias `text-to-speech`). Som models, like Bark, can also be conditioned to generate non-verbal communications such as laughing, and crying, or even add music.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.35.2\n!pip install datasets==2.15.0\n!pip install soundfile==0.12.1\n!pip install speechbrain==0.5.16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\n# example of pipelien text-to-speech\npipe=pipeline(\"text-to-speech\", model=\"suno/bark-small\", device='cuda')\ntext=\"[clears throat] This is a test ...  and I just took a long pause.\"\noutput=pipe(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Audio\n\nAudio(output[\"audio\"], rate=output[\"sampling_rate\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are goging to fine-tune a TTS model, we can currently fine-tune SpeechT5. It is pre-trained on a combination of `speech-to-text` and `text-to-speech` data, allowing it to learn a unified space of hidden representations shared by both text and speech. This means that the same pre-trained model can be fine-tuned for different tasks. Furthermore, SpeechT5 supports multiple speakers through x-vector speaker embeddings.","metadata":{}},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning speech-t5\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune model distilbert base uncased\"\nos.environ[\"WANDB_NAME\"] = \"ft-speech-t5-on-voxpopuli\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset\n\nHere we are going to use VoxPopuli dataset. It is a large-scale multilingual speech corpus consisting of data sourced from 2009-2020 European Parliament event recordings. It contains labelled audio-transcription data for 15 European languages.\n\n**Note that VoxPopuli or any other automated speech recognition(ASR) dataset may not be the most suitable option for training TTS models. The features that make it beneficial for ASR, such as excessive background noise, are typically undesirable in TTS. However, finding top-quality, multilingual, and multi-speaker TTS datasets can be quite challenging.**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Audio\n\ndataset=load_dataset(\"facebook/voxpopuli\",\"nl\", split=\"train\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SpeechT5 expects audio data to have a sampling rate of 16 kHz, so make sure the examples in the dataset meet this requirement.","metadata":{}},{"cell_type":"code","source":"dataset=dataset.cast_column(\"audio\", Audio(sampling_rate=16000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the data\nLet's begin by defining the model checkpoint to use and loading the appropriate processor:","metadata":{}},{"cell_type":"code","source":"from transformers import SpeechT5Processor\n\ncheckpoint=\"microsoft/speecht5_tts\"\nprocessor=SpeechT5Processor.from_pretrained(checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text cleanup for SpeechT5 tokenization\n\nThe dataset examples contain `raw_text` and `normalized_text` features. When deciding which feature to use as the text input, consider that the SpeechT5 tokenizer doesn't have any tokens for numbers. In `normalized_text` the numbers are written out as text. Thus, it is a better fit, and we recommend using `normalized_text` as input text.\n\nBecause SpeechT5 was trained on the English language, it may not recognize certrain characters in the Dutch dataset. If left as is, these characters will be converted to `<unk>` tokens. However, in Dutch, certain characters like `à` are used to stress syllables. In order to preserve the meaning of the text, we can replace this character with a regular `a`.\n\nTo identify `unsupported` tokens, extract all unique characters in the dataset using the SpeechT5 Tokenizer which works with chatacters as tokens. To do this, write the `extract_all_chars` mapping function that concatenates the transcriptions from all examples into one string and converts it to a set of characters. Make sure to set batched=True and batch_size=-1 in dataset.map() so that all transcriptions are avaliable at once for the mapping function.","metadata":{}},{"cell_type":"code","source":"def extract_all_chars(batch):\n    all_text=\" \".join(batch[\"normalized_text\"])\n    vocab=list(set(all_text))\n    return {\"vocab\": [vocab], \"all_text\":[all_text]}\n\nvocabs=dataset.map(\n    extract_all_chars,\n    batched=True,\n    batch_size=-1,\n    keep_in_memory=True,\n    remove_columns=dataset.column_names,\n)\n\n\ntokenizer=processor.tokenizer\n\ndataset_vocab=set(vocabs[\"vocab\"][0])\ntokenizer_vocab={k for k, _ in tokenizer.get_vocab().items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deal with Special Characters\n\nNow, we have two sets of characters: one with the vocabulary from the dataset and one with the vocabulary from the tokenizer. To identify any unsupported characters in the dataset, we can take the difference between these two sets. The resulting set will contain the characters that are in the dataset but not in the tokenizer.\n\nTo handle the unsupported characters identified in the previous step, define a function that maps these characters to valid tokens. Note that spaces are already replaced by _ in the tokenizer and do not need to be handled separately.","metadata":{}},{"cell_type":"code","source":"print(dataset_vocab-tokenizer_vocab)\n\n\nreplacements=[\n    (\"à\", \"a\"),\n    (\"ç\", \"c\"),\n    (\"è\", \"e\"),\n    (\"ë\", \"e\"),\n    (\"í\", \"i\"),\n    (\"ï\", \"i\"),\n    (\"ö\", \"o\"),\n    (\"ü\", \"u\"),\n]\n\n\ndef cleanup_text(inputs):\n    for src, dst in replacements:\n        inputs[\"normalized_text\"]=inputs[\"normalized_text\"].replace(src, dst)\n    return inputs\n\ndataset=dataset.map(cleanup_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have deal with special characters in the text, it's time to shift focus to the audio data.","metadata":{}},{"cell_type":"markdown","source":"# Checking Speakers\n\nThe VoxPopuli dataset includes speech from multiple speakers, but how many speakers are represented in the datasets? To determine this, we can count the number of unique speakers and the number of examples each speaker contributes to the dataset. With a total of 20968 examples in the dataset, this information will give us a better understanding of the distribution of speakers and examples in the data.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport matplotlib.pyplot as plt\n\nspeaker_counts=defaultdict(int)\n\nfor speaker_id in dataset[\"speaker_id\"]:\n    speaker_counts[speaker_id]+=1\n\nplt.figure()\nplt.hist(speaker_counts.values(), bins=20)\nplt.ylabel(\"Speakers\")\nplt.xlabel(\"Examples\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning Data\n\nThe histogram reveals that approximately one-third of the speakers in the dataset have fewer than 100 examples, while arounf ten speakers have more than 500 examples. To improve training effficiency and balance the dataset, we can limit the data to speakers with between 100 and 400 examples.\n\nNote that some speakers with few examples may actually have more audio available if the examples are long. However, determining the total amount of audio for each speaker requires scanning through the entire dataset, which is a time-consuming process that involves loading and decoding each audio file. As such, we have chosen to skip this step here.","metadata":{}},{"cell_type":"code","source":"def select_speaker(speaker_id):\n    return 100<=speaker_counts[speaker_id]<=400\n\ndataset=dataset.filter(select_speaker, input_columns=[\"speaker_id\"])\n\n# Let's check how many speakers remain\nprint(len(set(dataset[\"speaker_id\"])))\n\n# Let's see how many examples are left\nprint(len(dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Speaker embeddings\n\nTo enable the TTS model to differentiable between multiple speakers, we will need to create a speaker emebeddings for each example. The speaker embedding it an additional input into the model that captures a particular speaker's voice characteristics. To generate these speakers embeddings, use the pre-trained spkrec-xvect-voxceleb model from SpeechBrain. Create a function **create_speaker_emebdding()** that takes an input audio waveform and outputs a 512-element vector containing the corresponding speaker embedding. It's important to note that the speechbrain/spkrec-xvect-voxceleb model was trained on English speech from the VoxCeleb dataset, whereas the training examples in here are in Dutch. While we believe that this model will still generate reasonable speaker embeddings for our Dutch dataset, this assumption may not hold true in all cases.\n\nFor optimal results, we recommend training an X-vector model on the target speech first. This will ensure that the model is better able to capture the unique voice characteristics present in the Dutch language.","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom speechbrain.pretrained import EncoderClassifier\n\nspk_model_name=\"speechbrain/spkrec-xvect-voxceleb\"\n\ndevice=\"cuda\"\nspeaker_model=EncoderClassifier.from_hparams(\n    source=spk_model_name,\n    run_opts={\"device\": device},\n    savedir=os.path.join(\"/tmp\", spk_model_name),\n)\n\n\ndef create_speaker_embedding(waveform):\n    with torch.no_grad():\n        speaker_embeddings=speaker_model.encode_batch(torch.tensor(waveform))\n        speaker_embeddings=torch.nn.functional.normalize(speaker_embeddings, dim=2)\n        speaker_embeddings=speaker_embeddings.squeeze().cpu().numpy()\n    return speaker_embeddings\n\n\ndef prepare_dataset(example):\n    audio=example[\"audio\"]\n    \n    example=processor(\n        text=example[\"normalized_text\"],\n        audio_target=audio[\"array\"],\n        sampling_rate=audio[\"sampling_rate\"],\n        return_attention_mask=False,\n    )\n    \n    # strip off thje batch dimension\n    example[\"labels\"]=example[\"labels\"][0]\n    \n    # use SpeechBrain to obtain x-vector\n    example[\"speaker_embeddings\"]=create_speaker_embedding(audio[\"array\"])\n    \n    return example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verify the processing is correct by looking at a single example, the speaker embeddings should be a 512-element vector, and the labels should be a log-mel spectrogram with 80 mel bins.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nprocessed_example=prepare_dataset(dataset[0])\nprint(list(processed_example.keys()))\nprint(processed_example[\"speaker_embeddings\"].shape)\n\nplt.figure()\nplt.imshow(processed_example[\"labels\"].T)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying the processing function to the entire dataset\ndataset=dataset.map(prepare_dataset, remove_columns=dataset.column_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will see a warning saying that some examples in the dataset are longer than the maximum input length the model can handle(600 tokens). Remove those examples from the dataset. Here we go even further and to follow for larger batch sizes we remove anything over 200 tokens.","metadata":{}},{"cell_type":"code","source":"def is_not_too_long(input_ids):\n    input_length=len(input_ids)\n    return input_length<200\n\ndataset=dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data collator\n\nWe want to combine multiple examples into a batch, we need to define a custom data collator. This collator will pad shorter sequences with padding tokens, ensuring that all examples have the same length. For the spectrogram labels, the padded portions are replaced with the special value `-100`. This special value instructs the model to ignore that part of the spectrogram when calculating the spectrogram loss.","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass TTSDataCollatorWithPadding:\n    \n    processor: Any\n    \n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_ids=[{\"input_ids\":feature[\"input_ids\"]} for feature in features]\n        label_features=[{\"input_values\":feature[\"labels\"]} for feature in features]\n        speaker_features=[feature[\"speaker_embeddings\"] for feature in features]\n        \n        # collate the inputs and targets into a batch\n        batch=processor.pad(input_ids=input_ids, labels=label_features, return_tensors=\"pt\")\n        \n        # replace padding with -100 to ignore loss correctly\n        batch[\"labels\"]=batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1),-100)\n        \n        #not used during fine-tuning\n        del batch[\"decoder_attention_mask\"]\n        \n        # round down target lengths to multiple of reduction factor\n        if model.config.reduction_factor>1:\n            target_lengths=torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n            target_lengths=target_lengths.new(\n                [length-length%model.config.reduction_factor for length in target_lengths]\n            )\n            max_length=max(target_lengths)\n            batch[\"labels\"]=batch[\"labels\"][:, :max_length]\n        \n        # also add in the speaker embeddings\n        batch[\"speaker_embeddings\"]=torch.tensor(speaker_features)\n        \n        return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other words, it throws away every other timestep from the target sequence. The decoder then predicts a sequence that is twice as long. Since the original target sequence length may be odd, the data collator makes sure to round the maximum length of the batch down to be a multiple of 2.","metadata":{}},{"cell_type":"code","source":"data_collator=TTSDataCollatorWithPadding(processor=processor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model\n\nThe `use_cache=True` option is incompatible with gradient checkpointing. Disable it for training.","metadata":{}},{"cell_type":"code","source":"from transformers import SpeechT5ForTextToSpeech\n\nmodel=SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\nmodel.config.use_cache=False\nprint(model.config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\ntraining_args=Seq2SeqTrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=1e-5,\n    warmup_steps=50,\n    max_steps=100,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=2,\n    save_steps=50,\n    eval_steps=50,\n    logging_steps=25,\n    report_to=\"wandb\", # or report_to=\"tensorboard\"\n    run_name=os.getenv(\"WANDB_NAME\"),\n    load_best_model_at_end=True,\n    greater_is_better=False,\n    label_names=[\"labels\"],\n    push_to_hub=False,\n)\n\ntrainer=Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    tokenizer=processor,\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processor.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(os.getenv(\"WANDB_NAME\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline()\n\npipe=pipeline(\"text-to-speech\", model=os.getenv(\"WANDB_NAME\"), device='cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=\"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n\nexample=dataset[\"test\"][304]\nspeaker_emebeddings=torch.tensor(example[\"speaker_emebddings\"]).unsqueeze(0)\n\nforward_params={\"speaker_embeddings\": speaker_embeddings}\noutput=pipe(text, forward_params=forward_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Audio\nAudio(output['audio'], rate=output['sampling_rate'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}