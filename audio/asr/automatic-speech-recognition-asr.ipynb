{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/automatic-speech-recognition-asr?scriptVersionId=164769594\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nAutomatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Many of virtual assistants like Siri and Alexa use ASR model to help users everyday, ans there are many other applications like live captioning and note-taking during meetings. Let's finetune Wav2Vec2-base model which pretrained on 16kHz sampled speech audio with `Automatic Speech Recognition` label dataset. We need to make sure the speech input is also sampled at 16kHz.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.35.2\n!pip install datasets==2.15.0\n!pip install evaluate==0.4.1\n!pip install jiwer==3.0.3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# checking Huggingface services status if the login was failed\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tuning Wav2Vec2-base\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune model distilbert base uncased\"\nos.environ[\"WANDB_NAME\"] = \"ft-wav2vec2-with-minds-asr\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading MinDS-14\n\n\nLet's pick up a smaller subset of the MInDS-14.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Audio\n\nminds=load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train[:500]\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the dataset's train split into a train and test set with the `train_test_split` method","metadata":{}},{"cell_type":"code","source":"minds=minds.train_test_split(test_size=0.2)\nminds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's focus on the audio and transcription, and remove the other columns with the `remove_columns` method.","metadata":{}},{"cell_type":"code","source":"minds=minds.remove_columns([\"english_transcription\", \"intent_class\", \"lang_id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minds[\"train\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two fileds:\n\n* `audio`: a l-dimensional `array` of the speech signal that must be called to load and resample the audio file.\n* `transcription`: the target text","metadata":{}},{"cell_type":"markdown","source":"# Preprocess\n\nLet's load a Wav2Vec2 process to process the audio signal with `AutoProcessor` It is multimodel tasks require a processor that combines two types of preprocessing tools.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor\n\nprocessor=AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's resample the dataset to 16000kHz to use the pretrained Wav2Vec2 model:","metadata":{}},{"cell_type":"code","source":"minds=minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\nminds[\"train\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see in the `transcription` above, the text contains a mix of upper and lowercase characters. The Wav2Vec2 **tokenizer** is only trained on uppercase characters. So we will need to make sure the text maches the tokenizer's vocabulary:","metadata":{}},{"cell_type":"code","source":"def uppercase(example):\n    return {\"transcription\": example[\"transcription\"].upper()}\n\nminds=minds.map(uppercase)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a preprocessing function that:\n1. Calls the `audio` column to load and resample the audio file\n2. Extracts the `input_values` from the audio file and tokenize the transcription column with the processor.","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(batch):\n    audio=batch[\"audio\"]\n    batch=processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=batch[\"transcription\"])\n    batch[\"input_length\"]=len(batch[\"input_values\"][0])\n    return batch\n\nencoded_minds=minds.map(prepare_dataset, remove_columns=minds.column_names[\"train\"], num_proc=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to adapt the DataCollatorWithPadding to create a batch of examples. It will also dynamically pad our text and lables to the length of the longest element in its batch(instead of the entire dataset) so they are a uniform length. While it is possible to pad our text in the tokenizer function by setting padding=True, dynamic padding is more efficient.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    processor: AutoProcessor\n    padding: Union[bool, str]=\"longest\"\n        \n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Split inputs and labels since they have to be of different lengths and need different padding methods\n        input_features=[{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n        label_features=[{\"input_ids\": feature[\"labels\"]} for feature in features]\n        \n        batch=self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n        \n        labels_batch=self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n        \n        # replace padding with -100 to ignore loss correctly\n        labels=labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        \n        batch[\"labels\"]=labels\n        \n        return batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator=DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")\nprint(data_collator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate\n\nHere we load a evaluation method with Evaluate library. In this notebook, load the word error rate metric:","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\nwer_eva=evaluate.load(\"wer\")\n\n\ndef compute_metrics(pred):\n    pred_logits=pred.predictions\n    pred_ids=np.argmax(pred_logits, axis=-1)\n    \n    pred.label_ids[pred.label_ids==-100]=processor.tokenizer.pad_token_id\n    \n    pred_str=processor.batch_decode(pred_ids)\n    label_str=processor.batch_decode(pred.label_ids, group_tokens=False)\n    \n    wer=wer_eva.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": wer}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then create a function that passes our predictions and labels to compute to calcualte the WER:","metadata":{}},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCTC, TrainingArguments, Trainer\n\nmodel=AutoModelForCTC.from_pretrained(\n    \"facebook/wav2vec2-base\",\n    ctc_loss_reduction=\"mean\",\n    pad_token_id=processor.tokenizer.pad_token_id,\n)\n\nprint(model.config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=1e-5,\n    warmup_steps=500,\n    max_steps=1500,\n    gradient_checkpointing=True,\n    fp16=True,\n    group_by_length=True,\n    evaluation_strategy=\"steps\",\n    save_steps=1000,\n    eval_steps=1000,\n    logging_steps=25,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    report_to=\"wandb\",\n    run_name=os.getenv(\"WANDB_NAME\"),\n    push_to_hub=False,   \n)\n\ntrainer=Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_minds[\"train\"],\n    eval_dataset=encoded_minds[\"test\"],\n    tokenizer=processor,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\neval_results=trainer.evaluate()\nprint(f\"Perplexity:{math.exp(eval_results['eval_loss']):2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs={\n    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n    'finetuned_from': \"facebook/wav2vec2-base\",\n    'tasks': 'automatic speech recognition',\n    'dataset_tags':'automatic speech recognition',\n    'dataset':'PolyAI/minds14'\n}\n\nprocessor.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(**kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n\nLoad an audio file from dataset, and remember to resample the sampling rate of the audio file to match the sampling rate of the model.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Audio\n\ndataset=load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\ndataset=dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\nsampling_rate=dataset.features[\"audio\"].sampling_rate\naudio_file=dataset[0][\"audio\"][\"path\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\ntranscriber=pipeline(\"automatic-speech-recognition\", model=os.getenv(\"WANDB_NAME\"))\ntranscriber(audio_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## With PyTorch","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor\n\nprocessor=AutoProcessor.from_pretrained(os.getenv(\"WANDB_NAME\"))\ninputs=processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCTC\n\nmodel=AutoModelForCTC.from_pretrained(os.getenv(\"WANDB_NAME\"))\nwith torch.no_grad():\n    logits=model(**inputs).logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\npredicted_ids=torch.argmax(logits, dim=-1)\ntranscription=processor.batch_decode(predicted_ids)\ntranscription","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}