{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/preparing-gnndelete-parmeters?scriptVersionId=197399009\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nIn notebook [Loading data for gnndelete](https://www.kaggle.com/code/aisuko/loading-data-for-gnndelete), we failed to manipulate data. However, we deal with them by using RMIT race hub aws service. Currently, we release these data on:\n\n* https://www.kaggle.com/models/skywardai/ogbl-collab\n* https://www.kaggle.com/models/skywardai/ogbl-citation2\n\nI released them as models due to easier for us to do testing on Kaggle. They aren't models. Please noticed.\n\nAn also, we will twisted the code from GNNdelete project https://github.com/mims-harvard/GNNDelete.\n\n# What we do here?\n\nIn this notebook, we will learn and save all the parameters to json file. And we can load all these parameters in the other notebooks.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import argparse\n\nnum_edge_type_mapping = {\n    'FB15k-237': 237,\n    'WordNet18': 18,\n    'WordNet18RR': 11,\n    'ogbl-biokg': 51\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:11:51.455633Z","iopub.execute_input":"2024-09-20T05:11:51.456147Z","iopub.status.idle":"2024-09-20T05:11:51.469737Z","shell.execute_reply.started":"2024-09-20T05:11:51.456084Z","shell.execute_reply":"2024-09-20T05:11:51.468357Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"num_edge_type_mapping","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:11:51.472536Z","iopub.execute_input":"2024-09-20T05:11:51.47345Z","iopub.status.idle":"2024-09-20T05:11:51.506641Z","shell.execute_reply.started":"2024-09-20T05:11:51.473387Z","shell.execute_reply":"2024-09-20T05:11:51.505264Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'FB15k-237': 237, 'WordNet18': 18, 'WordNet18RR': 11, 'ogbl-biokg': 51}"},"metadata":{}}]},{"cell_type":"code","source":"import json\n\n# Specify the path of the JSON file\nfile_path = 'data.json'\n\n# Write the dictionary to the JSON file\nwith open(file_path, 'w') as json_file:\n    json.dump(num_edge_type_mapping, json_file, indent=4)  # 'indent=4' makes the JSON file more readable","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:11:51.508282Z","iopub.execute_input":"2024-09-20T05:11:51.508748Z","iopub.status.idle":"2024-09-20T05:11:51.516741Z","shell.execute_reply.started":"2024-09-20T05:11:51.508705Z","shell.execute_reply":"2024-09-20T05:11:51.515381Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import argparse\n\nsys.argv = ['notebook',\n        '--unlearning_model', 'retrain',\n        '--gnn', 'gcn',\n        '--in_dim', '128',\n        '--hidden_dim', '128',\n        '--out_dim', '64',\n        '--data_dir', './data',\n        '--df', 'none',\n        '--df_idx', 'none',\n        '--df_size', '0.5',\n        '--dataset', 'Cora',\n        '--random_seed', '42',\n        '--batch_size', '8192',\n        '--walk_length', '2',\n        '--num_steps', '32',\n        '--lr', '1e-3',\n        '--weight_decay', '0.0005',\n        '--optimizer', 'Adam',\n        '--epochs', '3000',\n        '--valid_freq', '100',\n        '--checkpoint_dir', './checkpoint',\n        '--alpha', '0.5',\n        '--neg_sample_random', 'non_connected',\n        '--loss_fct', 'mse_mean',\n        '--loss_type', 'both_layerwise',\n        '--num_clusters', '10',\n        '--kmeans_max_iters', '1',\n        '--shard_size_delta', '0.005',\n        '--terminate_delta', '0',\n        '--eval_steps', '1',\n        '--runs', '1',\n        '--num_remove_links', '11',\n        '--parallel_unlearning', '4',\n        '--lam', '0',\n        '--regen_feats', # This is a flag, so no value is needed\n        '--regen_neighbors', # This is a flag, so no value is needed\n        '--regen_links', # This is a flag, so no value is needed\n        '--regen_subgraphs', # This is a flag, so no value is needed\n        '--hop_neighbors', '20',\n        '--topk', '500',\n        '--eval_on_cpu', 'False',\n        '--num_edge_type', '0'\n    ]\n\n\ndef test():\n    \n    parser = argparse.ArgumentParser()\n    # Model\n    parser.add_argument('--unlearning_model', type=str, default='retrain', help='unlearning method')\n    parser.add_argument('--gnn', type=str, default='gcn', \n                        help='GNN architecture')\n    parser.add_argument('--in_dim', type=int, default=128, \n                        help='input dimension')\n    parser.add_argument('--hidden_dim', type=int, default=128, \n                        help='hidden dimension')\n    parser.add_argument('--out_dim', type=int, default=64, \n                        help='output dimension')\n\n    # Data\n    parser.add_argument('--data_dir', type=str, default='./data',\n                        help='data dir')\n    parser.add_argument('--df', type=str, default='none',\n                        help='Df set to use')\n    parser.add_argument('--df_idx', type=str, default='none',\n                        help='indices of data to be deleted')\n    parser.add_argument('--df_size', type=float, default=0.5,\n                        help='Df size')\n    parser.add_argument('--dataset', type=str, default='Cora',\n                        help='dataset')\n    parser.add_argument('--random_seed', type=int, default=42,\n                        help='random seed')\n    parser.add_argument('--batch_size', type=int, default=8192, \n                        help='batch size for GraphSAINTRandomWalk sampler')\n    parser.add_argument('--walk_length', type=int, default=2,\n                        help='random walk length for GraphSAINTRandomWalk sampler')\n    parser.add_argument('--num_steps', type=int, default=32,\n                        help='number of steps for GraphSAINTRandomWalk sampler')\n\n    # Training\n    parser.add_argument('--lr', type=float, default=1e-3, \n                        help='initial learning rate')\n    parser.add_argument('--weight_decay', type=float, default=0.0005, \n                        help='weight decay')\n    parser.add_argument('--optimizer', type=str, default='Adam', \n                        help='optimizer to use')\n    parser.add_argument('--epochs', type=int, default=3000, \n                        help='number of epochs to train')\n    parser.add_argument('--valid_freq', type=int, default=100,\n                        help='# of epochs to do validation')\n    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoint',\n                        help='checkpoint folder')\n    parser.add_argument('--alpha', type=float, default=0.5,\n                        help='alpha in loss function')\n    parser.add_argument('--neg_sample_random', type=str, default='non_connected',\n                        help='type of negative samples for randomness')\n    parser.add_argument('--loss_fct', type=str, default='mse_mean',\n                        help='loss function. one of {mse, kld, cosine}')\n    parser.add_argument('--loss_type', type=str, default='both_layerwise',\n                        help='type of loss. one of {both_all, both_layerwise, only2_layerwise, only2_all, only1}')\n\n    # GraphEraser\n    parser.add_argument('--num_clusters', type=int, default=10, \n                        help='top k for evaluation')\n    parser.add_argument('--kmeans_max_iters', type=int, default=1, \n                        help='top k for evaluation')\n    parser.add_argument('--shard_size_delta', type=float, default=0.005)\n    parser.add_argument('--terminate_delta', type=int, default=0)\n\n    # GraphEditor\n    parser.add_argument('--eval_steps', type=int, default=1)\n    parser.add_argument('--runs', type=int, default=1)\n\n    parser.add_argument('--num_remove_links', type=int, default=11)\n    parser.add_argument('--parallel_unlearning', type=int, default=4)\n\n    parser.add_argument('--lam', type=float, default=0)\n    parser.add_argument('--regen_feats', action='store_true')\n    parser.add_argument('--regen_neighbors', action='store_true')\n    parser.add_argument('--regen_links', action='store_true')\n    parser.add_argument('--regen_subgraphs', action='store_true')\n    parser.add_argument('--hop_neighbors', type=int, default=20)\n\n\n    # Evaluation\n    parser.add_argument('--topk', type=int, default=500, \n                        help='top k for evaluation')\n    parser.add_argument('--eval_on_cpu', type=bool, default=False, \n                        help='whether to evaluate on CPU')\n\n    # KG\n    parser.add_argument('--num_edge_type', type=int, default=0, \n                        help='number of edges types')\n\n    args = parser.parse_args()\n\n    if 'ogbl' in args.dataset:\n        args.eval_on_cpu = True\n\n    # For KG\n    if args.gnn in ['rgcn', 'rgat']:\n        args.lr = 1e-3\n        args.epochs = 3000\n        args.valid_freq = 500\n        args.batch_size //= 2\n        args.num_edge_type = num_edge_type_mapping[args.dataset]\n        args.eval_on_cpu = True\n        # args.in_dim = 512\n        # args.hidden_dim = 256\n        # args.out_dim = 128\n\n    if args.unlearning_model in ['original', 'retrain']:\n        args.epochs = 2000\n        args.valid_freq = 500\n        \n        # For large graphs\n        if args.gnn not in ['rgcn', 'rgat'] and 'ogbl' in args.dataset:\n            args.epochs = 600\n            args.valid_freq = 200\n        if args.gnn in ['rgcn', 'rgat'] and 'ogbl' in args.dataset:\n            args.batch_size = 1024\n\n    if 'gnndelete' in args.unlearning_model:\n        if args.gnn not in ['rgcn', 'rgat'] and 'ogbl' in args.dataset:\n            args.epochs = 600\n            args.valid_freq = 100\n        if args.gnn in ['rgcn', 'rgat']:\n            if args.dataset == 'WordNet18':\n                args.epochs = 50\n                args.valid_freq = 2\n                args.batch_size = 1024\n            if args.dataset == 'ogbl-biokg':\n                args.epochs = 50\n                args.valid_freq = 10\n                args.batch_size = 64\n\n    elif args.unlearning_model == 'gradient_ascent':\n        args.epochs = 10\n        args.valid_freq = 1\n    \n    elif args.unlearning_model == 'descent_to_delete':\n        args.epochs = 1\n\n    elif args.unlearning_model == 'graph_editor':\n        args.epochs = 400\n        args.valid_freq = 200\n\n\n    if args.dataset == 'ogbg-molhiv':\n        args.epochs = 100\n        args.valid_freq = 5\n\n    \n    return args\n\n\nargs=test()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:29:17.08821Z","iopub.execute_input":"2024-09-20T05:29:17.08923Z","iopub.status.idle":"2024-09-20T05:29:17.134747Z","shell.execute_reply.started":"2024-09-20T05:29:17.089159Z","shell.execute_reply":"2024-09-20T05:29:17.133015Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"args","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:29:49.656387Z","iopub.execute_input":"2024-09-20T05:29:49.656845Z","iopub.status.idle":"2024-09-20T05:29:49.666358Z","shell.execute_reply.started":"2024-09-20T05:29:49.656803Z","shell.execute_reply":"2024-09-20T05:29:49.664758Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Namespace(unlearning_model='retrain', gnn='gcn', in_dim=128, hidden_dim=128, out_dim=64, data_dir='./data', df='none', df_idx='none', df_size=0.5, dataset='Cora', random_seed=42, batch_size=8192, walk_length=2, num_steps=32, lr=0.001, weight_decay=0.0005, optimizer='Adam', epochs=2000, valid_freq=500, checkpoint_dir='./checkpoint', alpha=0.5, neg_sample_random='non_connected', loss_fct='mse_mean', loss_type='both_layerwise', num_clusters=10, kmeans_max_iters=1, shard_size_delta=0.005, terminate_delta=0, eval_steps=1, runs=1, num_remove_links=11, parallel_unlearning=4, lam=0.0, regen_feats=True, regen_neighbors=True, regen_links=True, regen_subgraphs=True, hop_neighbors=20, topk=500, eval_on_cpu=True, num_edge_type=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Acknowledgement\n\n* https://github.com/SkywardAI/GNNDelete/blob/main/framework/training_args.py","metadata":{}}]}