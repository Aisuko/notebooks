{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/zero-shot-image-classification?scriptVersionId=164773113\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nZero-shot image classification is a task that involves classifying images into different categories using a model that was not explicity trained on data containing labeled examples from those specific categories.\n\nTraditionally, image classifiction requires training a model on a specific set of labeled images, and this model learns to \"map\" certain image features to labels. When there's a need to use such model for a classification task that introduces a new set of labels, fine-tuning is required to \"recalibrate\" the model. In contrast, **zero-shot or open vocabulary image classification models are typically multi-modal models that have been trained on a large dataset of images and associated descriptions**. These models learn aligned vision-language representations that can be used for many downstream tasks including zero-shot image classification. This is a more flexible approach to image classification that allows models to generalize to new and unseen categories without the need for additional training data and enables users to query images with free-form text descriptions of their target objects.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.35.2","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:52:29.767457Z","iopub.execute_input":"2023-12-11T11:52:29.768084Z","iopub.status.idle":"2023-12-11T11:52:42.991784Z","shell.execute_reply.started":"2023-12-11T11:52:29.768038Z","shell.execute_reply":"2023-12-11T11:52:42.990566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Pipeline","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nmodel_checkpoint=\"openai/clip-vit-large-patch14\"\ndetector=pipeline(model=model_checkpoint, task=\"zero-shot-image-classification\")\ndetector.enable_cpu_offloading()\ndetector.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:52:42.994663Z","iopub.execute_input":"2023-12-11T11:52:42.995153Z","iopub.status.idle":"2023-12-11T11:53:16.135414Z","shell.execute_reply.started":"2023-12-11T11:52:42.995087Z","shell.execute_reply":"2023-12-11T11:53:16.134321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Image","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\n\nurl = \"https://unsplash.com/photos/g8oS8-82DxI/download?ixid=MnwxMjA3fDB8MXx0b3BpY3x8SnBnNktpZGwtSGt8fHx8fDJ8fDE2NzgxMDYwODc&force=true&w=640\"\nimage=Image.open(requests.get(url, stream=True).raw)\nimage","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:53:16.137296Z","iopub.execute_input":"2023-12-11T11:53:16.138533Z","iopub.status.idle":"2023-12-11T11:53:16.82611Z","shell.execute_reply.started":"2023-12-11T11:53:16.138484Z","shell.execute_reply":"2023-12-11T11:53:16.825192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=detector(image, candidate_labels=[\"fox\",\"bear\",\"seagull\",\"owl\"])\npredictions","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:53:16.827535Z","iopub.execute_input":"2023-12-11T11:53:16.827881Z","iopub.status.idle":"2023-12-11T11:53:22.492351Z","shell.execute_reply.started":"2023-12-11T11:53:16.82785Z","shell.execute_reply":"2023-12-11T11:53:22.49157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero-shot Image Classification\n\nLet's see how to use the zero-shot image classification pipeline.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n\nmodel=AutoModelForZeroShotImageClassification.from_pretrained(model_checkpoint)\nprocessor=AutoProcessor.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:53:22.494348Z","iopub.execute_input":"2023-12-11T11:53:22.494845Z","iopub.status.idle":"2023-12-11T11:53:29.09031Z","shell.execute_reply.started":"2023-12-11T11:53:22.494812Z","shell.execute_reply":"2023-12-11T11:53:29.089185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport requests\n\nurl = \"https://unsplash.com/photos/xBRQfR2bqNI/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjc4Mzg4ODEx&force=true&w=640\"\nimage=Image.open(requests.get(url, stream=True).raw)\nimage","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:53:29.091621Z","iopub.execute_input":"2023-12-11T11:53:29.091958Z","iopub.status.idle":"2023-12-11T11:53:29.624196Z","shell.execute_reply.started":"2023-12-11T11:53:29.091927Z","shell.execute_reply":"2023-12-11T11:53:29.623232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we use the processor to prepare the inputs for the model. The processor combines an image that prepares the image for the model by resizing and normalizing it, and a tokenizer that takes care of the text inputs.","metadata":{}},{"cell_type":"code","source":"candidate_labels=[\"tree\",\"car\",\"bike\",\"cat\"]\ninputs=processor(images=image, text=candidate_labels, return_tensors=\"pt\", padding=True)\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:53:29.625425Z","iopub.execute_input":"2023-12-11T11:53:29.626488Z","iopub.status.idle":"2023-12-11T11:53:29.653417Z","shell.execute_reply.started":"2023-12-11T11:53:29.626445Z","shell.execute_reply":"2023-12-11T11:53:29.652263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pass the inputs through the model, and post-process the results:","metadata":{}},{"cell_type":"code","source":"import torch\n\nwith torch.no_grad():\n    outputs=model(**inputs)\n    \nlogits=outputs.logits_per_image[0]\nprobs=logits.softmax(dim=-1).numpy()\nscores=probs.tolist()\n\nresult=[\n    {\"score\": score, \"label\": candidate_label}\n    for score, candidate_label in sorted(zip(probs,candidate_labels), key=lambda x:-x[0])\n]\n\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:53:44.703015Z","iopub.execute_input":"2023-12-11T11:53:44.703391Z","iopub.status.idle":"2023-12-11T11:53:47.181586Z","shell.execute_reply.started":"2023-12-11T11:53:44.70336Z","shell.execute_reply":"2023-12-11T11:53:47.180385Z"},"trusted":true},"execution_count":null,"outputs":[]}]}