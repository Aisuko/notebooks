{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n**Monocular Depth Estimation** is the task of estimating the depth value (distance relative to the camera) of each pixel given a single (monocular) RGB image. This challenging task is a ket prerequisite for determining scene understanding for applications such as 3D scene reconstruction, autonomous driving, and AR. State-of-the-art methods usually fall into one of two categories:\n\n* Designing a complex network that is powerful enough to directly regress the depth map\n* Splitting the input inot bins or windows to reduce computational complexity\n\nThe most popular benchmarks are the KITTI and NYUv2 datasets. Models are typically evaluated using RMSE or absolute relative error.\n\nIn this notebook, we will illustrate how to load and visualize depth map data, run monocular depth estimation models, and evaluate depth predictions. We will do so using data from the [SUN RGB-D](https://rgbd.cs.princeton.edu/) dataset.\n\nWe will use the Hugginf Face transformers and diffuers librarues ofr inference. FityOne for data management and visualization, and scikit-image for evaluation metrics.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers==4.36.2\n!pip install diffusers==0.23.1\n!pip install fiftyone==0.23.4\n!pip install scikit-image==0.22.0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and Visualizing SUN-RGBD Depth Data\n\nSUN RGB-D is one of the most popular datasets for monocular depth estimation and semantic segementation tasks. It contains images from the NYU depth v2, Berkeley B3DO, and SUN3D datasets. Here we will only use the NYU depth v2 positions. See [here](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2).\n\n\n## Downloading the Raw Data","metadata":{}},{"cell_type":"code","source":"!curl -o sunrgbd.zip https://rgbd.cs.princeton.edu/data/SUNRGBD.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will only be using the depth images, so we will only use the RGB images and the depth images(stored in the `depth_bfx` sub-directories).","metadata":{}},{"cell_type":"code","source":"!unzip sunrgbd.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the Dataset\n\nWe are just interested in getting the point across, we will restrict ourselves to the first 20 samples, which are all from the NYU Depth v2 portion of the dataset:","metadata":{}},{"cell_type":"code","source":"from glob import glob\nimport fiftyone as fo\nimport numpy as np\n\n## create, name and persist the dataset\ndataset=fo.Dataset(name='SUNRGBD-20', persistent=True)\n\n## pick out first 20 scenes\nscene_dirs=glob('SUNRGBD/kv1/NYUdata/*')[:20]\n\nsamples=[]\n\nfor scene_dir in scene_dirs:\n    # get image file path from scene directory\n    image_path=glob(f'{scene_dir}/image/*')[0]\n    depth_path=glob(f'{scene_dir}/depth_bfx/*')[0]\n    \n    depth_map=np.array(Image.open(depth_path))\n    depth_map=(depth_map*255/np.max(depth_map)).astype('unit8')\n    \n    ## create sample\n    sample=fo.Sample(\n        filepath=image_path,\n        gt_depth=fo.Heatmap(map=depth_map),\n    )\n    \n    samples.append(sample)\n\n## Add samples to dataset\ndataset.add_samples(samples)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are storing the depth maps as heatmaps. Everything is represented in terms of normalized, relative distances, where 255 reoresents the maximum distance in the scene and 0 represents the minimum distance in the scene. This is a common way to represent depth maps, although it is far from the only way to do so.\n\n## Visualizing Ground Truth Data","metadata":{}},{"cell_type":"code","source":"# Check the localhost:5151 in broser\nsession=fo.launch_app(dataset, auto=False)","metadata":{},"execution_count":null,"outputs":[]}]}