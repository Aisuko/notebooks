{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/object-detection?scriptVersionId=164769914\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nObject detection is the computer vision task of detecting instances in an image. Object detection models reveive an image as input and output coordinates of the bounding boxes and associated labels of the detected objects. An image can contain multiple objects, each with its own bounding box and a label, and each object can be present in different parts of an image. This task is commonly used in autonomous driving for detecting things like pedestrians, road signs, and traffic lights. Other application include counting objects in images, image search, and more.\n\nHere we are going to use model [`End-to-End Object Detection with Transformers`](https://arxiv.org/abs/2005.12872), a model that combines a convolutional backbone with an encoder-decoder Transformer, on the dataset with `Object Detection` label.\n\n\n- **timm:** PyTorch Image Models collection.\n- **albumentations:** Albumentations is a Python library for image augmentation. It is used in deep learning and computer vision task to increase the quality of trained model.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.35.2\n!pip install datasets==2.15.0\n!pip install evaluate==0.4.1\n!pip install timm==0.9.12\n!pip install albumentations==1.3.1\n!pip install pycocotools==2.0.7","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:52:14.731281Z","iopub.execute_input":"2024-02-28T23:52:14.731842Z","iopub.status.idle":"2024-02-28T23:53:36.981929Z","shell.execute_reply.started":"2024-02-28T23:52:14.731802Z","shell.execute_reply":"2024-02-28T23:53:36.980775Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# login(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tune-models\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune model distilbert base uncased\"\nos.environ[\"WANDB_NAME\"] = \"ft-detr-with-cppe-5\"","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:36.984261Z","iopub.execute_input":"2024-02-28T23:53:36.984577Z","iopub.status.idle":"2024-02-28T23:53:37.801409Z","shell.execute_reply.started":"2024-02-28T23:53:36.984547Z","shell.execute_reply":"2024-02-28T23:53:37.800578Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Loading the CPPE-5 dataset\n\nThe CPPE-5 dataset contains images with annotations identifying medical personal protective equipment (PPE) in the context of the COVID-19 pandemic.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ncppe5=load_dataset(\"cppe-5\")\ncppe5","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:37.802635Z","iopub.execute_input":"2024-02-28T23:53:37.803018Z","iopub.status.idle":"2024-02-28T23:53:40.414008Z","shell.execute_reply.started":"2024-02-28T23:53:37.802979Z","shell.execute_reply":"2024-02-28T23:53:40.411242Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m cppe5\u001b[38;5;241m=\u001b[39m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcppe-5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m cppe5\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2128\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2124\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2125\u001b[0m )\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2128\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1814\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1813\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1814\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1823\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1511\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1507\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1508\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1509\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1510\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1511\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1515\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1478\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1475\u001b[0m             msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. If the repo is private or gated, make sure to log in with `huggingface-cli login`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1476\u001b[0m         )\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1478\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [sibling\u001b[38;5;241m.\u001b[39mrfilename \u001b[38;5;28;01mfor\u001b[39;00m sibling \u001b[38;5;129;01min\u001b[39;00m dataset_info\u001b[38;5;241m.\u001b[39msiblings]:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1481\u001b[0m         path,\n\u001b[1;32m   1482\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1486\u001b[0m     )\u001b[38;5;241m.\u001b[39mget_module()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1452\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1450\u001b[0m hf_api \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1452\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m \u001b[43mhf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# noqa catch any exception of hf_hub and consider that the dataset doesn't exist\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1460\u001b[0m         e,\n\u001b[1;32m   1461\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1465\u001b[0m         ),\n\u001b[1;32m   1466\u001b[0m     ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1985\u001b[0m, in \u001b[0;36mHfApi.dataset_info\u001b[0;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[1;32m   1982\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblobs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(path, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m-> 1985\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1986\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetInfo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:330\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/api/datasets/cppe-5"],"ename":"HfHubHTTPError","evalue":"503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/api/datasets/cppe-5","output_type":"error"}]},{"cell_type":"markdown","source":"We will see that this dataset already comes with a training set containing 1000 images and a test set with 29 images. The examples in the dataset have the following fields:\n- `image_id`: the example image id\n- `image`: a PIL.Image.Image object containing the image\n- `width`: width of the image\n- `height`: height of the image\n- `objects`: a dictionary containing bounding box metadata for the objects in the image\n   - `id`: the annotation id\n   - `area`: the area of the bounding box\n   - `bbox`: the objecte's bounding box (in the (COCO)Common Objects in Context format)\n   - `category`: the object's category, with possible values including\n      - Coverall (0)\n      - Face_Shield (1)\n      - Gloves (2)\n      - Goggles (3)\n      - Mask (4)","metadata":{}},{"cell_type":"code","source":"cppe5[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.415034Z","iopub.status.idle":"2024-02-28T23:53:40.416124Z","shell.execute_reply.started":"2024-02-28T23:53:40.415852Z","shell.execute_reply":"2024-02-28T23:53:40.415878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing data\n\nThe `bbox` field follows the COCO format, which is the format that the DETR model expects. However, the grouping of the fields inside `objects` differs from the annotation format `DETR` requires. We will need to apply some preprocessing transformations before using this data for training.\n\n## Visualization Sample\n\nTo get an even better understanding of the data, visualize an example in the dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image, ImageDraw\n\nimage=cppe5[\"train\"][0][\"image\"]\nannotations=cppe5[\"train\"][0][\"objects\"]\ndraw=ImageDraw.Draw(image)\n\ncategories=cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n\nid2label={index: x for index, x in enumerate(categories, start=0)}\nlabel2id={v:k for k,v in id2label.items()}\n\nfor i in range(len(annotations[\"id\"])):\n    box=annotations[\"bbox\"][i]\n    class_idx=annotations[\"category\"][i]\n    x,y,w,h=tuple(box)\n    draw.rectangle((x,y,x+w,y+h), outline=\"red\", width=1)\n    draw.text((x,y), id2label[class_idx], fill=\"white\")\n\nimage","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.41769Z","iopub.status.idle":"2024-02-28T23:53:40.418174Z","shell.execute_reply.started":"2024-02-28T23:53:40.41793Z","shell.execute_reply":"2024-02-28T23:53:40.417952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To visualize the bounding boxes with associated labels, we can get the labels from the dataset's metadata, specifically the `category` field. We will also want to create dictionaries that map a label id to a label class(id2label) and the other way around(label2id). We can use them later when setting up the model. Including these maps will make our model reusable by others if you share it on the Hugging Face Hub.\n\nAs a final step of getting familiar with the data, explore it for potential issues. One common problem with datasets for object detection is bounding boxes that \"sterch\" beyond the edge of the image. Such \"runaway\" bounding boxes can raise errors during training and should be addressed at this stage. There are a few examples with this issue in this dataset. To keep things simple, we remove these images from the data.","metadata":{}},{"cell_type":"code","source":"remove_idx=[590, 821, 822, 875, 876,878,879]\nkeep=[i for i in range(len(cppe5[\"train\"])) if i not in remove_idx]\ncppe5[\"train\"]=cppe5[\"train\"].select(keep)\ncppe5","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.419726Z","iopub.status.idle":"2024-02-28T23:53:40.420126Z","shell.execute_reply.started":"2024-02-28T23:53:40.419921Z","shell.execute_reply":"2024-02-28T23:53:40.419939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading AutoImageProcessor\n\nHere we are going to use AutoImageProcessor takes care of processing image data to create pixel_values, pixel_mask, and labels that a DETR model can train with. The image processor has attributes:\n\n- image_mean=[0.485, 0.456, 0.406]\n- image_std=[0.229, 0.224, 0.225]\n\nThese are the mean and standard deviation used to normalize images during the model pre-training and crucial to replicate when doing inference or finetuning a pre-trained image model.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoImageProcessor\n\nmodel_checkpoint=\"facebook/detr-resnet-50\"\nimage_processor=AutoImageProcessor.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.421571Z","iopub.status.idle":"2024-02-28T23:53:40.421945Z","shell.execute_reply.started":"2024-02-28T23:53:40.421761Z","shell.execute_reply":"2024-02-28T23:53:40.421779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we passing the images to the image_processor, apply two preprocessing transformations to the dataset:\n\n- Augmenting images\n- Reformatting annotations to meet DETR expectations\n\n\n## Augmenting Images\n\nTo make sure the model does not overfit on the training data, we can apply image augmentation with any data augmentation library. Here we use Albumentations, this library ensures that transformations affect the image and update the bounding boxes accordingly. Here is an [example](https://huggingface.co/docs/datasets/object_detection). We apply the same approach here, resize each image to (480, 480), flip it horizontally, and brighten it.","metadata":{}},{"cell_type":"code","source":"import albumentations\nimport numpy as np\nimport torch\n\ntransform=albumentations.Compose(\n    [\n        albumentations.Resize(480, 480),\n        albumentations.HorizontalFlip(p=1.0),\n        albumentations.RandomBrightnessContrast(p=1.0),\n    ],\n    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.422954Z","iopub.status.idle":"2024-02-28T23:53:40.423381Z","shell.execute_reply.started":"2024-02-28T23:53:40.423165Z","shell.execute_reply":"2024-02-28T23:53:40.423183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reformatting Annotations\n\nThe `image_processor` expects the annotations to be in the following format `{'image_id':int, 'annotations': List[Dict]}`, where each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:","metadata":{}},{"cell_type":"code","source":"def formatted_anns(image_id, category, area, bbox):\n    annotations=[]\n    for i in range(0, len(category)):\n        new_ann={\n            \"image_id\":image_id,\n            \"category_id\":category[i],\n            \"isCrowd\":0,\n            \"area\":area[i],\n            \"bbox\": list(bbox[i]),\n        }\n        annotations.append(new_ann)\n    return annotations","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.425352Z","iopub.status.idle":"2024-02-28T23:53:40.425702Z","shell.execute_reply.started":"2024-02-28T23:53:40.42553Z","shell.execute_reply":"2024-02-28T23:53:40.425548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's combine the image and annotation transformations to use on a batch of examples:","metadata":{}},{"cell_type":"code","source":"# transforming a batch\ndef transform_aug_ann(examples):\n    image_ids=examples[\"image_id\"]\n    images, bboxes, area, categories=[],[],[],[]\n    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n        image=np.array(image.convert(\"RGB\"))[:,:,::-1]\n        out=transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n        \n        area.append(objects[\"area\"])\n        images.append(out[\"image\"])\n        bboxes.append(out[\"bboxes\"])\n        categories.append(out[\"category\"])\n\n    targets=[\n        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n    ]\n    \n    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n\n\ncppe5[\"train\"]=cppe5[\"train\"].with_transform(transform_aug_ann)\ncppe5[\"train\"][15]","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.427011Z","iopub.status.idle":"2024-02-28T23:53:40.427387Z","shell.execute_reply.started":"2024-02-28T23:53:40.427186Z","shell.execute_reply":"2024-02-28T23:53:40.427223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batch Data\n\nWe have successfully augmented the individual images and prepared their annotations. However, preprocessing isn't complete yet. In the final step, create a custom collate_fn to batch images together. Pad images to the largest image in a batch, and create a corresponding pixel_mask to indicate which pixels are real(1)  and which are padding(0).","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    pixel_values=[item[\"pixel_values\"] for item in batch]\n    encoding=image_processor.pad(pixel_values, return_tensors=\"pt\")\n    labels=[item[\"labels\"] for item in batch]\n    batch={}\n    batch[\"pixel_values\"]=encoding[\"pixel_values\"]\n    batch[\"pixel_mask\"]=encoding[\"pixel_mask\"]\n    batch[\"labels\"]=labels\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.428401Z","iopub.status.idle":"2024-02-28T23:53:40.428724Z","shell.execute_reply.started":"2024-02-28T23:53:40.428562Z","shell.execute_reply":"2024-02-28T23:53:40.428577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForObjectDetection\n\nmodel=AutoModelForObjectDetection.from_pretrained(\n    model_checkpoint,\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\n\nprint(model.config)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.430048Z","iopub.status.idle":"2024-02-28T23:53:40.43043Z","shell.execute_reply.started":"2024-02-28T23:53:40.430242Z","shell.execute_reply":"2024-02-28T23:53:40.43026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_checkpointing=4,\n    num_train_epochs=5,\n    fp16=True,\n    save_steps=200,\n    logging_steps=50,\n    learning_rate=1e-5,\n    weight_decay=1e-4,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=False,\n    report_to=\"wandb\",\n    run_name=os.getenv(\"WANDB_NAME\"),\n)\n\ntrainer=Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    train_dataset=cppe5[\"train\"],\n    tokenizer=image_processor,\n)\n\n# Related to the issue https://github.com/huggingface/transformers/issues/13197\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.433624Z","iopub.status.idle":"2024-02-28T23:53:40.434024Z","shell.execute_reply.started":"2024-02-28T23:53:40.43384Z","shell.execute_reply":"2024-02-28T23:53:40.433858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_processor.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntrainer.push_to_hub(os.getenv(\"WANDB_NAME\"))","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.436047Z","iopub.status.idle":"2024-02-28T23:53:40.436537Z","shell.execute_reply.started":"2024-02-28T23:53:40.436275Z","shell.execute_reply":"2024-02-28T23:53:40.436301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate\n\nObject detection models are commonly evaluated with a set of COCO-style metrics but here wel will use the one from torchvision to evaluate the final model.\n\nWe need to to prepare a ground truth COCO dataset. The API to build a COCO dataset requires the data to be stored in a certain format, so we will need to save images and annotations to disk first. Here are three major steps:\n\n- Prepare the cppe5[\"test\"] set: format the annotations and save the data to disk.","metadata":{}},{"cell_type":"code","source":"import json\nimport torchvision\n\n\n# foramt annotations the same as for training, no need for data augmentation\ndef val_formatted_anns(image_id, objects):\n    annotations=[]\n    for i in range(0, len(objects[\"id\"])):\n        new_ann={\n            \"id\": objects[\"id\"][i],\n            \"category_id\": objects[\"category\"][i],\n            \"iscrowd\":0,\n            \"image_id\":image_id,\n            \"area\":objects[\"area\"][i],\n            \"bbox\":objects[\"bbox\"][i],\n        }\n        annotations.append(new_ann)\n    return annotations\n\n\n# Save images and annotations into the files\ndef save_cppe5_annotation_file_images(cppe5):\n    output_json={}\n    path_output_cppe5=f\"{os.getcwd()}/cppe5/\"\n    \n    if not os.path.exists(path_output_cppe5):\n        os.makedirs(path_output_cppe5)\n    \n    path_anno=os.path.join(path_output_cppe5, \"cppe5_ann.json\")\n    categories_json=[{\"supercategory\": \"none\", \"id\":id, \"name\": id2label[id]} for id in id2label]\n    output_json[\"images\"]=[]\n    output_json[\"annotations\"]=[]\n    for example in cppe5:\n        ann=val_formatted_anns(example[\"image_id\"], example[\"objects\"])\n        output_json[\"images\"].append(\n            {\n                \"id\": example[\"image_id\"],\n                \"width\":example[\"image\"].width,\n                \"height\": example[\"image\"].height,\n                \"file_name\":f\"{example['image_id']}.png\",\n            }\n        )\n        output_json[\"annotations\"].extend(ann)\n    output_json[\"categories\"]=categories_json\n    \n    with open(path_anno, \"w\") as file:\n        json.dump(output_json, file, ensure_ascii=False, indent=4)\n    \n    for im, img_id in zip(cppe5[\"image\"], cppe5[\"image_id\"]):\n        path_img=os.path.join(path_output_cppe5, f\"{img_id}.png\")\n        im.save(path_img)\n    \n    return path_output_cppe5, path_anno\n\n\n\n# Preparing an instance of a CocoDetection class that can be used with cocoevaluator.\nclass CocoDetection(torchvision.datasets.CocoDetection):\n    def __init__(self, img_folder, image_processor, ann_file):\n        super().__init__(img_folder, ann_file)\n        self.image_processor=image_processor\n        \n    def __getitem__(self, idx):\n        img, target=super(CocoDetection, self).__getitem__(idx)\n        \n        image_id=self.ids[idx]\n        target={\"image_id\": image_id,\"annotations\": target}\n        encoding=self.image_processor(images=img, annotations=target, return_tensors=\"pt\")\n        pixel_values=encoding[\"pixel_values\"].squeeze()\n        target=encoding[\"labels\"][0]\n        \n        return {\"pixel_values\":pixel_values, \"labels\":target}\n\n\nim_processor=AutoImageProcessor.from_pretrained(os.getenv(\"WANDB_NAME\"))\n\npath_output_cppe5, path_anno=save_cppe5_annotation_file_images(cppe5[\"test\"])\ntest_ds_coco_format=CocoDetection(path_output_cppe5, im_processor, path_anno)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.438861Z","iopub.status.idle":"2024-02-28T23:53:40.43931Z","shell.execute_reply.started":"2024-02-28T23:53:40.439088Z","shell.execute_reply":"2024-02-28T23:53:40.439107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, load the metrics and run the evaluation:","metadata":{}},{"cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\n\nmodel=AutoModelForObjectDetection.from_pretrained(os.getenv(\"WANDB_NAME\"))\nmodule=evaluate.load(\"ybelkada/cocoevaluate\", coco=test_ds_coco_format.coco)\nval_dataloader=torch.utils.data.DataLoader(\n    test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn\n)\n\nwith torch.no_grad():\n    for idx, batch in enumerate(tqdm(val_dataloader)):\n        pixel_values=batch[\"pixel_values\"]\n        pixel_mask=batch[\"pixel_mask\"]\n        \n        labels=[\n            {k: v for k,v in t.items()} for t in batch[\"labels\"]\n        ]\n        \n        outputs=model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n        \n        orig_target_sizes=torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n        results=im_processor.post_process(outputs, orig_target_sizes)\n        \n        module.add(prediction=results, reference=labels)\n        del batch\n\nresults=module.compute()\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.440713Z","iopub.status.idle":"2024-02-28T23:53:40.441038Z","shell.execute_reply.started":"2024-02-28T23:53:40.440868Z","shell.execute_reply":"2024-02-28T23:53:40.440882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nimport requests\n\nurl=\"https://i.imgur.com/2lnWoly.jpg\"\nimage=Image.open(requests.get(url, stream=True).raw)\n\nobj_detector=pipeline(\"object-detection\", model=os.getenv(\"WANDB_NAME\"))\nobj_detector(image)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.442588Z","iopub.status.idle":"2024-02-28T23:53:40.442939Z","shell.execute_reply.started":"2024-02-28T23:53:40.442766Z","shell.execute_reply":"2024-02-28T23:53:40.442783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also manually replicate the results of the pipeline.","metadata":{}},{"cell_type":"code","source":"image_processor=AutoImageProcessor.from_pretrained(os.getenv(\"WANDB_NAME\"))\nmodel=AutoModelForObjectDetection.from_pretrained(os.getenv(\"WANDB_NAME\"))\n\nwith torch.no_grad():\n    inputs=image_processor(images=image, return_tensors=\"pt\")\n    outputs=model(**inputs)\n    target_sizes=torch.tensor([image.size[::-1]])\n    results=image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n\nfor  score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box=[round(i,2) for i in box.tolist()]\n    print(\n        f\"Detected {model.config.id2label[label.item()]} with confidence\"\n        f\"{round(score.item(), 3)} at location {box}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.444523Z","iopub.status.idle":"2024-02-28T23:53:40.444872Z","shell.execute_reply.started":"2024-02-28T23:53:40.4447Z","shell.execute_reply":"2024-02-28T23:53:40.444717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw=ImageDraw.Draw(image)\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box=[round(i,2) for i in box.tolist()]\n    x,y,x2,y2=tuple(box)\n    draw.rectangle((x,y,x2,y2), outline=\"red\", width=1)\n    draw.text((x,y), model.config.id2label[label.item()], fill=\"white\")\n\nimage","metadata":{"execution":{"iopub.status.busy":"2024-02-28T23:53:40.446064Z","iopub.status.idle":"2024-02-28T23:53:40.446892Z","shell.execute_reply.started":"2024-02-28T23:53:40.44671Z","shell.execute_reply":"2024-02-28T23:53:40.446729Z"},"trusted":true},"execution_count":null,"outputs":[]}]}