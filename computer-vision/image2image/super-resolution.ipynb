{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/super-resolution?scriptVersionId=164773906\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nImage-to-Image task is the task where an application receives an image and outputs another image. This has various subtasks, including:\n\n- image enhancement,like(super resolution, low light enhancement, deraining etc.)\n- imgae inpainting\n- etc.\n\nHere, we are going to use an image-to-image pipeline for super resolution task, and run image-to-image models for same task without a pipeline.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n!pip install transformers==4.35.2","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:58:54.52563Z","iopub.execute_input":"2023-12-11T23:58:54.52589Z","iopub.status.idle":"2023-12-11T23:59:07.109951Z","shell.execute_reply.started":"2023-12-11T23:58:54.525866Z","shell.execute_reply":"2023-12-11T23:59:07.108845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\npipe=pipeline(task=\"image-to-image\", model=\"caidas/swin2SR-lightweight-x2-64\", device=\"cuda\")\npipe.enable_cpu_offloading()\nprint(pipe)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:59:07.111893Z","iopub.execute_input":"2023-12-11T23:59:07.112216Z","iopub.status.idle":"2023-12-11T23:59:30.982568Z","shell.execute_reply.started":"2023-12-11T23:59:07.112187Z","shell.execute_reply":"2023-12-11T23:59:30.981535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport requests\n\nurl=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/cat.jpg\"\nimage=Image.open(requests.get(url, stream=True).raw)\nprint(image.size)\n\nimage","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:01:39.801889Z","iopub.execute_input":"2023-12-12T00:01:39.802851Z","iopub.status.idle":"2023-12-12T00:01:39.977209Z","shell.execute_reply.started":"2023-12-12T00:01:39.802809Z","shell.execute_reply":"2023-12-12T00:01:39.976312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Upscaled the Image\n\nWe can now do inference with the pipeline. We will get an upscaled version of the cat image.","metadata":{}},{"cell_type":"code","source":"upscaled=pipe(image)\nprint(upscaled.size)\n\nupscaled","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:01:42.681991Z","iopub.execute_input":"2023-12-12T00:01:42.682928Z","iopub.status.idle":"2023-12-12T00:01:44.743487Z","shell.execute_reply.started":"2023-12-12T00:01:42.682896Z","shell.execute_reply":"2023-12-12T00:01:44.742582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Without Pipeline","metadata":{}},{"cell_type":"code","source":"from transformers import Swin2SRForImageSuperResolution, Swin2SRImageProcessor\n\nmodel=Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-lightweight-x2-64\").to(\"cuda\")\nprocessor = Swin2SRImageProcessor(\"caidas/swin2SR-lightweight-x2-64\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:59:37.808848Z","iopub.execute_input":"2023-12-11T23:59:37.809208Z","iopub.status.idle":"2023-12-11T23:59:38.197562Z","shell.execute_reply.started":"2023-12-11T23:59:37.809173Z","shell.execute_reply":"2023-12-11T23:59:38.196836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pipeline asbtracts away the preprocessing and postprocessing steps that we have to do ourselves. We will pass the image to the processor and then move the pixel values to GPU.","metadata":{}},{"cell_type":"code","source":"pixel_values=processor(image, return_tensors=\"pt\").pixel_values\nprint(pixel_values.shape)\n\npixel_values=pixel_values.to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:59:38.198767Z","iopub.execute_input":"2023-12-11T23:59:38.199128Z","iopub.status.idle":"2023-12-11T23:59:38.212349Z","shell.execute_reply.started":"2023-12-11T23:59:38.199089Z","shell.execute_reply":"2023-12-11T23:59:38.211638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now infer the image by passing pixel values to the model.","metadata":{}},{"cell_type":"code","source":"import torch\n\nwith torch.no_grad():\n    outputs=model(pixel_values)\n\noutputs","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:02:44.404642Z","iopub.execute_input":"2023-12-12T00:02:44.405383Z","iopub.status.idle":"2023-12-12T00:02:46.501273Z","shell.execute_reply.started":"2023-12-12T00:02:44.405339Z","shell.execute_reply":"2023-12-12T00:02:46.50041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization the Image\n\nWe need to get the reconstruction and post-process it for visualization.","metadata":{}},{"cell_type":"code","source":"outputs.reconstruction.data.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:59:39.971827Z","iopub.execute_input":"2023-12-11T23:59:39.972188Z","iopub.status.idle":"2023-12-11T23:59:39.979586Z","shell.execute_reply.started":"2023-12-11T23:59:39.972155Z","shell.execute_reply":"2023-12-11T23:59:39.978716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to squeeze the output and get rid of axis 0, clip the values, then convert it to be numpy float. Then we will arrange axes to have the shape [1072,880], and finally, bring the output back to range [0,255].","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# squeeze, take to CPU and clip the values\noutput=outputs.reconstruction.data.squeeze().cpu().clamp_(0,1).numpy()\n# rearrange the axes\noutput=np.moveaxis(output, source=0, destination=-1)\n# bring values back to pixel values range\noutput=(output*255.0).round().astype(np.uint8)\nImage.fromarray(output)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:03:41.646706Z","iopub.execute_input":"2023-12-12T00:03:41.647697Z","iopub.status.idle":"2023-12-12T00:03:41.975769Z","shell.execute_reply.started":"2023-12-12T00:03:41.647664Z","shell.execute_reply":"2023-12-12T00:03:41.974893Z"},"trusted":true},"execution_count":null,"outputs":[]}]}