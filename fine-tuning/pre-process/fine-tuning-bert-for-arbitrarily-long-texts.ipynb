{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn the [Introducing pro-precess of fine-tuning a llm step by steps](https://www.kaggle.com/code/aisuko/introducing-pro-process-of-ft-a-llm-step-by-steps), we tokenzie the text, and truncate all the tokens to 512 tokens. In this notebook, we will train the model with the data in native PyTorch.\n\nThe main idea is comes from [GitHub issue #27](https://github.com/google-research/bert/issues/27).\n\nRecall that to apply the fine-tuned classifier model to a single long text in the previously notebook above. We first tokenize the entire sequence, then split it into chunks, get the model prediction for each chunk and calculate the mean/max of predictions. There is no problem in doing it sequenctially, that is:\n\n* Put the 1st chunk into the model,and get 1st prediction\n* Put the 2st chunk into the model, and get 2and prediction\n* so on...\n* Take the mean/max of these predictions and stop\n\nHowever, training it sequentially on each chunk leads to a myriad of problems and questions:\n\n* Put the 1 st chunk of the 1st text to the model, calculate the loss of the prediction and the label...\n* What label?\n* We have only one binary label for the entire text...Then maybe run backpropagation? But when?\n* Should we update the model weights after each chunk?\n\nInstead, we must do it all at once by putting all the chunks into on mini-batch. This solves all the problems:\n* From K chunks obtained for the 1st text, create 1 mini-batch and obtain K predictions\n* Pool the predictions using the mean/max function to obtain a single prediction for the entire text\n* Calculate the loss between this single prediction for the entire text\n* Run backpropagation. Be careful to make sure that all the tensor oprtations are done on tensors with attached gradients before running `loss.backward()`.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers==4.36.2\n!pip install datasets==2.15.0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import all the code in the previously notebook.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import Tensor\n\ndef split_overlapping(tensor, chunk_size, stride, minimal_chunk_length=5):\n    result=[tensor[i:i+chunk_size] for i in range(0, len(tensor), stride)]\n    if len(result)>1:\n        result=[x for x in result if len(x)>=minimal_chunk_length]\n    return result\n\ndef add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks):\n    \"\"\"\n    Adds special CLS token (token id =101) at the beginning.\n    Adds SEP token (token id =102) at the end of each chunk.\n    Adds corresponding attention masks equal to 1 (attention mask is boolean)\n    \"\"\"\n    \n    for i in range(len(input_id_chunks)):\n        # adding CLS (token id 101) and SEP (token id 102) tokens\n        input_id_chunks[i]=torch.cat([Tensor([101]), input_id_chunks[i], Tensor([102])])\n        # adding attention masks corresponding to special tokens\n        mask_chunks[i]=torch.cat([Tensor([1]), mask_chunks[i], Tensor([1])])\n\ndef add_padding_tokens(input_id_chunks, mask_chunks):\n    \"\"\"\n    Adds padding tokens (token id=0) at the end to make sure that all chunks have exactly 512 tokens\n    \"\"\"\n    for i in range(len(input_id_chunks)):\n        # get required padding length\n        pad_len=512-input_id_chunks[i].shape[0]\n        # check if tensor length satisfies required chunk size\n        if pad_len>0:\n            # if padding length is more than 0, we must add padding\n            input_id_chunks[i]=torch.cat([input_id_chunks[i], Tensor([0]*pad_len)])\n            mask_chunks[i]=torch.cat([mask_chunks[i], Tensor([0]*pad_len)])\n\ndef stack_tokens_from_all_chunks(input_id_chunks, mask_chunks):\n    \"\"\"\n    Reshapes data to a form compatible with BERT model input.\n    \"\"\"\n    input_ids=torch.stack(input_id_chunks)\n    attention_mask=torch.stack(mask_chunks)\n    \n    return input_ids.long(), attention_mask.int()\ndef tokenize_whole_text(text, tokenizer):\n    \"\"\"Tokenizes the entire text without truncation and without special tokens.\"\"\"\n    tokens = tokenizer(text, add_special_tokens=False, truncation=False, return_tensors=\"pt\")\n    return tokens\n\n\ndef tokenize_text_with_truncation(text, tokenizer, maximal_text_length):\n    \"\"\"Tokenizes the text with truncation to maximal_text_length and without special tokens.\"\"\"\n    tokens = tokenizer(\n        text, add_special_tokens=False, max_length=maximal_text_length, truncation=True, return_tensors=\"pt\"\n    )\n    return tokens\n\ndef split_tokens_into_smaller_chunks(\n    tokens,\n    chunk_size,\n    stride,\n    minimal_chunk_length,\n):\n    \"\"\"Splits tokens into overlapping chunks with given size and stride.\"\"\"\n    input_id_chunks = split_overlapping(tokens[\"input_ids\"][0], chunk_size, stride, minimal_chunk_length)\n    mask_chunks = split_overlapping(tokens[\"attention_mask\"][0], chunk_size, stride, minimal_chunk_length)\n    return input_id_chunks, mask_chunks\n\ndef preprocess_func(\n    text,\n    tokenizer,\n    chunk_size,\n    stride,\n    minimal_chunk_length,\n    maximal_text_length\n):\n    \"\"\"Transforms (the entire) text to model input of BERT model.\"\"\"\n    if maximal_text_length:\n        tokens=tokenize_text_with_truncation(text, tokenizer, maximal_text_length)\n    else:\n        tokens=tokenize_whole_text(text, tokenizer)\n    \n    input_id_chunks, mask_chunks=split_tokens_into_smaller_chunks(tokens, chunk_size, stride, minimal_chunk_length)\n    add_special_tokens_at_beginning_and_end(input_id_chunks, mask_chunks)\n    add_padding_tokens(input_id_chunks, mask_chunks)\n    input_ids, attention_mask=stack_tokens_from_all_chunks(\n        input_id_chunks,\n        mask_chunks\n    )\n    return input_ids, attention_mask","metadata":{},"execution_count":null,"outputs":[]}]}