{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/fine-tuning-a-llama2-for-code-generation?scriptVersionId=160680290\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nLet's trying to fine-tune Llama2 on a the dataset which is included Python code solves a given task.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers==4.36.2\n!pip install datasets==2.15.0\n!pip install peft==0.7.1\n!pip install bitsandbytes==0.41.3\n!pip install accelerate==0.25.0\n!pip install trl==0.7.7\n!pip install tqdm==4.66.1\n# Although flash-attn is not supported in Kaggle env.However, we prepare the notebook for future usage.\n!pip install flash-attn==2.4.2","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:12:52.002637Z","iopub.execute_input":"2024-01-28T00:12:52.003056Z","iopub.status.idle":"2024-01-28T00:15:12.693546Z","shell.execute_reply.started":"2024-01-28T00:12:52.003012Z","shell.execute_reply":"2024-01-28T00:15:12.692169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tune-models-with-QLoRA\"\nos.environ[\"WANDB_NOTES\"] = \"Fine-tuning casual language models with QLoRA\"\nos.environ[\"WANDB_NAME\"] = \"fine-tuning-Llama2-with-pycode-instructions-with-QLoRA\"\nos.environ[\"MODEL_NAME\"] = \"meta-llama/Llama-2-7b-hf\"\nos.environ[\"DATASET_NAME\"]=\"iamtarun/python_code_instructions_18k_alpaca\"","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:15:12.696518Z","iopub.execute_input":"2024-01-28T00:15:12.696994Z","iopub.status.idle":"2024-01-28T00:15:13.643714Z","shell.execute_reply.started":"2024-01-28T00:15:12.696931Z","shell.execute_reply":"2024-01-28T00:15:13.642488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name=os.getenv(\"DATASET_NAME\")\n\ndataset=load_dataset(dataset_name, split=\"train[:100]\") #It can be a smaller slice for fit the lower GPU memory\nlen(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:26:10.546202Z","iopub.execute_input":"2024-01-28T00:26:10.547327Z","iopub.status.idle":"2024-01-28T00:26:11.646575Z","shell.execute_reply.started":"2024-01-28T00:26:10.547286Z","shell.execute_reply":"2024-01-28T00:26:11.645301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_instruction(sample):\n    return f\"\"\"Instruction:\n    Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n    \n    ### Task:\n    {sample['instruction']}\n    \n    ### Input:\n    {sample['input']}\n    \n    ### Response\n    {sample['output']}\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:15:44.21122Z","iopub.execute_input":"2024-01-28T00:15:44.212853Z","iopub.status.idle":"2024-01-28T00:15:44.218938Z","shell.execute_reply.started":"2024-01-28T00:15:44.212797Z","shell.execute_reply":"2024-01-28T00:15:44.217639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the model","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig, AutoModelForCausalLM\nimport torch\n\nbnb_config= BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel=AutoModelForCausalLM.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    quantization_config=bnb_config,\n    use_cache=False,\n    device_map='auto',\n    torch_dtype=torch.bfloat16\n)\n\nmodel.config","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:15:47.002923Z","iopub.execute_input":"2024-01-28T00:15:47.003364Z","iopub.status.idle":"2024-01-28T00:17:23.949741Z","shell.execute_reply.started":"2024-01-28T00:15:47.003328Z","shell.execute_reply":"2024-01-28T00:17:23.948548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.pretraining_tp=1\nmodel.get_memory_footprint()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:17:23.951511Z","iopub.execute_input":"2024-01-28T00:17:23.952068Z","iopub.status.idle":"2024-01-28T00:17:23.963626Z","shell.execute_reply.started":"2024-01-28T00:17:23.952028Z","shell.execute_reply":"2024-01-28T00:17:23.96256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, get_peft_model, prepare_model_for_kbit_training\n\n# to save memory\nmodel.gradient_checkpointing_enable()\nmodel.get_memory_footprint()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:20:17.65414Z","iopub.execute_input":"2024-01-28T00:20:17.654868Z","iopub.status.idle":"2024-01-28T00:20:17.670475Z","shell.execute_reply.started":"2024-01-28T00:20:17.654829Z","shell.execute_reply":"2024-01-28T00:20:17.669337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze the base model layers and cast layernorm in fp32\nmodel=prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:20:20.236849Z","iopub.execute_input":"2024-01-28T00:20:20.237456Z","iopub.status.idle":"2024-01-28T00:20:20.275132Z","shell.execute_reply.started":"2024-01-28T00:20:20.237403Z","shell.execute_reply":"2024-01-28T00:20:20.274043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType\n\npeft_config=LoraConfig(\n    # Alpha parameter for LoRA scaling\n    lora_alpha=16,\n    # Dropout probability for LoRA layers\n    lora_dropout=0.1,\n    # LoRA attention dimension\n    r=64,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\npeft_model=get_peft_model(model,peft_config)\npeft_model.get_memory_footprint()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:20:33.36192Z","iopub.execute_input":"2024-01-28T00:20:33.362931Z","iopub.status.idle":"2024-01-28T00:20:33.970075Z","shell.execute_reply.started":"2024-01-28T00:20:33.362894Z","shell.execute_reply":"2024-01-28T00:20:33.968946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n    trust_remote_code=False,\n    use_fast=True\n)\n\ntokenizer.pad_token=tokenizer.eos_token\ntokenizer.padding=\"right\"","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:20:36.532206Z","iopub.execute_input":"2024-01-28T00:20:36.532701Z","iopub.status.idle":"2024-01-28T00:20:37.33069Z","shell.execute_reply.started":"2024-01-28T00:20:36.532658Z","shell.execute_reply":"2024-01-28T00:20:37.329459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ntraining_args=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    num_train_epochs=1,\n    # Number of training steps (overrides num_train epochs)\n#     max_teps=-1,\n    per_device_train_batch_size=16, # 6 if use flash attention else 4\n    # Number of update steps to accumulate the gradients for\n    gradient_accumulation_steps=1,\n    # Enable gradient checkpointing\n    gradient_checkpointing=True,\n    # Optimizer to use\n    optim='paged_adamw_8bit',\n    # Log every X updates steps\n    logging_steps=25,\n    save_strategy=\"no\",\n    # Initial learning rate (AdamW optimizer)\n    learning_rate=2e-4,\n    # Weight decay to apply to all layers except bias/LayerNorm weights\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    # Maximum gradient normal(gradient clipping)\n    max_grad_norm=0.3,\n    # Ratio of steps for a linear warmup(from 0 to learning rate)\n    warmup_ratio=0.03,\n    # Group sequences into batches with same length\n    # Save memory and speeds up training considerably\n    group_by_length=True,\n    lr_scheduler_type='cosine',\n    disable_tqdm=False,\n    report_to=\"wandb\",\n    seed=42,\n    run_name=os.getenv(\"WANDB_NAME\")\n)\n\nsft_trainer=SFTTrainer(\n    model=peft_model,\n    train_dataset=dataset,\n    # Maximum sequence length to use\n    max_seq_length=2048,\n    tokenizer=tokenizer,\n    # Pack multiple short examples in the same input sequence to increase efficiency\n    packing=True,\n    formatting_func=format_instruction,\n    args=training_args,\n)\n\nsft_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:22:25.823296Z","iopub.execute_input":"2024-01-28T00:22:25.823714Z","iopub.status.idle":"2024-01-28T00:23:29.150923Z","shell.execute_reply.started":"2024-01-28T00:22:25.823681Z","shell.execute_reply":"2024-01-28T00:23:29.149043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sft_trainer.push_to_hub(os.getenv(\"WANDB_NAME\"))\ntokenizer.push_to_hub(os.getnev(\"WANDB_NAME\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:17:24.375351Z","iopub.status.idle":"2024-01-28T00:17:24.375903Z","shell.execute_reply.started":"2024-01-28T00:17:24.375627Z","shell.execute_reply":"2024-01-28T00:17:24.375654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import gc\n\ndel peft_model, model, trainer\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:15:14.140242Z","iopub.status.idle":"2024-01-28T00:15:14.140755Z","shell.execute_reply.started":"2024-01-28T00:15:14.140489Z","shell.execute_reply":"2024-01-28T00:15:14.140513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\n\npeft_config=PeftConfig.from_pretrained(\"aisuko/\"+os.getenv(\"WANDB_NAME\"))\nbase_model=AutoModelForCausalLM.from_pretrained(peft_config.base_)\npeft_model=PeftModel.from_pretrained(base_model,\"aisuko/\"+os.getenv(\"WANDB_NAME\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:15:14.143031Z","iopub.status.idle":"2024-01-28T00:15:14.143423Z","shell.execute_reply.started":"2024-01-28T00:15:14.143235Z","shell.execute_reply":"2024-01-28T00:15:14.143254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instrunction=\"Optimize a code snippet written in Python. The code snippet should create a list of numbers from 0 to 10 that are divisible by 2.\"\ninputs=\"\"\n\nprompt=f\"\"\"### Instruction:\nUse the Task below and the Input given to write the Response, which is a programming code that can solve the Task.\n\n### Task:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n\"\"\"\ninput_ids=tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\noutputs=model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)\n\ntokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T00:15:14.144697Z","iopub.status.idle":"2024-01-28T00:15:14.145129Z","shell.execute_reply.started":"2024-01-28T00:15:14.144887Z","shell.execute_reply":"2024-01-28T00:15:14.144905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Credit\n\n* https://pub.towardsai.net/fine-tuning-a-llama-2-7b-model-for-python-code-generation-865453afdf73\n* https://github.com/edumunozsala/llama-2-7B-4bit-python-coder/blob/main/Llama-2-finetune-qlora-python-coder.ipynb","metadata":{}}]}