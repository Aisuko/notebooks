{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1885658,"sourceType":"datasetVersion","datasetId":1123189}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nIn this notebook, we will try to fine-tune Mistal 7b for a multiclass classification task.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers==4.36.2\n!pip install accelerate==0.25.0\n!pip install evaluate==0.4.1\n!pip install datasets==2.15.0\n!pip install peft==0.7.1\n!pip install bitsandbytes==0.41.3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tune-models-with-QLoRA\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune model with QLoRA\"\nos.environ[\"WANDB_NAME\"] = \"ft-mistral-with-customize-ds-with-QLoRA\"\nos.environ[\"MODEL_NAME\"] = \"mistralai/Mistral-7B-v0.1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset\n\nHere we use the function `iterative_train_test_split` from skmultilearn. This creates an even split for unbalanced multilabel datasets for us. ","metadata":{}},{"cell_type":"code","source":"import csv\nimport random\nimport numpy as np\nfrom skmultilearn.model_selection import iterative_train_test_split\nfrom datasets import Dataset, DatasetDict\n\nrandom.seed(0)\n\nwith open('/kaggle/input/multilabel-classification-dataset/train.csv', newline='') as instance:\n    data=list(csv.reader(instance, delimiter=','))\n    header_row=data.pop(0)\n\n# shuffle data\nrandom.shuffle(data)\n\n# reshape\nidx, text, labels=list(\n    zip(*[(int(row[0]), \n        f'Title:{row[1].strip()}\\n\\nAbstract: {row[2].strip()}',\n        row[3:]) for row in data]))\nlabels=np.array(labels, dtype=int)\n\n# create label weights\nlabel_weights=1-labels.sum(axis=0)/labels.sum()\n\n# stratified train test split for multilabel datasets\nrow_ids=np.arange(len(labels))\ntrain_idx,y_train, val_idx, y_val=iterative_train_test_split(row_ids[:,np.newaxis], labels, test_size=0.1)\nx_train=[text[i] for i in train_idx.flatten()]\nx_val=[text[i] for i in val_idx.flatten()]\n\n# create dataset in hf format\nds=DatasetDict({\n    'train': Dataset.from_dict({'text':x_train,'labels':y_train}),\n    'val': Dataset.from_dict({'text':x_val,'labels':y_val})\n})\nds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading tokenizer and define preprocess function","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer=AutoTokenizer.from_pretrained(os.getenv('MODEL_NAME'))\ntokenizer.pad_token=tokenizer.eos_token\ntokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import functools\n\ndef preprocess_func(examples, tokenizer):\n    tokenized_inputs=tokenizer(examples['text'])\n    tokenized_inputs['labels']=examples['labels']\n    return tokenized_inputs\n\ntokenized_ds=ds.map(functools.partial(preprocess_func, tokenizer=tokenizer), batched=True)\ntokenized_ds=tokenized_ds.with_format('torch')\ntokenized_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the model\n\nWe download Mistral 7B and quantize it with nf4(QLoRA). And training it by using LoRA.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\nfrom peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n\n# quantization config\nquantization_config= BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16 #bfloat16 for special hardware, we use float16\n)\n\n# lora config\nlora_config=LoraConfig(\n    r=16, # the dimension of the low-rank matrices\n    lora_alpha=8, # scaling factor for LoRA activations vs pre-trained weight activations\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05, # dropout probability of the LoRA layers\n    bias='none',\n    task_type=TaskType.SEQ_CLS\n)\n\n# load model\nmodel=AutoModelForSequenceClassification.from_pretrained(\n    os.getenv('MODEL_NAME'),\n    quantization_config=quantization_config,\n    num_labels=labels.shape[1]\n)\n\nmodel=prepare_model_for_kbit_training(model)\nmodel=get_peft_model(model, lora_config)\nmodel.config.pad_token_id=tokenizer.pad_token_id\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\nBefore we do the training, we have to define some custom functions that our trianer will use, like: data collator and metrics.\n\n**Data Collator**\n\nWe need to tell the trainer how it should preprocess batches coming from the dataset before they can be passed to the model\n\n**Metrics**\n\nWe furthermore need pass a function to the trainer which defindes the evaluation metrics we want to compute in addition to the loss.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# define custom batch preprocessor\ndef collate_fn(batch, tokenizer):\n    dict_keys=['input_ids', 'attention_mask', 'labels']\n    d={k:[dic[k] for dic in batch] for k in dict_keys}\n    d['input_ids']=torch.nn.utils.rnn.pad_sequence(\n        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n    )\n    d['attention_mask']=torch.nn.utils.rnn.pad_sequence(\n        d['attention_mask'], batch_first=True, padding_value=0\n    )\n    d['labels']=torch.stack(d['labels'])\n    return d\n\n# define which metrics to compute for evaluation\ndef compute_metrics(p):\n    predictions,labels=p\n    f1_micro=f1_score(labels, predictions>0, average='micro')\n    f1_macro=f1_score(labels, predictions>0, average='macro')\n    f1_weighted=f1_score(labels, predictions>0, average='weighted')\n    return {\n        'f1_micro':f1_micro,\n        'f1_macro':f1_macro,\n        'f1_weighted': f1_weighted\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define customer trainer\n\nWe need to define a custom trainer class to able to calculate our multilabel loss which treats each output neuron as a binary classification instance. To be able to use our label weights for the loss we also need to define it as a class attribute in the `__init__` method so the `compute_loss` method has access to it.","metadata":{}},{"cell_type":"code","source":"# create custom trainer class to be able to pass label weights and calculate mutilabel loss\nfrom transformers import Trainer\nimport torch.nn.functional as F\n\nclass CustomTrainer(Trainer):\n    def __init__(self, label_weights, **kwargs):\n        super().__init__(**kwargs)\n        self.label_weights=label_weights\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels=inputs.pop(\"labels\")\n        \n        #forward pass\n        outputs=model(**inputs)\n        logits=outputs.get(\"logits\")\n        \n        # compute custom loss\n        loss=F.binary_cross_entropy_with_logits(\n            logits, \n            labels.to(torch.float32),\n            pos_weight=self.label_weights\n        )\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args=TrainingArguments(\n    output_dir=os.getenv('WANDB_NAME'),\n    learning_rate=1e-4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    report_to='wandb',\n    run_name=os.getenv('WANDB_NAME')\n)\n\ntrainer=CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds['train'],\n    eval_dataset=tokenized_ds['val'],\n    tokenizer=tokenizer,\n    data_collator=functools.partial(collate_fn, tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n    label_weights=torch.tensor(label_weights, device=model.device)\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(os.getenv('WANDB_NAME'))\ntokenizer.push_to_hub(os.getenv('WANDB_NAME'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}