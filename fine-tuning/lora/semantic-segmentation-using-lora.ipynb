{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aisuko/semantic-segmentation-using-lora?scriptVersionId=160569009\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Overview\n\nWith LoRA we can reduce the number of trainable parameters in the SegFormer model to only 14% of the original trainable parameters.\n\nLoRA achieves this reduction by adding low-rank **update matrices** to specific blocks of the model, such as the attention blocks. During fine-tuning, only these matrices are trained, while the original model parameters are left unchanged. At inference time, the update matrices are merged with the original model parameters to produce the final classification result. More detail see [About Low Rank Adaptation](https://www.kaggle.com/code/aisuko/fine-tuning-t5-small-with-lora#About-LoRA(Low-Rank-Adaptation)) and [QLora and LoRA](https://www.kaggle.com/code/aisuko/fine-tuning-llama2-with-qlora#Overview)\n\nSemantic segmentation without usign LoRA see here [Semantic segmentation](https://www.kaggle.com/code/aisuko/semantic-segmentation)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers==4.36.2\n!pip install accelerate==0.25.0\n!pip install evaluate==0.4.1\n!pip install datasets==2.15.0\n!pip install peft==0.7.1\n# !pip install bitsandbytes==0.41.3","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:39:20.245426Z","iopub.execute_input":"2024-01-27T00:39:20.245904Z","iopub.status.idle":"2024-01-27T00:40:33.562877Z","shell.execute_reply.started":"2024-01-27T00:39:20.245865Z","shell.execute_reply":"2024-01-27T00:40:33.561535Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.36.2 in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.2) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.36.2) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.2) (2023.11.17)\nRequirement already satisfied: accelerate==0.25.0 in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.25.0) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.25.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.25.0) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.25.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.25.0) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\nCollecting evaluate==0.4.1\n  Obtaining dependency information for evaluate==0.4.1 from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.1) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.1) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate==0.4.1) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.1) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.1) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.1) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.1) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m880.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting datasets==2.15.0\n  Obtaining dependency information for datasets==2.15.0 from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (11.0.0)\nCollecting pyarrow-hotfix (from datasets==2.15.0)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets==2.15.0)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (3.8.5)\nRequirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.15.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.15.0) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.15.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.15.0) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.15.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.16.0)\nDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow-hotfix, fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.15.0 fsspec-2023.10.0 pyarrow-hotfix-0.6\nCollecting peft==0.7.1\n  Obtaining dependency information for peft==0.7.1 from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (4.36.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.1) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.7.1) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.7.1) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.7.1) (0.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.1) (1.3.0)\nDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"] = \"Fine-tune-models-with-LoRA\"\nos.environ[\"WANDB_NOTES\"] = \"Fine tune model with low rank adaptation\"\nos.environ[\"WANDB_NAME\"] = \"ft-mit-b0-with-scene-parse-150-lora\"\nos.environ[\"MODEL_NAME\"] = \"nvidia/mit-b0\"\n\n# For debuging on GPU\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:40:33.565527Z","iopub.execute_input":"2024-01-27T00:40:33.56592Z","iopub.status.idle":"2024-01-27T00:40:34.512265Z","shell.execute_reply.started":"2024-01-27T00:40:33.565887Z","shell.execute_reply":"2024-01-27T00:40:34.511082Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate estimate-memory ${MODEL_NAME} --library_name transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:40:34.513911Z","iopub.execute_input":"2024-01-27T00:40:34.514229Z","iopub.status.idle":"2024-01-27T00:40:46.714711Z","shell.execute_reply.started":"2024-01-27T00:40:34.514202Z","shell.execute_reply":"2024-01-27T00:40:46.713355Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading pretrained config for `nvidia/mit-b0` from `transformers`...\nconfig.json: 100%|█████████████████████████| 70.0k/70.0k [00:00<00:00, 3.77MB/s]\n┌────────────────────────────────────────────────────┐\n│      Memory Usage for loading `nvidia/mit-b0`      │\n├───────┬─────────────┬──────────┬───────────────────┤\n│ dtype │Largest Layer│Total Size│Training using Adam│\n├───────┼─────────────┼──────────┼───────────────────┤\n│float32│   1.41 MB   │ 12.66 MB │      50.65 MB     │\n│float16│   720.5 KB  │ 6.33 MB  │      25.32 MB     │\n│  int8 │  360.25 KB  │ 3.17 MB  │      12.66 MB     │\n│  int4 │  180.12 KB  │ 1.58 MB  │      6.33 MB      │\n└───────┴─────────────┴──────────┴───────────────────┘\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds=load_dataset(\"scene_parse_150\", split=\"train[:150]\")\nds=ds.train_test_split(test_size=0.1)\ntrain_ds=ds[\"train\"]\ntest_ds=ds[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:40:46.716984Z","iopub.execute_input":"2024-01-27T00:40:46.717444Z","iopub.status.idle":"2024-01-27T00:41:31.885809Z","shell.execute_reply.started":"2024-01-27T00:40:46.717398Z","shell.execute_reply":"2024-01-27T00:41:31.884939Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/21.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f47e1c16aa6243729334e5f75f078255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/50.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9fa1e8689334605a7c08deec056634c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807600060a1f42948dde161a4b62aa30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc1a5a77a134e97807c157ab32ae108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c3dc25b6d884075a005cd4911363233"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557256de1e09479996898592ca526697"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d25ac1adf8e4481a343ea0c9a6cf03c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3352 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3bf79bae89f433f8897c67b0bc666bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c56e7ccff404652aab604495b5f4108"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare label maps\n\nCreate a dictionary that maps a label id to a label class, which will be useful when setting up the model later:\n\n* **label2id**: maps the semantic classes of the dataset to integer ids\n* **id2label**: maps integer ids back to the semantic classes","metadata":{}},{"cell_type":"code","source":"import json\nfrom huggingface_hub import cached_download, hf_hub_url\n\nrepo_id=\"huggingface/label-files\"\nfilename=\"ade20k-id2label.json\"\nid2label=json.load(\n    open(\n        cached_download(\n            hf_hub_url(repo_id, filename, repo_type=\"dataset\")),\"r\"\n    )\n)\n\nid2label={int(k):v for k,v in id2label.items()}\nlabel2id={v:k for k,v in id2label.items()}","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:31.888481Z","iopub.execute_input":"2024-01-27T00:41:31.888854Z","iopub.status.idle":"2024-01-27T00:41:32.092271Z","shell.execute_reply.started":"2024-01-27T00:41:31.888824Z","shell.execute_reply":"2024-01-27T00:41:32.09126Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:655: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ade20k-id2label.json:   0%|          | 0.00/2.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fe468301a8942dfb4506b80de498bcb"}},"metadata":{}}]},{"cell_type":"markdown","source":"Load the SegFormer image processor to prepare the images and annotations for the model. This dataset uses the zero-index as the background class, so make sure to set **do_reduce_labels=True** to subtract one from all labels since the background class is not among the 150 classes.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoImageProcessor\n\nimage_processor=AutoImageProcessor.from_pretrained(os.getenv(\"MODEL_NAME\"), do_reduce_labels=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:32.093692Z","iopub.execute_input":"2024-01-27T00:41:32.094131Z","iopub.status.idle":"2024-01-27T00:41:48.797314Z","shell.execute_reply.started":"2024-01-27T00:41:32.094093Z","shell.execute_reply":"2024-01-27T00:41:48.796276Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe56b72d8044fb19689dbbc780c0999"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n/opt/conda/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Add a function to apply data augmentation to the images, so that the model is more robust against overfitting. Here we use the ColorJitter function from torchvision to randomly change the color properties of an image.","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import ColorJitter\n\njitter=ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:48.798955Z","iopub.execute_input":"2024-01-27T00:41:48.799768Z","iopub.status.idle":"2024-01-27T00:41:48.928594Z","shell.execute_reply.started":"2024-01-27T00:41:48.799725Z","shell.execute_reply":"2024-01-27T00:41:48.92751Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Add a function to handle grayscale images and ensure that each input image has three color channels, regradless of whether it was originally grayscale or RGB. The function converts RGB images to array as is, and for grayscale images that have only one color channel, the function replicates the same channel three times using `np.tile()` before converting the image into an array.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef handle_grayscale_image(image):\n    np_image=np.array(image)\n    if np_image.ndim==2:\n        tiled_image=np.tile(np.expand_dims(np_image, -1),3)\n        return Image.fromarray(tiled_image)\n    else:\n        return Image.fromarray(np_image)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:48.930031Z","iopub.execute_input":"2024-01-27T00:41:48.930427Z","iopub.status.idle":"2024-01-27T00:41:48.936576Z","shell.execute_reply.started":"2024-01-27T00:41:48.930388Z","shell.execute_reply":"2024-01-27T00:41:48.935623Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Finally, combine everything in two functions that you will use to transform training and validation data. The two functions are similar except data augmentation is applied only to the training data.","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\ndef train_transforms(example_batch):\n    images=[jitter(handle_grayscale_image(x)) for x in example_batch[\"image\"]]\n    labels=[x for x in example_batch[\"annotation\"]]\n    inputs=image_processor(images, labels)\n    return inputs\n\ndef val_transforms(example_batch):\n    images=[handle_grayscale_image(x) for x in example_batch[\"image\"]]\n    labels=[x for x in example_batch[\"annotation\"]]\n    inputs=image_processor(images, labels)\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:48.938024Z","iopub.execute_input":"2024-01-27T00:41:48.938409Z","iopub.status.idle":"2024-01-27T00:41:48.954509Z","shell.execute_reply.started":"2024-01-27T00:41:48.938372Z","shell.execute_reply":"2024-01-27T00:41:48.953518Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_ds.set_transform(train_transforms)\ntest_ds.set_transform(val_transforms)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:48.955707Z","iopub.execute_input":"2024-01-27T00:41:48.95603Z","iopub.status.idle":"2024-01-27T00:41:48.978039Z","shell.execute_reply.started":"2024-01-27T00:41:48.956004Z","shell.execute_reply":"2024-01-27T00:41:48.977087Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation function","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport evaluate\n\nmetric=evaluate.load(\"mean_iou\")\n\ndef compute_metrics(eval_pred):\n    with torch.no_grad():\n        logits, labels=eval_pred\n        logits_tensor=torch.from_numpy(logits)\n        logits_tensor=nn.functional.interpolate(\n            logits_tensor,\n            size=labels.shape[-2:],\n            mode=\"bilinear\",\n            align_corners=False,\n        ).argmax(dim=1)\n        \n        pred_labels=logits_tensor.detach().cpu().numpy()\n        \n        metrics=metric._compute(\n            predictions=pred_labels,\n            references=labels,\n            num_labels=len(id2label),\n            ignore_index=0,\n            reduce_labels=image_processor.do_reduce_labels,\n        )\n        \n        per_category_accuracy=metrics.pop(\"per_category_accuracy\").tolist()\n        per_category_iou=metrics.pop(\"per_category_iou\").tolist()\n        \n        metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n        metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n        \n        return metrics","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:48.979321Z","iopub.execute_input":"2024-01-27T00:41:48.979634Z","iopub.status.idle":"2024-01-27T00:41:52.289724Z","shell.execute_reply.started":"2024-01-27T00:41:48.979607Z","shell.execute_reply":"2024-01-27T00:41:52.28885Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef164a3fa06f41e18a4e80be893b4bb6"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Load a base model","metadata":{}},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params=0\n    all_param=0\n    for _,param in model.named_parameters():\n        all_param+=param.numel()\n        if param.requires_grad:\n            trainable_params+=param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100* trainable_params /all_param:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:52.291123Z","iopub.execute_input":"2024-01-27T00:41:52.291933Z","iopub.status.idle":"2024-01-27T00:41:52.298411Z","shell.execute_reply.started":"2024-01-27T00:41:52.291892Z","shell.execute_reply":"2024-01-27T00:41:52.297134Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSemanticSegmentation, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training\n\n# Quantization can cause the \"NotImplementedError\"\n# bnb_config=BitsAndBytesConfig(\n#     load_in_8bit=True\n# )\n\nmodel=AutoModelForSemanticSegmentation.from_pretrained(\n    os.getenv(\"MODEL_NAME\"),\n#     quantization_config=bnb_config,\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)\n\n# model=prepare_model_for_kbit_training(model)\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:52.299802Z","iopub.execute_input":"2024-01-27T00:41:52.300114Z","iopub.status.idle":"2024-01-27T00:41:53.450801Z","shell.execute_reply.started":"2024-01-27T00:41:52.300087Z","shell.execute_reply":"2024-01-27T00:41:53.449752Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/14.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8a1a7106074a6986568b16c084f3ac"}},"metadata":{}},{"name":"stderr","text":"Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.batch_norm.bias', 'decode_head.batch_norm.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.classifier.bias', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 3752694 || all params: 3752694 || trainable%: 100.00\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Wrap the base model as PeftModel for LoRA training\n\nAfter we wrap our base model with PeftModel along with the config, we get a new model where only the LoRA parameters are trainable(**update matrices**) while the pre-trained parameters are kept frozen. These include the parameters of the randomly initialized classifier parameters too. This is NOT we want when fine-tuning the base model on our custom dataset. To ensure that the classifier parameters are also trained, we specify `modules_to_save`. This also ensures that these modules are serialized alongside the LoRA trainable parameters when using utilities like `save_pretrained()` and `push_to_hub()`.\n\n\nThe weight matrix is scaled by `lora_alpha/r`, and higher `lora_alpha` value assigns more weight to the LoRA activations.  For performance, let's set `bias` to `None` first, and then `lora_only`, before trying `all`.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n# We try to employ LoRA to the no support type, but it is still works.\n# https://github.com/huggingface/peft/blob/v0.7.1/src/peft/utils/peft_types.py#L38\n\npeft_config=LoraConfig(\n    r=32,\n    lora_alpha=32,\n    # we want to target query and value matrices in the attention blocks of the base model\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"decode_head\"],\n)\n\npeft_model=get_peft_model(model, peft_config)\nprint_trainable_parameters(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:53.455257Z","iopub.execute_input":"2024-01-27T00:41:53.455618Z","iopub.status.idle":"2024-01-27T00:41:53.822794Z","shell.execute_reply.started":"2024-01-27T00:41:53.455589Z","shell.execute_reply":"2024-01-27T00:41:53.821628Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"trainable params: 564374 || all params: 4317068 || trainable%: 13.07\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's check what modules are trainable in the **lora_model**.","metadata":{}},{"cell_type":"code","source":"for name, param in peft_model.named_parameters():\n    if param.requires_grad:\n        print(name, param.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:53.824067Z","iopub.execute_input":"2024-01-27T00:41:53.824413Z","iopub.status.idle":"2024-01-27T00:41:53.833044Z","shell.execute_reply.started":"2024-01-27T00:41:53.824366Z","shell.execute_reply":"2024-01-27T00:41:53.831959Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"base_model.model.segformer.encoder.block.0.0.attention.self.query.lora_A.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.0.0.attention.self.query.lora_B.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.0.0.attention.self.value.lora_A.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.0.0.attention.self.value.lora_B.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.0.1.attention.self.query.lora_A.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.0.1.attention.self.query.lora_B.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.0.1.attention.self.value.lora_A.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.0.1.attention.self.value.lora_B.default.weight torch.Size([32, 32])\nbase_model.model.segformer.encoder.block.1.0.attention.self.query.lora_A.default.weight torch.Size([32, 64])\nbase_model.model.segformer.encoder.block.1.0.attention.self.query.lora_B.default.weight torch.Size([64, 32])\nbase_model.model.segformer.encoder.block.1.0.attention.self.value.lora_A.default.weight torch.Size([32, 64])\nbase_model.model.segformer.encoder.block.1.0.attention.self.value.lora_B.default.weight torch.Size([64, 32])\nbase_model.model.segformer.encoder.block.1.1.attention.self.query.lora_A.default.weight torch.Size([32, 64])\nbase_model.model.segformer.encoder.block.1.1.attention.self.query.lora_B.default.weight torch.Size([64, 32])\nbase_model.model.segformer.encoder.block.1.1.attention.self.value.lora_A.default.weight torch.Size([32, 64])\nbase_model.model.segformer.encoder.block.1.1.attention.self.value.lora_B.default.weight torch.Size([64, 32])\nbase_model.model.segformer.encoder.block.2.0.attention.self.query.lora_A.default.weight torch.Size([32, 160])\nbase_model.model.segformer.encoder.block.2.0.attention.self.query.lora_B.default.weight torch.Size([160, 32])\nbase_model.model.segformer.encoder.block.2.0.attention.self.value.lora_A.default.weight torch.Size([32, 160])\nbase_model.model.segformer.encoder.block.2.0.attention.self.value.lora_B.default.weight torch.Size([160, 32])\nbase_model.model.segformer.encoder.block.2.1.attention.self.query.lora_A.default.weight torch.Size([32, 160])\nbase_model.model.segformer.encoder.block.2.1.attention.self.query.lora_B.default.weight torch.Size([160, 32])\nbase_model.model.segformer.encoder.block.2.1.attention.self.value.lora_A.default.weight torch.Size([32, 160])\nbase_model.model.segformer.encoder.block.2.1.attention.self.value.lora_B.default.weight torch.Size([160, 32])\nbase_model.model.segformer.encoder.block.3.0.attention.self.query.lora_A.default.weight torch.Size([32, 256])\nbase_model.model.segformer.encoder.block.3.0.attention.self.query.lora_B.default.weight torch.Size([256, 32])\nbase_model.model.segformer.encoder.block.3.0.attention.self.value.lora_A.default.weight torch.Size([32, 256])\nbase_model.model.segformer.encoder.block.3.0.attention.self.value.lora_B.default.weight torch.Size([256, 32])\nbase_model.model.segformer.encoder.block.3.1.attention.self.query.lora_A.default.weight torch.Size([32, 256])\nbase_model.model.segformer.encoder.block.3.1.attention.self.query.lora_B.default.weight torch.Size([256, 32])\nbase_model.model.segformer.encoder.block.3.1.attention.self.value.lora_A.default.weight torch.Size([32, 256])\nbase_model.model.segformer.encoder.block.3.1.attention.self.value.lora_B.default.weight torch.Size([256, 32])\nbase_model.model.decode_head.modules_to_save.default.linear_c.0.proj.weight torch.Size([256, 32])\nbase_model.model.decode_head.modules_to_save.default.linear_c.0.proj.bias torch.Size([256])\nbase_model.model.decode_head.modules_to_save.default.linear_c.1.proj.weight torch.Size([256, 64])\nbase_model.model.decode_head.modules_to_save.default.linear_c.1.proj.bias torch.Size([256])\nbase_model.model.decode_head.modules_to_save.default.linear_c.2.proj.weight torch.Size([256, 160])\nbase_model.model.decode_head.modules_to_save.default.linear_c.2.proj.bias torch.Size([256])\nbase_model.model.decode_head.modules_to_save.default.linear_c.3.proj.weight torch.Size([256, 256])\nbase_model.model.decode_head.modules_to_save.default.linear_c.3.proj.bias torch.Size([256])\nbase_model.model.decode_head.modules_to_save.default.linear_fuse.weight torch.Size([256, 1024, 1, 1])\nbase_model.model.decode_head.modules_to_save.default.batch_norm.weight torch.Size([256])\nbase_model.model.decode_head.modules_to_save.default.batch_norm.bias torch.Size([256])\nbase_model.model.decode_head.modules_to_save.default.classifier.weight torch.Size([150, 256, 1, 1])\nbase_model.model.decode_head.modules_to_save.default.classifier.bias torch.Size([150])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train the model\n\nLet's define the training hyperparameters. Make sure to set `remove_unused_columns=False`, otherwise the image column will be dropped, and it's required here.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args=TrainingArguments(\n    output_dir=os.getenv(\"WANDB_NAME\"),\n    learning_rate=5e-4,\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    save_total_limit=3,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=5,\n    remove_unused_columns=False,\n    push_to_hub=False,\n    label_names=[\"labels\"],\n    report_to=\"wandb\",\n    run_name=os.getenv(\"WANDB_NAME\"),\n)\n\ntrainer=Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    compute_metrics=compute_metrics,\n)\n\n# https://github.com/huggingface/peft/issues/269\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T00:41:53.834681Z","iopub.execute_input":"2024-01-27T00:41:53.8351Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33murakiny\u001b[0m (\u001b[33mcausal_language_trainer\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240127_004156-po6c6hkk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/causal_language_trainer/Fine-tune-models-with-LoRA/runs/po6c6hkk' target=\"_blank\">ft-mit-b0-with-scene-parse-150-lora</a></strong> to <a href='https://wandb.ai/causal_language_trainer/Fine-tune-models-with-LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/causal_language_trainer/Fine-tune-models-with-LoRA' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tune-models-with-LoRA</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/causal_language_trainer/Fine-tune-models-with-LoRA/runs/po6c6hkk' target=\"_blank\">https://wandb.ai/causal_language_trainer/Fine-tune-models-with-LoRA/runs/po6c6hkk</a>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub(os.getenv(\"WANDB_NAME\"))\nimage_processor.push_to_hub(os.getenv(\"WANDB_NAME\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"del trainer, image_processor\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\n\nconfig=PeftConfig.from_pretrained(\"aisuko/\"+os.getenv(\"WANDB_NAME\"))\nmodel=AutoModelForSemanticSegmentation.from_pretrained(\n    config.base_model_name_or_path,\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)\n\ninference_model=PeftModel.from_pretrained(model, \"aisuko/\"+os.getenv(\"WANDB_NAME\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nurl=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png\"\nimage=Image.open(requests.get(url, stream=True).raw)\nimage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_processor=AutoImageProcessor.from_pretrained(\"aisuko/\"+os.getenv(\"WANDB_NAME\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding=image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    outputs=inference_model(pixel_values=encoding.pixel_values)\n    logits=outputs.logits\n\nupsampled_logits=torch.nn.functional.interpolate(\n    logits,\n    size=image.size[::-1],\n    mode=\"bilinear\",\n    align_corners=False,\n)\n\npred_seg=upsampled_logits.argmax(dim=1)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize the result","metadata":{}},{"cell_type":"code","source":"# The original code is here: https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51\n# License: Apache License 2.0\n# Adapte by Aisuko\n# Adapte: Sun 10 Dec 2023\ndef create_ade20k_label_colormap():\n  \"\"\"Creates a label colormap used in ADE20K segmentation benchmark.\n\n  Returns:\n    A colormap for visualizing segmentation results.\n  \"\"\"\n  return np.asarray([\n      [0, 0, 0],\n      [120, 120, 120],\n      [180, 120, 120],\n      [6, 230, 230],\n      [80, 50, 50],\n      [4, 200, 3],\n      [120, 120, 80],\n      [140, 140, 140],\n      [204, 5, 255],\n      [230, 230, 230],\n      [4, 250, 7],\n      [224, 5, 255],\n      [235, 255, 7],\n      [150, 5, 61],\n      [120, 120, 70],\n      [8, 255, 51],\n      [255, 6, 82],\n      [143, 255, 140],\n      [204, 255, 4],\n      [255, 51, 7],\n      [204, 70, 3],\n      [0, 102, 200],\n      [61, 230, 250],\n      [255, 6, 51],\n      [11, 102, 255],\n      [255, 7, 71],\n      [255, 9, 224],\n      [9, 7, 230],\n      [220, 220, 220],\n      [255, 9, 92],\n      [112, 9, 255],\n      [8, 255, 214],\n      [7, 255, 224],\n      [255, 184, 6],\n      [10, 255, 71],\n      [255, 41, 10],\n      [7, 255, 255],\n      [224, 255, 8],\n      [102, 8, 255],\n      [255, 61, 6],\n      [255, 194, 7],\n      [255, 122, 8],\n      [0, 255, 20],\n      [255, 8, 41],\n      [255, 5, 153],\n      [6, 51, 255],\n      [235, 12, 255],\n      [160, 150, 20],\n      [0, 163, 255],\n      [140, 140, 140],\n      [250, 10, 15],\n      [20, 255, 0],\n      [31, 255, 0],\n      [255, 31, 0],\n      [255, 224, 0],\n      [153, 255, 0],\n      [0, 0, 255],\n      [255, 71, 0],\n      [0, 235, 255],\n      [0, 173, 255],\n      [31, 0, 255],\n      [11, 200, 200],\n      [255, 82, 0],\n      [0, 255, 245],\n      [0, 61, 255],\n      [0, 255, 112],\n      [0, 255, 133],\n      [255, 0, 0],\n      [255, 163, 0],\n      [255, 102, 0],\n      [194, 255, 0],\n      [0, 143, 255],\n      [51, 255, 0],\n      [0, 82, 255],\n      [0, 255, 41],\n      [0, 255, 173],\n      [10, 0, 255],\n      [173, 255, 0],\n      [0, 255, 153],\n      [255, 92, 0],\n      [255, 0, 255],\n      [255, 0, 245],\n      [255, 0, 102],\n      [255, 173, 0],\n      [255, 0, 20],\n      [255, 184, 184],\n      [0, 31, 255],\n      [0, 255, 61],\n      [0, 71, 255],\n      [255, 0, 204],\n      [0, 255, 194],\n      [0, 255, 82],\n      [0, 10, 255],\n      [0, 112, 255],\n      [51, 0, 255],\n      [0, 194, 255],\n      [0, 122, 255],\n      [0, 255, 163],\n      [255, 153, 0],\n      [0, 255, 10],\n      [255, 112, 0],\n      [143, 255, 0],\n      [82, 0, 255],\n      [163, 255, 0],\n      [255, 235, 0],\n      [8, 184, 170],\n      [133, 0, 255],\n      [0, 255, 92],\n      [184, 0, 255],\n      [255, 0, 31],\n      [0, 184, 255],\n      [0, 214, 255],\n      [255, 0, 112],\n      [92, 255, 0],\n      [0, 224, 255],\n      [112, 224, 255],\n      [70, 184, 160],\n      [163, 0, 255],\n      [153, 0, 255],\n      [71, 255, 0],\n      [255, 0, 163],\n      [255, 204, 0],\n      [255, 0, 143],\n      [0, 255, 235],\n      [133, 255, 0],\n      [255, 0, 235],\n      [245, 0, 255],\n      [255, 0, 122],\n      [255, 245, 0],\n      [10, 190, 212],\n      [214, 255, 0],\n      [0, 204, 255],\n      [20, 0, 255],\n      [255, 255, 0],\n      [0, 153, 255],\n      [0, 41, 255],\n      [0, 255, 204],\n      [41, 0, 255],\n      [41, 255, 0],\n      [173, 0, 255],\n      [0, 245, 255],\n      [71, 0, 255],\n      [122, 0, 255],\n      [0, 255, 184],\n      [0, 92, 255],\n      [184, 255, 0],\n      [0, 133, 255],\n      [255, 214, 0],\n      [25, 194, 194],\n      [102, 255, 0],\n      [92, 0, 255],\n  ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncolor_seg=np.zeros((pred_seg.shape[0], pred_seg.shape[1],3), dtype=np.uint8)\npalette=np.array(create_ade20k_label_colormap())\n\nfor label, color in enumerate(palette):\n    color_seg[pred_seg==label,:]=color\ncolor_seg=color_seg[...,::-1] # convert to RGB\n\n# plot the image with segmentation map\nimg=np.array(image)*0.5 +color_seg*0.5\nimg=img.astype(np.uint8)\n\nplt.figure(figsize=(15,10))\nplt.imshow(img)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}